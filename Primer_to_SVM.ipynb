{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer to Support Vector Machines\n",
    "\n",
    "### By Dr. Jonne Pohjankukka\n",
    "\n",
    " This tutorial is an ongoing self-made project based on my exercise work on mathematical optimization in 2015. The project is yet unfinished and not in a fully readable state. The upcoming versions will include: \n",
    "\n",
    " updates in the text, fixing of typos, references, added code etc. \n",
    "\n",
    "1. [Machine learning](#1.)<br>\n",
    "2. [Linear vs. nonlinear models](#2.)<br>\n",
    "3. [Support vector machines](#3.)<br>\n",
    "    3.1 [How to find the maximum margin model?](#3.1)<br>\n",
    "    3.2 [Is maximum margin model really better?](#3.2)<br>\n",
    "    3.3 [What if data is not linearly separable?](#3.3)<br>\n",
    "    3.4 [Kernel methods](#3.4)<br>\n",
    "4. [Solving the optimal SVM](#4.)<br>\n",
    "    4.1 [Quadratic programming](#4.1)<br>\n",
    "    4.2 [Lagrangian dual](#4.2)<br>\n",
    "    4.3 [The dual of hard margin SVM](#4.3)<br>\n",
    "    4.4 [Example: solving the SVM classifier via the Lagrangian dual form](#4.4)<br>\n",
    "    4.5 [The dual in the $\\mathcal{Z}$-space](#4.5)<br>\n",
    "5. [Simple algorithm for solving the SVM model](#5.)<br>\n",
    "    5.1 [Active set methods](#5.1)<br>\n",
    "    5.2 [Gradient projection](#5.2)<br>\n",
    "    5.3 [Pseudocode for simple SVM solver](#5.3)<br>\n",
    "6. [Appendix: code listing](#Appendix)<br>\n",
    "\n",
    "#### Preface: to whom is this tutorial for? \n",
    "I think it was back in 2014 or 2015 when I first came across with support vector machines and I remember being frustrated for the difficulty of finding good practical tutorials on the subject. I could find many tutorials online describing the theory and general ideas, but everytime after reading them I had one question: okay, now what? What do I type in my IDE? I could understand the idea, but when I needed to do some programming I found out I actually did not understand it that well. So what I did next was to search again online for some advices, and I found out these guides repeating themselves: \"use a package\", \"download and apply a package\". I understand that this is a perfectly valid solution if your goal is simply to apply support vector machines. In my case, I wanted to understand **every step going on under the hood**. I wanted to understand how one constructs a support vector machine model, how does one train it et cetera **without using any packages at all** (excluding trivial packages of course such as NumPy). How does one implement the model, the mathematical optimization, kernel tricks, everything by yourself? \n",
    "\n",
    "This tutorial is an attempt to provide readers a primer into the subject who have similar problems and frustrations that I had when first running into the subject. I will provide the reader with the general idea of support vector machines, its basic theretical background, practical pen-and-paper examples, pseudocode and Python codes which do not apply any packages related to machines learning or mathematical optimization. I will assume the reader has some understanding on probability, linear algebra and mathematical optimization. Enjoy the ride =) \n",
    "\n",
    "\n",
    "\n",
    "<a id='1.'></a>\n",
    "#### 1. Machine learning\n",
    "What is machine learning (ML)? ML is a subfield of computer science focused on the research and desing of models, which aim to discover and learn patterns from data. Applications of ML could be for example the prediction of stock price values, classification of soil bearing capacity, or forecasting the effects of drinking milk to the acidity levels in human stomach. ML combines techniques from many fields of science, such as probability theory, statistics, physics, mathematical optimization and neuroscience. \n",
    "\n",
    "One of the most important (or maybe better say **the most important**) issues in ML is the concept of generalizability, which measures how well a ML model performs in making predictions in new situations (that is, with new data). A model probably fails to generalize well, if it is \"fitted\" too much to the data (this claim is also backed up theoretically). The notion of \"fitting a model to data\", usually means that we minimize some error function, which describes the goodness-of-fit of our model to the data. The lower the error, the better the fit. In fields such as mathmetical optimization or calculus of variation, the goal is many times to find the absolute minimum of this error (say, the optimum trajectory of a particle). It has however been shown, both from a theoretical and practical perspective, that if you train a model too much _overfitting_ will occur. \n",
    "\n",
    "Overfitting means that the model has learned not only the intrinsic phenomena in the data, but also an additional non-existing relationship called _noise_, which can not be learned by definition. Noise is present in all data you ever measure and can be caused e.g. by measurement errors, weather or malfunctioning sensors. Thus by overfitting a model, we have learned an incorrent relationship from the data, and are more likely to generalize worse. A common way to tackel overfitting is to apply a method known as _regularization_, which basically means restricting the learning process by preventing it from learning too complicated functions. There are many ways to tackle overfitting such as using penalty term or early-stopping methods, but we will not go deeper into this subject in this tutorial. Readers interested with overfitting can find more information from standard ML literature. \n",
    "\n",
    "<a id='2.'></a>\n",
    "#### 2. Linear vs. nonlinear models\n",
    "All methods of ML can be divided into two groups: linear or nonlinear. Linear models are simple and effective methods in many applications describing real world phenomena, but sometimes their expressive power is not enough to learn more complicated relationships in the data. In cases like this, nonlinear methods are usually applied due to their higher expressive power. However, due to its simplicity a linear model is less likely to overfit than nonlinear model. Also in general, nonlinear models require more data than linear models to achieve succesful generalization. There is therefore a trade-off between the expressive power of a model and its likelihood of overfitting to the data. Because of the higher expressive power, nonlinear models are more easily fitted to the noise in the data. With this in mind, it begs now the question: does there exist a model which contains both the resistance towards noise (as in linear models) and high expressive power (as in nonlinear models). A clever method called _support vector machines_ (SVM) was proposed for achieving this by Vladimir Vapnik and Alexey Chervonenkis in 1963, which we will discuss next. In what follows, we will go through the motivation, theory, examples and a self-made implementation (in pseudo- and Python code) of the SVM method.   \n",
    "\n",
    "<a id='3.'></a>\n",
    "#### 3. Support vector machines\n",
    "We will begin with a simple geometric illustration, which best explains the intuition behind SVM. In figure 1 is presented data from two different classes denoted by blue crosses and red circles. The lines depict three competing decision lines which we use to determine the classification regions. The data is _linearly separable_ which means that the data can be divided into two distinct subspaces by the line (or a hyperplane in higher dimensions). Which of these three lines would the best choice and why? Or would you say they are all equally good? After all, all the lines perfectly classify the data. I'm betting however, that you would probably select the line in the rightmost image. Why would we choose this line? What is the intuition behind our choice? Remember that all data we ever measure contains noise. It would be good therefore to select such a decision line which would be most robust against this noise. You can think of the effect of noise in the four data points by shifting them randomly into arbitrary directions a tiny bit. We would want the decision line to allow as much of this random shifting (due to noise) as possible and still achieve correct classification. You would agree that decision line in the rightmost image allows the largest amount of random shifting in the points in the image, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drawClassifierLines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-592c359452a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make Figure 1 plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdrawClassifierLines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshowRadius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'drawClassifierLines' is not defined"
     ]
    }
   ],
   "source": [
    "# Make Figure 1 plot\n",
    "drawClassifierLines(showRadius=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 1: Geometric intuition behind the SVM.</center>\n",
    "\n",
    "As we discussed, we would like to select a decision line which allows as much as possible this random shifting in the data (i.e. noise) without affecting the classification. We can picture this random shifting caused by noise in terms of circles or spheres with radius $r$ around the data points (see figure 2). Notice that in the rightmost image the circles have the largest radius, and so we can think that the data points are allowed to move within this circle (i.e. we have uncertainty) and still we get correct classification. What we would like to do, is to select a linear classifier which maximizes the radius of these spheres. In this way, we have maximized the model's robustness against noise in the data. Notice that in the rightmost image, the distance from the line to the closest data points is maximized. The data points at which the spheres first touch the decision line are called _support vectors_, from which the name of the SVM comes from. The support vectors play a special role in the SVM since they are solely responsible in determining the classifier line. If we would for example remove the rightmost data point from the below images, it would not affect the choice of the classifier line, only the support vectors have impact on this. Lastly, since in SVM the point is to maximize the distance of the line to the support vectors (called the _margin_), SVMs are also called _maximum margin models_.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make Figure 2 plot\n",
    "drawClassifierLines(showRadius=True)"
   ]
  },
  {
   "attachments": {
    "pic.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 2: Uncertainties around the data points in figure 1 illustrated.</center>\n",
    "\n",
    "<a id='3.1'></a>\n",
    "#### 3.1 How to find the maximum margin model? \n",
    "Lets now proceed to formalize the concepts of the SVM and find out how can we solve the maximum margin hyperplane. Denote by $\\textbf{x}\\in\\mathcal{X}\\subset\\mathbb{R}^d$ an input vector, $\\textbf{w}\\in\\mathbb{R}^d$ as hyperplane weight values, $y\\in\\{-1, 1\\}$ as label value and $b\\in\\mathbb{R}$ as a constant intercept term. A hyperplane $h$ defined by vector $\\textbf{w}$ and constant $b$ separates the data points $(\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), ..., (\\textbf{x}_n, y_n)$ if and only if: \n",
    "\n",
    "$$y_i(\\textbf{w}^T\\textbf{x}_i+b) > 0\\;\\;\\;(i = 1, 2, ..., n),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(1)$$\n",
    "\n",
    "where $h(\\textbf{x}) = \\textbf{w}^T\\textbf{x}+b$ is called the _signal_ of input $\\textbf{x}$. That is, for data points having $y=-1$ we want the hyperplane to have a negative signal $h(\\textbf{x}) <0$ and for data points with $y=1$ we want $h(\\textbf{x}) >0$ correspondingly. Notice next that the equations in $(1)$ are always satisfied also for any $h(\\textbf{x})/\\rho$, where $\\rho > 0$. Define next (for reasons later coming clear):\n",
    "\n",
    "$$\\rho := \\underset{i = 1, 2, ..., n}{\\text{min}} y_i(\\textbf{w}^T\\textbf{x}_i+b),$$\n",
    "\n",
    "and redefine the separating hyperplane as $h(\\textbf{x})/\\rho$. For this redefined hyperplane we have: \n",
    "\n",
    "$$\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\frac{h(\\textbf{x})}{\\rho}\\right)=\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\frac{\\textbf{w}^T\\textbf{x}_i}{\\rho}+\\frac{b}{\\rho}\\right)=\\frac{1}{\\rho}\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)=\\frac{\\rho}{\\rho}=1,$$\n",
    "\n",
    "that is, the hyperplane separates all the data points if and only if: \n",
    "\n",
    "$$\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)=1.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(2)$$\n",
    "\n",
    "So we have now learned that a hyperplane $h$ separates the data points only if the condition in $(2)$ is met. This condition is not yet though sufficient enough to define the problem of finding a maximum margin hyperplane. Recall that one of the defining factors of the SVM was also the fact that the margin (distance to the closest, a.k.a support vectors) needs to maximized. In other words, we want to maximize the distance between the hyperplane $h$ and the vector $\\textbf{x}$ closest to it. \n",
    "\n",
    "To start, lets first figure out how one generally calculates the distance between a hyperplane and a vector point. To solve this distance we need to calculate the perpendicular distance between $h$ and $\\textbf{x}$. Let $\\textbf{u}$ be a unit vector perpendicular to $h$ and $\\textbf{x}'$ some point on $h$, i.e. $h(\\textbf{x}') = \\textbf{w}^T\\textbf{x}' + b = 0$. Then, from basic linear algebra (make e.g. a Google search) we know that the distance between $h$ and point $\\textbf{x}$ is the projection $d(h, \\textbf{x}) = |\\textbf{u}^T(\\textbf{x}-\\textbf{x}')|$. Note that $\\textbf{w}$ is perpendicular to the plane $h$, since for two points $\\textbf{x}', \\textbf{x}''$ on the plane $h$ we have:\n",
    "\n",
    "$$\\textbf{w}^T(\\textbf{x}''-\\textbf{x}') = \\textbf{w}^T\\textbf{x}''-\\textbf{w}^T\\textbf{x}'=-b+b = 0,$$\n",
    "\n",
    "that is $\\textbf{w}$ is perpendicular to any vector $\\textbf{x}''-\\textbf{x}'$ on $h$. We can now set $\\textbf{u} = \\textbf{w}/||\\textbf{w}||$, where $||\\textbf{w}||$ is the magnitude of vector $\\textbf{w}$. Hence, we get the distance from an arbitrary point $\\textbf{x}$ to plane $h$ as: \n",
    "\n",
    "$$d(h, \\textbf{x}) = \\frac{\\left\\lvert\\textbf{w}^T(\\textbf{x}-\\textbf{x}')\\right\\rvert}{||\\textbf{w}||}= \\frac{\\left\\lvert\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\textbf{x}'\\right\\rvert}{||\\textbf{w}||} = \\frac{\\left\\lvert\\textbf{w}^T\\textbf{x}+b\\right\\rvert}{||\\textbf{w}||}.$$\n",
    "\n",
    "Furthermore, since for a binary SVM classifier we have $y_i\\in \\{-1, 1\\} \\forall i$, we get: \n",
    "\n",
    "$$\\left\\lvert\\textbf{w}^T\\textbf{x}_i+b\\right\\rvert = y_i(\\textbf{w}^T\\textbf{x}_i+b)\\;\\forall i,$$\n",
    "\n",
    "and because of $(2)$ we have that:\n",
    "\n",
    "$$\\underset{i = 1, 2, ..., n}{\\text{min}} d(h, \\textbf{x}_i) = \\underset{i = 1, 2, ..., n}{\\text{min}} \\frac{y_i(\\textbf{w}^T\\textbf{x}_i+b)}{||\\textbf{w}||}= \\frac{1}{||\\textbf{w}||}\\underset{i = 1, 2, ..., n}{\\text{min}} y_i(\\textbf{w}^T\\textbf{x}_i+b) = \\frac{1}{||\\textbf{w}||}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(3)$$\n",
    "\n",
    "The nice and simple form of equation $(3)$ is the reason why we redefined the hyperplane $h$ earlier by scaling it with the constant $\\rho$, giving us thus a nice numerator of 1. We have now found all the ingredients we need to define the problem of solving the maximum margin model: A SVM model (binary classification) is such a hyperplane, which maximizes the value of $1/||\\textbf{w}||$ (margin) and satisfies the condition in equation $(2)$. This can expressed in the following optimization problem: \n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & \\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)=1.\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(4)\n",
    "\\end{alignat}\n",
    "\n",
    "From the perspective of mathematical optimization, it is easier to solve the problem:\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)\\geq 1,\\;\\;\\;(i=1,2, ..., n)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(5)\n",
    "\\end{alignat}\n",
    "\n",
    "which is equivalent to problem $(4)$ at the optimal solution, given that the data set contains samples with negative and positive labels. To show that equations $(4)$ and $(5)$ are equivalent at the optimum, suppose the solution $(\\textbf{w}^*, b^*)$ of equation $(5)$ has: \n",
    "\n",
    "$$\\rho^* = \\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^{*T}\\textbf{x}_i+b^*\\right) > 1,$$\n",
    "\n",
    "which means that $(\\textbf{w}^*, b^*)$ is not a solution to equation $(4)$. Consider now a hyperplane $(\\textbf{w}, b) = \\frac{1}{\\rho^*}(\\textbf{w}^*, b^*)$ which satisfies the constraints in equation $(5)$. But now we have that $||\\textbf{w}|| = \\frac{1}{\\rho^*}||\\textbf{w}^*|| < ||\\textbf{w}^*||$, which means that $\\textbf{w}^*$ can not be optimal for equation $(5)$ unless $\\textbf{w}^*=\\textbf{0}$. This however is not possible, because the hyperplane $(\\textbf{0}, b)$ can not correctly classify the negative and positive data samples $\\blacksquare$. So, once we have solved the optimal solution $(\\textbf{w}^*, b^*)$ to equation $(5)$, we get the maximum margin classifier model (a.k.a SVM) as: \n",
    "\n",
    "$$g(\\textbf{x})=\\text{sign}\\left(\\textbf{w}^{*T}\\textbf{x}+b^*\\right)\\in\\{-1, 1\\}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(6)$$\n",
    "\n",
    "In the upcoming sections, we will see how to solve the optimization problem of equation $(5)$. First however, we will have a short discussion on why the maximum margin model is better than a non-maximum margin model in terms of generalizability. \n",
    "\n",
    "<a id='3.2'></a>\n",
    "#### 3.2 Is maximum margin model really better?\n",
    "In earlier sections, we had the intuition that a linear classifier with a larger margin would probably perform better than one with a smaller margin. In fact, our intuition is justified also mathematically by a special number called the _Vapnik-Chervonenkis-dimension_ $d_{\\text{VC}}\\in \\mathbb{N}_{>0}$ (VC-dimension), which quantifies a probabilistic bound for the classification error of the SVM model. Generally speaking, a model with a smaller VC-dimension is more likely to achieve succesful generalization than one with a higher VC-dimension. And it is indeed the case (see the works of e.g. V. Vapnik), that the VC-dimension of a SVM model is smaller than the VC-dimension of an unrestricted linear classifier (unrestricted by the margin that is). This fact is a result from an inequality called the _VC-inequality_ (related closely to the Hoeffding inequality), which states that: \n",
    "\n",
    "$$P\\left[\\underset{h\\in\\mathcal{H}}{\\text{sup}}\\left\\lvert E_{\\text{in}}(h)-E_{\\text{out}}(h)\\right\\rvert > \\varepsilon \\right] \\leq 4m_{\\mathcal{H}}(2n)\\exp\\left(-\\frac{1}{8}\\varepsilon^2 n\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(7)$$\n",
    "\n",
    "where $\\mathcal{H}$ is the hypothesis set, that is, a set of functions from which we are searching for the model $h$, $\\varepsilon >0$ is an error bound and $n\\in\\mathbb{N}_{>0}$ is the number of data points. The functions $E_{\\text{in}}(h), E_{\\text{out}}(h): \\mathcal{H}\\to \\mathbb{R}$ denote the training and generalization errors of hypothesis $h$. That is, $E_{\\text{in}}(h)$ describes how well we were able to fit model $h$ to the observed data $\\mathcal{D}=\\{(\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), ..., (\\textbf{x}_n, y_n)\\}$, and $E_{\\text{out}}(h)$ describes how well the model $h$ performs with new yet unseen data. Our goal is to have $E_{\\text{in}}(h)\\approx E_{\\text{out}}(h)$, because in this case the we can trust that our model's performance estimated with the data set $\\mathcal{D}$ reflects its performance in a general situation. \n",
    "\n",
    "The function $m_{\\mathcal{H}}(2n):\\mathbb{N}_{>0}\\to \\mathbb{N}$ maps $2n$ to a number, which describes the maximum number of dichotomies that can be generated by the hypothesis set $\\mathcal{H}$ on any $2n$ data points. In other words, it gives a quantifying number on how many ways the hypothesis set $\\mathcal{H}$ can split $2n$ data points into two categories. If $m_{\\mathcal{H}}(c) = 2^{c}$ for $c$ data points, then we say that the hypothesis set $\\mathcal{H}$ is able _shatter_ the $c$ data points, that is find all possible dichotomies (classifications) for the data. To put the relationship between $d_{\\text{VC}}$ and $m_{\\mathcal{H}}(c)$ explicit, the VC-dimension is defined as $d_{\\text{VC}}\\equiv \\max\\{c\\in\\mathbb{N}\\,|\\,m_\\mathcal{H}(c)=2^c\\}$. In other words, VC-dimension is the maximum number of data points the hypothesis set $\\mathcal{H}$ is able to shatter. The next question now is, what has the VC-dimension got to do with the VC-inequality? It is known, that if a hypothesis set $\\mathcal{H}$ has a finite $d_{\\text{VC}}$, then it holds that: \n",
    "\n",
    "$$m_{\\mathcal{H}}(2n) \\leq \\sum_{i=0}^{d_{\\text{VC}}} {{2n}\\choose i},$$\n",
    "\n",
    "and therefore a hypothesis set with a smaller VC-dimension has a smaller multiplying factor in the right side of equation $(7)$ resulting in a smaller probability bound. This is also somewhat intuitive if you think about. If you are using a hypothesis set with unrestricted (by the margin) classifier models, then you would probably be able to find more dichotomies for the data set $\\mathcal{D}$, than with the hypothesis set consisting from models with the restriction to maximize the margin. With a large number of equally good hypotheses to choose from, you would have smaller chances (a higher probability bound in the VC-inequality) to pick the right one unless you are very lucky.    \n",
    "\n",
    "<a id='3.3'></a>\n",
    "#### 3.3 What if data is not linearly separable?\n",
    "So far, we have been talking about cases where the data set can be separated by a linear model. In many practical situations however the data set can not be separated by a linear model. In Figure 3 I have illustrated two examples of data sets which can not be separated by a linear model.\n",
    "\n",
    "![pic.png](attachment:pic.png)\n",
    "<center>Figure 3: Data which is not linearly separable.</center>\n",
    "\n",
    "So how should we proceed from this? Well, fortunately we two options two consider. Firstly, we can loosen up the condition on the SVM which requires all the data points to be classified correctly. We do this by allowing few misclassifications for the model to occur. An SVM with a looser condition like this is called the _soft margin SVM_, which does not force a perfect classification and allows some mistakes to occur. On the left side of figure 3, we see an example of a soft margin SVM model. Our second option to the separability problem, is to first apply a nonlinear transformation to the data making it linearly separable, and then fit a hard margin linear SVM into this transformed data. In the right side of figure 3 is an example of this second approach. Note that the nonlinear model you seen here is a representation of the linear model of the transformed space in the original data space. The nonlinear transformation is achieved by a function $\\Phi:\\mathcal{X} \\to \\mathcal{Z}\\subset \\mathbb{R}^q$, which transforms the input vectors into $\\textbf{z}_i = \\Phi(\\textbf{x}_i)$. After the transformation, the optimization problem in equation $(5)$ becomes: \n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & y_i\\left(\\textbf{w}^T\\textbf{z}_i+b\\right)\\geq 1,\\;\\;\\;(i=1,2, ..., n),\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(8)\n",
    "\\end{alignat}\n",
    "\n",
    "and similarly to equation $(6)$ we get the nonlinear hard margin SVM model as:\n",
    "\n",
    "$$g(\\textbf{x})=\\text{sign}\\left(\\textbf{w}^{*T}\\Phi(\\textbf{x})+b^*\\right)\\in\\{-1, 1\\}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(9)$$\n",
    "\n",
    "What happens to the generalization capability if we use nonlinear transformation for the data? Does it get worse? In general, the price we pay for using nonlinear models with higher expressive power is that we have a larger risk to overfit the data and that the probability of successful generalization decreases. Fortunately, a neat mathematical theorem exists (see e.g. Mostafa et al. for proofs) which helps us tackle the generalization problem of SVMs using nonlinear transformation. The theorem states that: \n",
    "\n",
    "**Theorem:** the VC-dimension of maximum margin classifier SVM with bounded input data $||\\textbf{x}||\\leq R$ and margin $r\\in \\mathbb{R}^{+}$ follows the inequality:\n",
    "\n",
    "$$d_{\\text{VC}} \\leq \\lceil R^2 / r^2 \\rceil +1,$$\n",
    "\n",
    "regardless on the nonlinear transformation $\\Phi$. This means that even if SVM incorporates an infinite dimensional transformation, generalization is achieved as long as we use large enough margin $r$.\n",
    "\n",
    "<a id='3.4'></a>\n",
    "#### 3.4 Kernel methods\n",
    "In earlier sections, we went through formulating the mathematical optimization problem of solving a SVM model. This problem was presented explicitly in equations $(5)$ and $(8)$. It will in later sections become clear that when solving these problems, we need to calculate inner products $\\textbf{x}_i^T\\textbf{x}_j$ (linear SVM) and $\\textbf{z}_i^T\\textbf{z}_j=\\Phi(\\textbf{x}_i)^T\\Phi(\\textbf{x}_j)$ (nonlinear SVM). This seems simple enough, as it is in many cases but one can find transformations $\\Phi$ which map the inputs $\\textbf{x}$ into infinite dimensional vectors (i.e. $d=\\infty$). It is obvious, that we can not calculate the inner products of infinite dimensional vectors $\\textbf{z}$, since this would require us first to explicitly calculate the infinite vectors $\\textbf{z}$. Fortunately though, there exists cool functions which allow us to calculate these infinite inner products without explicitly knowing the vectors $\\textbf{z}$. These functions are called _kernel functions_, which describe the inner products of vectors in some (possibly infinite dimensional) space $\\mathcal{Z}$. To be explicit, the kernel functions defined by transformation $\\Phi$ are defined as: \n",
    "\n",
    "$$K_{\\Phi}(\\textbf{x}, \\textbf{x}')\\equiv \\Phi(\\textbf{x})^T\\Phi(\\textbf{x}').$$\n",
    "\n",
    "In other words, kernel function $K_{\\Phi}$ takes two vectors $\\textbf{x}, \\textbf{x}' \\in \\mathcal{X}$ as input and returns the inner product of $\\Phi$-transformed vectors in $\\mathcal{Z}$-space. This process is called the _kernel trick_, where the trick part comes from the fact that we do not need to explicitly calculate the vectors in $\\mathcal{Z}$-space in order to calculate their inner product. To give few examples of common kernel functions, a first example is the _polynomial kernel function_ of $Q$-degree defined as: \n",
    "\n",
    "$$K(\\textbf{x}, \\textbf{x}') \\equiv (\\lambda + \\gamma\\textbf{x}^T\\textbf{x}')^Q,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(10)$$\n",
    "\n",
    "where $\\lambda, \\gamma > 0$. Another  common example is the _Gaussian radial basis function kernel_ defined as: \n",
    "\n",
    "$$K(\\textbf{x}, \\textbf{x}') \\equiv \\exp\\left(-\\frac{||\\textbf{x}-\\textbf{x}'||^2}{2\\sigma^2}\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(11)$$\n",
    "\n",
    "where $\\sigma > 0$. It is straightforward to show, that if we set $\\sigma = 1$ in equation $(11)$, then we have\n",
    "\n",
    "$$K(\\textbf{x}, \\textbf{x}') = \\exp\\left(-\\frac12||\\textbf{x}-\\textbf{x}'||^2\\right) = \\sum_{k=0}^{\\infty} \\frac{(\\textbf{x}^T\\textbf{x}')^k}{k!}\\exp\\left(-\\frac{1}{2}||\\textbf{x}||^2\\right)\\exp\\left(-\\frac{1}{2}||\\textbf{x}'||^2\\right),$$\n",
    "\n",
    "which represents an inner product in an _infinite dimensional_ $\\mathcal{Z}$-space (the $\\Phi$-function produces an infinite dimensional vector). It turns out also, that one can construct his own kernel functions $K$, if it is valid that the symmetric matrix: \n",
    "\n",
    "$$M=\\begin{bmatrix}\n",
    "    K(\\textbf{x}_1, \\textbf{x}_1) & K(\\textbf{x}_1, \\textbf{x}_2) & \\cdots &  K(\\textbf{x}_1, \\textbf{x}_n) \\\\\n",
    "    K(\\textbf{x}_2, \\textbf{x}_1) & K(\\textbf{x}_2, \\textbf{x}_2) & \\cdots &  K(\\textbf{x}_2, \\textbf{x}_n) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    K(\\textbf{x}_n, \\textbf{x}_1) & K(\\textbf{x}_n, \\textbf{x}_2) & \\cdots &  K(\\textbf{x}_n, \\textbf{x}_n) \n",
    "\\end{bmatrix}, \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(12)$$\n",
    "\n",
    "is positive semidefinite (i.e. $\\textbf{x}_i^TM\\textbf{x}_i \\geq 0\\; \\forall\\; i$) for all vectors $\\{\\textbf{x}_1, \\textbf{x}_2, ..., \\textbf{x}_n\\}$. This condition is called _Mercer's condition_. In other words, if your $K$ satisfies Mercer's condition, then you have a valid kernel function.  \n",
    "\n",
    "<a id='4.'></a>\n",
    "#### 4. Solving the optimal SVM\n",
    "We have so far gone through the core ideas of SVM models: the maximum margin classifier, VC-inequality, linearly/non-linearly separable data and the kernel methods. We have observed, that in order to solve the SVM classifier $(\\textbf{w}, b)$ we need to solve the problem in equation $(5)$:\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)\\geq 1\\;\\;\\;(i=1,2, ..., n).\n",
    "\\end{alignat}\n",
    "\n",
    "Lets now go on and solve this problem in a toy example. Let the input data be defined in the following way:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    0 & 0 \\\\\n",
    "    2 & 2 \\\\\n",
    "    2 & 0 \\\\\n",
    "    3 & 0 \n",
    "\\end{bmatrix}\\;\\;\\;\\textbf{y} = \\begin{bmatrix}\n",
    "    -1 \\\\ -1 \\\\ +1 \\\\ +1 \n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where matrix $X$ represents the container of input vectors of $\\textbf{x}$ and $\\textbf{y}$ represents the class labels correspondingly. Each row in $X$ and $\\textbf{y}$ correspond to a single data point. Formulating the problem of equation $(5)$ in terms of this data we get: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{w_1, w_2, b}{\\text{minimize:}}\n",
    "& & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{1}{2}(w_1^2+w_2^2) \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\begin{aligned}[c]\n",
    "-b & \\geq 1\\;\\;(\\text{i})\\\\\n",
    "-(2w_1+2w_2+b) & \\geq 1 \\;\\; (\\text{ii})\\\\\n",
    "2w_1+b & \\geq 1 \\;\\; (\\text{iii})\\\\\n",
    "3w_1+b & \\geq 1 \\;\\; (\\text{iv}).\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "By combining inequalities (i), (iii) and (ii), (iii) together we get the conditions $w_1 \\geq 1$ and $w_2 \\leq -1$. By now squaring both these two new conditions and combining them we get the condition $\\frac{1}{2}(w_1^2+w_2^2)\\geq 1$. From this we see that the minimum is achieved when (with conditions satisfied) $w_1 = 1$ ja $w_2 = -1$. Substituting these values of $w_1, w_2$ into inequalities (ii)-(iv) we get that $b=-1$ and so the SVM model in this case is: \n",
    "\n",
    "$$(w_1^*, w_2^*, b^*) = (1, -1, -1),$$  \n",
    "\n",
    "and so in terms of equation $(6)$ we get the SVM classifier as:  \n",
    "\n",
    "\\begin{equation*}\n",
    "g(\\textbf{x}) = \\text{sign}\\left(x_1 - x_2 -1 \\right).\n",
    "\\end{equation*}\n",
    "\n",
    "Notice that the width of the margin in this case is $\\frac{1}{||\\textbf{w}^*||}=\\frac{1}{\\sqrt{2}}\\approx 0.707$. This is illustrated in figure X, with the distance from the line to the support vectors is $\\frac{1}{\\sqrt{2}}$. It was fairly easy to solve this particular SVM model but in general it is not this easy. The problem might have a lot more parameters and inequalities in which case we need to use more sophisticated methods to solve the SVM model. We will next consider _quadratic programming_, which we use for solving more general SVM models. \n",
    "\n",
    "<a id='4.1'></a>\n",
    "#### 4.1 Quadratic programming\n",
    "In this section we will aim to solve the parameters $(\\textbf{w}, b)$ of the SVM model in equation $(6)$ using the methods of quadratic programming (QP). In a general QP-problem, we aim to solve problem:    \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{a}_i^T\\textbf{u} \\geq c_i\\;\\;\\;\\;\\;\\left(i = 1, ..., m\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(13)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $Q$ is a $l\\times l$ symmetric positive semidefinite matrix, $\\textbf{u}, \\textbf{p}, \\textbf{a}_i$ are $l$-dimensional vectors and $c_i$ is some constant. The optimization problem in $(13)$ contains a qudratic term $\\frac{1}{2}\\textbf{u}^T Q\\textbf{u}$ and a linear term $\\textbf{p}^T\\textbf{u}$. The constraints $\\textbf{a}_i^T\\textbf{u}\\geq c_i$ are linear. Because $Q$ is positive semidefinite the problem in $(13)$ is a convex optimization problem. If the vectors $\\textbf{a}_i$ and constants $c_i$ are grouped into matrix $A$ and vector $\\textbf{c}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left[\\begin{array}{c}\n",
    "\\textbf{a}_1^T  \\\\\n",
    "\\vdots   \\\\\n",
    "\\textbf{a}_m^T\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{c} = \\left[\\begin{array}{c}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_m\n",
    "\\end{array}\\right],\n",
    "\\end{equation*}\n",
    "\n",
    "then the optimization problem $(13)$ can be presented in the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{quadprog2}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^T Q\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A\\textbf{u} \\geq \\textbf{c}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The constraints of problem $(13)$ can also be presented in the form $A\\textbf{u}\\leq \\textbf{c}$ by multiplying both sides with $-1$. Next we will show that the SVM optimization problem in $(5)$ can be represented as a QP-problem. In order to do this, we must identify $A, Q, \\textbf{u}, \\textbf{p}$ ja $\\textbf{c}$ from problem $(5)$. This requires finding the optimal values of $(\\textbf{w}, b)$ so we have now: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\textbf{u} = \\left[ \\begin{array}{c}\n",
    "b\\\\\n",
    "\\textbf{w}\n",
    "\\end{array} \\right] \\in \\mathbb{R}^{d+1},\n",
    "\\end{equation*}\n",
    "\n",
    "so $l=d+1$. Our task is to minimize the term $\\frac{1}{2}\\textbf{w}^T\\textbf{w}$, which must be presented in the form $\\frac{1}{2}\\textbf{u}^T Q\\textbf{u} + \\textbf{p}^T\\textbf{u}$. We note that:  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\textbf{w}^T\\textbf{w} = \\left[b \\;\\;\\textbf{w}^T \\right]\\left[\\begin{array}{cc}\n",
    "0 & \\textbf{0}_d^T\\\\\n",
    "\\textbf{0}_d & \\textbf{I}_d\n",
    "\\end{array}\\right]\n",
    "\\left[ \\begin{array}{c}\n",
    "b\\\\\n",
    "\\textbf{w}\n",
    "\\end{array} \\right],\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\textbf{I}_d$ is a $d\\times d$ identity matrix and $\\textbf{0}_d$ is a $d$-dimensional zero vector. From this we see that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q = \\left[\\begin{array}{cc}\n",
    "0 & \\textbf{0}_d^T\\\\\n",
    "\\textbf{0}_d & \\textbf{I}_d\n",
    "\\end{array}\\right] \\;\\;\\;\\;\\;\\;\n",
    "\\textbf{p} = \\textbf{0}_{d+1},\n",
    "\\end{equation*}\n",
    "\n",
    "where $Q$ is positive semidefinite. Because there are total of $n$ $y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right) \\geq 1$ inequalities we get that $m=n$. Also note that these inequalities are equivalent with the inequalities:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left[y_i \\;\\; y_i\\textbf{x}_i^T\\right]\\textbf{u} \\geq 1,\n",
    "\\end{equation*}\n",
    "\n",
    "and so by setting $\\textbf{a}_i^T = y_i\\left[1 \\;\\; \\textbf{x}_i^T\\right]$ and $c_i=1$ in problem $(13)$ we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left[\\begin{array}{cc}\n",
    "y_1 & y_1\\textbf{x}_1^T \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "y_n & y_n\\textbf{x}_n^T\n",
    "\\end{array}\\right] \\;\\;\\;\\;\\;\\;\n",
    "\\textbf{c} = \\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore we have shown that problem $(5)$ is indeed a QP-problem of form $(13)$. Next, we will take a look at the Lagrangian duality form for hard margin SVM, which makes solving the problem $(13)$ easier and brings in the kernel methods introduced in section 3.4. We will also go through why the problem $(5)$ includes calculating the inner products $\\textbf{x}_i^T\\textbf{x}_j, \\forall i,j \\in \\{1, ..., n\\}$ as we promised earlier in section 3.4.\n",
    "\n",
    "<a id='4.2'></a>\n",
    "#### 4.2 Lagrangian dual\n",
    "In section 3, we discussed about applying the nonlinear transformation $\\Phi: \\mathcal{X} \\to \\mathcal{Z} \\subset \\mathbb{R}^q$ to the input vectors and we ended up into the optimization problem $(8)$. In this problem, we have $q+1$ variables to solve since $\\textbf{u} = [\\tilde{\\textbf{w}}, \\tilde{b}] \\in \\mathbb{R}^{q+1}$. It is quite difficult to solve this problem if $q$ is very large or even infinite $(q=\\infty)$. By transforming the problem $(8)$ called the _primal_ into another form called the _Lagrangian dual form_ the SVM problem is transformed into a QP-problem with $n$ variables to be solved with $n+1$ constraints. The dual problem is independent of the dimensionality of the space $\\mathcal{Z}$, and depends only from the amount of data points $n$. This is a very useful method to apply especially when $q$ is very large. We will next see how to transform the QP-problem in $(13)$ into a dual form. To begin, we start from the primal form of the problem: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{a}_i^T\\textbf{u} \\geq c_i\\;\\;\\;\\;\\;\\left(i = 1, ..., m\\right).\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We proceed next by dropping out the constraints in the above problem and adding a _penalty term_ in the following way: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} +  \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\sum_{i=1}^m \\alpha_i \\left( c_i - \\textbf{a}_i^T\\textbf{u} \\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(14)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\boldsymbol\\alpha = \\left(\\alpha_1, ..., \\alpha_m\\right)$. It can be shown (see e.g. literature on nonlinear programming) that the primal problem $(13)$ is equivalent with this new problem $(14)$, as long as there exists at least one solution which satisfies the constraints of problem $(13)$. The penalty term in $(14)$ encourages the optimization to choose vectors $\\textbf{u}$ such that $c_i-\\textbf{a}_i^T\\textbf{u}\\leq 0$ (because $\\alpha_i \\geq 0$), satisfying thus the constraints in $(13)$. Notice that in $(14)$, at the optimum solution $\\left(\\textbf{u}^*, \\boldsymbol\\alpha^*\\right)$ we have either $\\alpha^*_i = 0$ or $c_i - \\textbf{a}_i^T\\textbf{u}^*=0\\;\\forall\\,i$. Thus, we can instead of problem $(13)$ solve the simpler unconstrained problem $(14)$. The function in $(14)$ is called the _Lagrangian_ which is defined as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right) = \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} + \\sum_{i=1}^m \\alpha_i \\left( c_i - \\textbf{a}_i^T\\textbf{u} \\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(15)\n",
    "\\end{equation}\n",
    "\n",
    "so our task is to solve the optimization task: \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{LagrangeOpt}\n",
    "\\min_{\\textbf{u}\\in \\mathbb{R}^l}\\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(16)\n",
    "\\end{equation}\n",
    "\n",
    "In convex quadratic programming we can take advantage of the so-called _strong duality_: \n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\textbf{u}\\in \\mathbb{R}^l}\\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right) = \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}}\\min_{\\textbf{u}\\in \\mathbb{R}^l} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(17)\n",
    "\\end{equation}\n",
    "\n",
    "which can be shown to be true if the Lagrangian $\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right)$ has the form as in $(15)$ and there exists a solution satisying the constraints $\\textbf{a}_i^T\\textbf{u}\\geq c_i$. A proof for these facts can be found from literature related to quadratic programming and convex optimization. With the help of the Lagrangian and strong duality we have transformed the original problem $(13)$ into an easier unconstrained optimization problem:   \n",
    "\n",
    "\\begin{equation}\n",
    " \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}}\\min_{\\textbf{u}\\in \\mathbb{R}^l} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right),\n",
    "\\end{equation}\n",
    "\n",
    "which is called the _Lagrangian dual problem_. To wrap up this section, we will state the necessary conditions a solution of the QP-problem in $(13)$ must satisfy.\n",
    "\n",
    "**Necessary optimality conditions for the QP-problem $(13)$.** If the primal convex QP-problem in $(13)$: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{a}_i^T\\textbf{u} \\geq c_i\\;\\;\\;\\;\\;\\left(i = 1, ..., m\\right),\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "has the corresponding Lagrangian function in $(15)$: \n",
    "\n",
    "$$\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right) = \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} + \\sum_{i=1}^m \\alpha_i \\left( c_i - \\textbf{a}_i^T\\textbf{u} \\right),$$ \n",
    "\n",
    "then the solution $\\textbf{u}^*$ is optimal for the primal problem $(13)$, if and only if $\\left(\\textbf{u}^*, \\boldsymbol\\alpha^*\\right)$ is a solution for the dual problem in $(14)$:\n",
    "\n",
    "$$ \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}}\\min_{\\textbf{u}} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right).$$\n",
    "\n",
    "The optimal solution $\\left(\\textbf{u}^*, \\boldsymbol\\alpha^*\\right)$ of problem $(14)$ must satisfy the following conditions: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}[c]\n",
    "(\\text{i}) & \\;\\;\\;\\textbf{a}_i^T\\textbf{u}^* \\geq c_i\\;\\;\\text{and}\\;\\;\\alpha_i^* \\geq 0\\;\\;\\forall\\,i.\\\\[2mm]\n",
    "(\\text{ii}) & \\;\\;\\; \\alpha_i^*\\left( \\textbf{a}_i^T\\textbf{u}^* - c_i \\right) = 0\\;\\;\\forall\\,i.\\\\[2mm]\n",
    "(\\text{iii}) & \\;\\;\\;\\nabla_{\\textbf{u}}\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right)|_{\\textbf{u}=\\textbf{u}^*,\\; \\boldsymbol\\alpha = \\boldsymbol\\alpha^*} = \\textbf{0},\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\nabla_{\\textbf{u}}$ denotes the gradient with respect to $\\textbf{u}$. The conditions (i)-(iii) are called the Karush-K$\\ddot{\\text{u}}$hn-Tucker (KKT)-conditions. The KKT-conditions give us an analytical way to either check or solve the optimal solution of the QP-problem $(13)$. At the last sections of this tutorial, we will produce an iterative algorithm for solving the SVM model and we therefore do not directly apply the KKT-conditions for solving the optimal SVM model. We can however use the KKT-conditions to check whether our iteratively solved solution is indeed optimal or not.  \n",
    "\n",
    "<a id='4.3'></a>\n",
    "#### 4.3 The dual of hard margin SVM\n",
    "Lets now next get back to solving the SVM model. In section 4.1, we identified the factors $A, Q, \\textbf{u}, \\textbf{p}, \\textbf{c}$ in order to incorporate the SVM problem $(5)$ into the form in $(13)$. We will now proceed to find the Lagrangian dual function $\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right)$ of the SVM problem. To recall, our original problem was: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{b,\\textbf{w}}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\;y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right) \\geq 1\\;\\;\\;\\;\\;\\;\\;(i = 1, ..., n).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The optimal solution we aim to find for this problem is the vector $\\textbf{u} = [b, \\textbf{w}]^T$. Lets go on and solve the Lagrangian function. Because there are $n$ contraints, we will get $n$ penalty terms into the Lagrangian with coefficients $\\alpha_i$. The Lagrangian function is therefore: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(b, \\textbf{w}, \\boldsymbol\\alpha\\right) &= \\frac{1}{2}\\textbf{w}^T\\textbf{w} + \\sum_{i=1}^n \\alpha_i\\left(\n",
    "1-y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)\\right)\\\\\n",
    "&= \\frac{1}{2}\\textbf{w}^T\\textbf{w} - \\sum_{i=1}^n\\alpha_iy_i\\textbf{w}^T\\textbf{x}_i - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(18)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Our first step is to minimize the Lagrangian function with respect to the vector $\\textbf{u}$ and then maximize with respect to $\\boldsymbol\\alpha \\geq \\textbf{0}$. So lets take the derivative of $\\mathcal{L}$ with respect to $b$ and $\\textbf{w}$. We get: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^n\\alpha_iy_i \\;\\;\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;\\;\\;\\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} = \\textbf{w}- \\sum_{i=1}^n\\alpha_iy_i\\textbf{x}_i,\n",
    "\\end{equation*}\n",
    "\n",
    "and setting these to zero we get: \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n\\alpha_iy_i = 0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(19)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{w} = \\sum_{i=1}^n\\alpha_iy_i\\textbf{x}_i.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(20)\n",
    "\\end{equation}\n",
    "\n",
    "By plugging these into the Lagrangian function $(15)$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(b, \\textbf{w}, \\boldsymbol\\alpha\\right) &= \\frac{1}{2}\\textbf{w}^T\\textbf{w} - \\sum_{i=1}^n\\alpha_iy_i\\textbf{w}^T\\textbf{x}_i - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i\\\\\n",
    "& = \\frac{1}{2}\\sum_{i=1}^n\\alpha_iy_i\\textbf{x}_i^T\\sum_{m=1}^n\\alpha_my_m\\textbf{x}_m - \\sum_{i=1}^n\\alpha_iy_i\\sum_{m=1}^n\\alpha_my_m\\textbf{x}_m^T\\textbf{x}_i \\\\ \n",
    "& - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "&= \\frac{1}{2}\\sum_{i=1}^n\\sum_{m=1}^ny_iy_m\\alpha_i\\alpha_m\\textbf{x}_i^T\\textbf{x}_m - \\sum_{i=1}^n\\sum_{m=1}^ny_iy_m\\alpha_i\\alpha_m\\textbf{x}_i^T\\textbf{x}_m + \\sum_{i=1}^n\\alpha_i \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^n\\sum_{m=1}^ny_iy_m\\alpha_i\\alpha_m\\textbf{x}_i^T\\textbf{x}_m + \\sum_{i=1}^n\\alpha_i,\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "and so we get that: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) = -\\frac{1}{2}\\sum_{i=1}^n\\sum_{m=1}^ny_iy_m\\alpha_i\\alpha_m\\textbf{x}_i^T\\textbf{x}_m + \\sum_{i=1}^n\\alpha_i,\n",
    "\\end{equation}\n",
    "\n",
    "which is function of only the vector $\\boldsymbol\\alpha \\geq \\textbf{0}$. Our task is therefore to maximize $\\mathcal{L}\\left(\\boldsymbol\\alpha\\right)$ so that $\\boldsymbol\\alpha \\geq \\textbf{0}$. \n",
    "\n",
    "We have now new constraints for the variables $\\alpha_m$ by the equation $(19)$ from which we get a total of $n+1$ constraints. Instead of maximizing the function $\\mathcal{L}\\left(\\boldsymbol\\alpha\\right)$ we can also equivalently minimize the function $-\\mathcal{L}\\left(\\boldsymbol\\alpha\\right)$, and so we have transformed the original optimization problem $(5)$ into the problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{1}{2}\\sum_{i=1}^n\\sum_{m=1}^ny_iy_m\\alpha_i\\alpha_m\\textbf{x}_i^T\\textbf{x}_m - \\sum_{i=1}^n\\alpha_i \\\\\n",
    "& \\text{subject to:}\n",
    "  \\;\\;\\;\\;\\;\\; \\sum_{i=1}^n\\alpha_iy_i = 0 \\\\\n",
    "& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha_i \\geq 0\\;\\;\\;\\;\\;\\;\\;(i = 1, ..., n).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(21)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "From problem $(21)$ we see why the calculation of inner products $\\textbf{x}_i^T\\textbf{x}_j, \\forall \\:i, j \\in \\{1,...,n\\}$ is important as we discussed before in section 3.4. The problem $(21)$ can be rewritten as the concex QP-problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A_D\\boldsymbol\\alpha \\geq \\textbf{0}_{n+2},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(22)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{ccc}\n",
    "y_1y_1\\textbf{x}_1^T\\textbf{x}_1 & \\cdots & y_1y_n\\textbf{x}_1^T\\textbf{x}_n \\\\\n",
    "y_2y_1\\textbf{x}_2^T\\textbf{x}_1 & \\cdots & y_2y_n\\textbf{x}_2^T\\textbf{x}_n \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "y_ny_1\\textbf{x}_n^T\\textbf{x}_1 & \\cdots & y_ny_n\\textbf{x}_n^T\\textbf{x}_n\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "A_D = \\left[\\begin{array}{c}\n",
    "\\textbf{y}^T \\\\\n",
    "-\\textbf{y}^T \\\\\n",
    "\\textbf{I}_{n\\times n}\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "It can be shown, that if $Q_D$ is positive semidefinite, the problem $(21)$ is a convex optimization problem. When the optimal solution $\\boldsymbol\\alpha^*$ for this problem has been solved, we get the parameters $(\\textbf{w}^*, b^*)$ of the optimal SVM hyperplane as (see equations 2 and 20): \n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{w}^* = \\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(23)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "b^* &= \\frac{1}{y_s} - \\textbf{w}^{*T}\\textbf{x}_s\\\\\n",
    "&= \\frac{1}{y_s} -\\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i^T\\textbf{x}_s,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(24)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textbf{x}_s$ is any support vectors satisfying $\\alpha_s^* > 0$. For all non-support vectors it holds that $\\alpha_i^* = 0$. For support vector $\\textbf{x}_s$ it holds that:\n",
    "\n",
    "$$y_s\\left(\\textbf{w}^{*T}\\textbf{x}_s + b^*\\right) = 1.$$ \n",
    "\n",
    "The optimal SVM hyperplane model $(\\textbf{w}^*, b^*)$ is therefore:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "g(\\textbf{x}) &= \\text{sign}\\left(\\textbf{w}^{*T}\\textbf{x} + b^*\\right)\\\\\n",
    "&= \\text{sign}\\left(\\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i^T\\textbf{x} + b^*\\right)\\\\\n",
    "&= \\text{sign}\\left(\\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i^T\\left(\\textbf{x}-\\textbf{x}_s\\right) + y_s\\right).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(25)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Because only the support vectors $\\textbf{x}_s$ affect the selection of the SVM hyperplane $(\\alpha_s^* > 0)$ we can also write the model $(24)$ also only with the support vectors as:\n",
    "\n",
    "\\begin{equation}\n",
    "g(\\textbf{x}) = \\text{sign}\\left(\\sum_{\\alpha^*_i > 0} y_i\\alpha_i^*\\textbf{x}_i^T\\textbf{x} + b^*\\right).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(26)\n",
    "\\end{equation}\n",
    "\n",
    "<a id='4.4'></a>\n",
    "#### 4.4 Example: solving the SVM classifier via the Lagrangian dual form\n",
    "Let the observed data in this example be the same as earlier, that is: \n",
    "\n",
    "\\begin{equation*}\n",
    "X = \\left[\\begin{array}{cc}\n",
    "0 & 0 \\\\\n",
    "2 & 2 \\\\\n",
    "2 & 0 \\\\\n",
    "3 & 0\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{y} = \\left[\\begin{array}{c}\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "+1 \\\\\n",
    "+1\n",
    "\\end{array}\\right].\\\\[6mm]\n",
    "\\end{equation*}\n",
    "\n",
    "Here we have that $n=4$. The matrices $Q_D$ and $A_D$ are identified as: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{rrrr}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 8 & -4 & -6\\\\\n",
    "0 & -4 & 4 & 6\\\\\n",
    "0 & -6 & 6 & 9\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "A_D = \\left[\\begin{array}{rrrr}\n",
    "-1 & -1 & 1 & 1 \\\\\n",
    "1 & 1 & -1 & -1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "By taking advantage of $(22)$ we get our optimization problem as minimizing the Lagrangian function: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\mathcal{L}\\left(\\boldsymbol\\alpha\\right) =\n",
    "  4\\alpha_2^2 + 2\\alpha_3^2 + \\frac{9}{2}\\alpha_4^2 - 4\\alpha_2\\alpha_3 -6\\alpha_2\\alpha_4 + 6\\alpha_3\\alpha_4 -\\alpha_1 -\\alpha_2-\\alpha_3-\\alpha_4 \\\\[2mm]\n",
    "& \\text{subject to:}\n",
    "  \\;\\;\\;\\;\\;\\; \\alpha_1 + \\alpha_2 = \\alpha_3 + \\alpha_4; \\\\\n",
    "& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4 \\geq 0.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "By plugging the first constraint equation into the Lagrangian function we get: \n",
    "\n",
    "$$\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) =\n",
    "  4\\alpha_2^2 + 2\\alpha_3^2 + \\frac{9}{2}\\alpha_4^2 - 4\\alpha_2\\alpha_3 -6\\alpha_2\\alpha_4 + 6\\alpha_3\\alpha_4 -2\\alpha_3-2\\alpha_4.$$\n",
    "  \n",
    "Now we calculate the partial derivative with respect to parameter $\\alpha_2$ and setting this to zero we get:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_2} = 8\\alpha_2-4\\alpha_3-6\\alpha_4 = 0\\;\\;\\;\\;\\leftrightarrow\\;\\;\\;\\; \\alpha_2 = \\frac{1}{2}\\alpha_3 + \\frac{3}{4}\\alpha_4.$$\n",
    "\n",
    "It then follows that:\n",
    "\n",
    "$$\\alpha_1 = \\alpha_3 + \\alpha_4 - \\alpha_2 = \\frac{1}{2}\\alpha_3 +\\frac{1}{4}\\alpha_4.$$\n",
    "\n",
    "Note also that $\\alpha_1, \\alpha_2 \\geq 0$. By plugging $\\alpha_1$ and $\\alpha_2$ into the Lagrangian we get: \n",
    "\n",
    "$$\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) = \\alpha_3^2+\\frac{9}{4}\\alpha_4^2+3\\alpha_3\\alpha_4-2\\alpha_3-2\\alpha_4.$$\n",
    "\n",
    "Next we note that:\n",
    "\n",
    "$$\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) = \\frac{1}{4}\\left(2\\alpha_3 + 3\\alpha_4-2\\right)^2+\\alpha_4-1,$$\n",
    "\n",
    "and so taking into account the constraints $\\alpha_3, \\alpha_4\\geq 0$ we see that the minimum is achieved at the point $\\alpha_3 = 1, \\alpha_4 = 0$ and so: \n",
    "\n",
    "$$\\alpha_1 = \\frac{1}{2}\\;\\;\\;\\;\\text{ja}\\;\\;\\;\\;\\alpha_2 = \\frac{1}{2},$$\n",
    "\n",
    "so the solution is: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\boldsymbol\\alpha^* = \\left[\\begin{array}{c}\n",
    "1/2\\\\\n",
    "1/2\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "Now by using equations $(23)$ and $(24)$ we get:\n",
    "\n",
    "$$\\textbf{w}^*=-\\frac{1}{2}\\textbf{x}_1-\\frac{1}{2}\\textbf{x}_2 + \\textbf{x}_3 = (0, 0) - (1, 1) + (2, 0) = (1, -1),$$\n",
    "\n",
    "$$b^* = -1 + \\textbf{w}^{*T}\\textbf{x}_1 = -1 + 0 = -1,$$\n",
    "\n",
    "where $\\textbf{x}_1, \\textbf{x}_2, \\textbf{x}_3$ are support vectors. According to equation $(25)$ the optimal SVM classifier is therefore:\n",
    "\n",
    "$$g(\\textbf{x}) = \\text{sign}\\left(x_1 -x_2 -1 \\right),$$\n",
    "\n",
    "just like we obtained in the earlier example without using the Lagrangian dual form. ADD PICTURE HERE XXXX\n",
    "\n",
    "<a id='4.5'></a>\n",
    "#### 4.5 The dual in the $\\mathcal{Z}$-space\n",
    "\n",
    "So what part do the kernel methods play in the SVM model? If we incorporate them to the SVM model does the developed theory change much? The answer is no, it is actually very easy to add the kernel methods into the developed SVM models, namely the problem in $(21)$. We simply replace the inner products $\\textbf{x}_i^T\\textbf{x}_j$ with the inner products of space $\\mathcal{Z}$, that is: \n",
    "\n",
    "$$\\textbf{z}_i^T\\textbf{z}_j = \\Phi\\left(\\textbf{x}_i\\right)^T\\Phi\\left(\\textbf{x}_j\\right) = K\\left(\\textbf{x}_i, \\textbf{x}_j\\right),$$\n",
    "\n",
    "and so the problem $(21)$ becomes: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{1}{2}\\sum_{i=1}^n\\sum_{m=1}^ny_iy_m\\alpha_i\\alpha_m K\\left(\\textbf{x}_i, \\textbf{x}_m\\right) - \\sum_{i=1}^n\\alpha_i \\\\\n",
    "& \\text{subject to:}\n",
    "  \\;\\;\\;\\;\\;\\; \\sum_{i=1}^n\\alpha_iy_i = 0 \\\\\n",
    "& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha_i \\geq 0\\;\\;\\;\\;\\;\\;\\;(i = 1, ..., n).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(27)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The matrix $Q_D$ in problem $(22)$ becomes the so-called _kernel matrix_:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{ccc}\n",
    "y_1y_1K\\left(\\textbf{x}_1, \\textbf{x}_1\\right) & \\cdots & y_1y_nK\\left(\\textbf{x}_1, \\textbf{x}_n\\right) \\\\\n",
    "y_2y_1K\\left(\\textbf{x}_2, \\textbf{x}_1\\right) & \\cdots & y_2y_nK\\left(\\textbf{x}_2, \\textbf{x}_n\\right) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "y_ny_1K\\left(\\textbf{x}_n, \\textbf{x}_1\\right) & \\cdots & y_ny_nK\\left(\\textbf{x}_n, \\textbf{x}_n\\right)\n",
    "\\end{array}\\right],\n",
    "\\end{equation*}\n",
    "\n",
    "and so, with the nonlinear transformation $\\Phi$ the SVM model becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "g(\\textbf{x}) = \\text{sign}\\left(\\sum_{\\alpha^*_i > 0} y_i\\alpha_i^*K\\left(\\textbf{x}_i, \\textbf{x}\\right) + b^*\\right).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(28)\n",
    "\\end{equation}\n",
    "\n",
    "<a id='5.'></a>\n",
    "#### 5. Simple algorithm for solving the SVM model\n",
    "\n",
    "Recall that the SVM model was solved by finding the optimal solution to problem: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A_C\\boldsymbol\\alpha \\leq \\textbf{b}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(29)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $Q_D$ is the kernel matrix and\n",
    "\n",
    "\\begin{equation*}\n",
    "A_C = \\left[\\begin{array}{c}\n",
    "\\textbf{y}^T \\\\\n",
    "-\\textbf{y}^T \\\\\n",
    "-\\textbf{I}_{n\\times n}\\\\\n",
    "\\textbf{I}_{n\\times n}\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{b} = \\left[\\begin{array}{c}\n",
    "\\textbf{0}_{n+2}\\\\\n",
    "\\textbf{C}_{n}\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "This problem is equivalent with the problem: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{y}^T\\boldsymbol\\alpha = 0\n",
    "\\\\\n",
    "& & & \\;\\textbf{0} \\leq \\boldsymbol\\alpha \\leq C\\cdot \\textbf{1}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(30)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Next, we will describe the pseudocode of a simple algorithm for solving the SVM model of problem $(30)$. This algorithm is based on active set and gradient projection methods from quadratic programming. This method is simple and not optimized but it is an example the reader of this tutorial can self program. The SVM model is normally solved using more optimized packages such as LIBSVM, which contains the most recent algorithms for solving SVM. LIBSVM-package is based on method based on _sequential minimial optimization_ (SMO) created by John Platt in 1998. The SMO-method solves the parameters of the SMO method by optimizing two coefficients $\\alpha_i$, $\\alpha_j$ at a time while keeping rest of the coefficients constant. The utility of this method is that these coefficients can solved analytically. Also, the SMO method does not suffer from numerical issues like gradient projection method, which with large kernel matrices can result in calculation errors. Due to the limited scope of this tutorial, we will not consider the SMO method here.\n",
    "\n",
    "<a id='5.1'></a>\n",
    "#### 5.1 Active set methods\n",
    "\n",
    "<a id='5.2'></a>\n",
    "#### 5.2 Gradient projection\n",
    "\n",
    "<a id='5.3'></a>\n",
    "#### 5.3 Pseudocode for simple SVM solver \n",
    "\n",
    "\n",
    "\\subsection{Aktiivijoukkomenetelmät}\n",
    "\n",
    "Aktiivijoukkomenetelmien tarkoituksena on pitää kirjaa optimointitehtävän niin sanotuista \\textit{aktiivirajoitteista} ja suorittaa optimointi näiden aktiivirajoitteiden määrittelemässä aliavaruudessa. TKV-luokittelijan optimointiongelman rajoitteet muodostavat konveksin suljetun aliavaruuden, jossa optimointi suoritetaan. Niin sanotuista \\textit{ei-aktiivisista rajoitteista} ei tarvitse välittää optimointia suoritettaessa kunnes jotkin ei-aktiiviset rajoitteet muuttuvat aktiivisiksi. TKV-optimointitehtävän rajoitteet ovat epäyhtälörajoitteita $A\\textbf{x} \\leq \\textbf{y}$, jotka ovat aktiivisia pisteessä $\\textbf{x}$ jos $A\\textbf{x} = \\textbf{y}$ ja ei-aktiivisia jos $A\\textbf{x} < \\textbf{y}$. Optimoitaessa funktiota $f(\\textbf{x})$ ei siis tarvitse välittää kuin aktiivisista rajoitteista. Aktiivirajoiteiden joukkoa päivitetään sen mukaan kun aktiiviset rajoitteet muuttuvat ei-aktiivisiksi tai toisinpäin.  \n",
    "\n",
    "\\subsection{Gradienttiprojektio}\n",
    "Gradienttiprojektio perustuu gradienttilaskeutumiseen jossa funktion $f(\\textbf{x})$ minimointi suoritetaan funktion $f(\\textbf{x})$ projisoidun negatiivisen gradientin $-P\\,\\nabla f(\\textbf{x})$ suunnassa, missä $P$ on symmetrinen positiivisemidefinitti projektiomatriisi. TKV-luokittelijan optimointi tapahtuu siis iteratiivisesti suorittamalla jokaisella iteraatiolla viivahaku projisoidun gradientin suunnassa. Projektiomatriisi $P$ määrätään kullakin iteraatiolla uudestaan riippuen, mitkä epäyhtälörajoitteista  ovat aktiivisia. Seuraavassa kappaleessa esitämme algoritmin yleisen (eli pehmeän marginaalin) TKV-luokittelijan ratkaisuun. Algoritmin ydin perustuu projisoituun gradienttilaskeutumiseen.\n",
    "\n",
    "\\subsection{Algoritmi yleisen SVM-mallin ratkaisuun}\n",
    "\n",
    "\n",
    "\\begin{algorithm}\n",
    "  \\caption{TKV-luokittelijan optimointi gradienttiprojektiolla\n",
    "    \\label{alg:packed-dna-hamming}}\n",
    "    \n",
    "   \\begin{spacing}{1.3}\n",
    "  \\begin{algorithmic}[1]\n",
    "  \\renewcommand{\\algorithmicrequire}{\\textbf{Syötteet:}}\n",
    "  \\renewcommand{\\algorithmicensure}{\\textbf{Palaute:}}\n",
    "    \\Require{$i\\leftarrow0$, $\\boldsymbol\\alpha_i\\leftarrow\\textbf{0}$, $A_i = \\textbf{y}^T$, $\\varepsilon\\in \\mathbb{R}^+$, $C\\in\\mathbb{R}^+$}\n",
    "    \\Ensure{$\\text{Optimaaliratkaisu} \\; \\boldsymbol\\alpha^*$}\n",
    "    \\Statex\n",
    "    \\Do\n",
    "    \t\\State $\\textbf{g}_i = \\boldsymbol\\alpha_i^TQ_D-\\textbf{1}$ \\Comment{Kohdefunktion gradientti}\n",
    "        \\State $P_i = I-A_i^T\\left(A_iA_i^T\\right)^{-1}A_i$ \\Comment{Projektiomatriisi}\n",
    "        \\State $\\textbf{d}_i = -P_i\\textbf{g}_i$ \n",
    "        \\State $I_s = \\left\\{j\\in\\{1,...,n\\} \\biggr|\\left(\\boldsymbol\\alpha_i^{(j)}=0\\;\\text{and}\\;\\textbf{d}_i^{(j)}<0\\right)\\text{or}\\left(\\boldsymbol\\alpha_i^{(j)}=C\\;\\text{and}\\;\\textbf{d}_i^{(j)}>0\\right)\\right\\}$ \n",
    "\n",
    "\\If{$I_s \\neq \\emptyset$} \n",
    "\t\\State $M = \\textbf{0}_{\\left|I_s\\right|\\times n}$\n",
    "    \\For{$k=1, ..., \\left|I_s\\right|$}\n",
    "    \t\\State $M_{k, I_s(k)} = 1$\n",
    "    \\EndFor\n",
    " %   \\begin{spacing}{1}\n",
    "    \t\\State $A_i=\\left[ \\begin{array}{c} \\textbf{y}^T \\\\ M \\end{array} \\right]$\n",
    " %   \\end{spacing}\n",
    "\\State $P_i = I-A_i^T\\left(A_iA_i^T\\right)^{-1}A_i$\n",
    "        \\State $\\textbf{d}_i = -P_i\\textbf{g}_i$ \\Comment{Projisoitu hakusuunta}\n",
    "\\EndIf\n",
    "\n",
    "\\State $\\lambda^+ =\\displaystyle\\frac{\\textbf{g}_i^T\\textbf{d}_i}{\\textbf{d}_i^TQ_D\\textbf{d}_i}$ \\Comment{Viivahaku}\n",
    "\n",
    "\\State $\\lambda_l = \\displaystyle\\underset{j\\in\\{1,...,n\\}}{\\min} \\;-\\frac{\\boldsymbol\\alpha_i^{(j)}}{\\textbf{d}_i^{(j)}}$\n",
    "\n",
    "\\State $\\lambda_u = \\displaystyle\\underset{j\\in\\{1,...,n\\}}{\\min} \\;\\frac{C-\\boldsymbol\\alpha_i^{(j)}}{\\textbf{d}_i^{(j)}}$\n",
    "\n",
    "\\State $\\lambda^* = \\displaystyle \\max\\left\\{0, \\min\\left\\{\\lambda^+, \\lambda_l, \\lambda_u\\right\\}\\right\\}$ \\Comment{Optimaalinen askel}\n",
    "\n",
    "\\State $i = i + 1$\n",
    "\n",
    "\\State $\\boldsymbol\\alpha_i = \\boldsymbol\\alpha_{i-1}+\\lambda^*\\textbf{d}_{i-1}$\n",
    "\n",
    "\\doWhile{$\\left|f\\left(\\boldsymbol\\alpha_i\\right)-f\\left(\\boldsymbol\\alpha_{i-1}\\right)\\right|\\geq\\varepsilon$} \\Comment{Lopetusehto}\n",
    "    \n",
    "\\State $\\boldsymbol\\alpha^*=\\boldsymbol\\alpha_i$ \n",
    "   \n",
    " %   \\Function{Distance}{$x, y$}\n",
    " %     \\Let{$z$}{$x \\oplus y$} \\Comment{$\\oplus$: bitwise exclusive-or}\n",
    " %     \\Let{$\\delta$}{$0$}\n",
    " %     \\For{$i \\gets 1 \\textrm{ to } n$}\n",
    " %       \\If{$z_i \\neq 0$}\n",
    " %         \\Let{$\\delta$}{$\\delta + 1$}\n",
    " %       \\EndIf\n",
    " %     \\EndFor\n",
    " %     \\State \\Return{$\\delta$}\n",
    " %   \\EndFunction\n",
    "    \n",
    "  \\end{algorithmic}\n",
    "  \\end{spacing}\n",
    "\\end{algorithm}\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\begin{figure}\n",
    "\\includegraphics[scale=1]{alphas.pdf}\n",
    "\\caption{Gradienttiprojektion eteneminen pehmeän marginaalin TKV-esimerkkiongelmassa, missä $C=2$ ja $\\varepsilon = 10^{-3}$. Optimaalinen ratkaisu on $\\boldsymbol\\alpha^* = (\\alpha_1^*, \\alpha_2^*, \\alpha_3^*) = (1/2, 1/2, 1)$. Sininen laatikko kuvaa $\\alpha$-parametrien laatikkorajoitteita (eng. box constraints) ja punainen taso taas kuvaa rajoitetta $\\textbf{y}^T\\boldsymbol\\alpha = 0.$}\n",
    "\\label{GradProjExample}\n",
    "\\end{figure}\n",
    "\n",
    "Kuvassa \\ref{GradProjExample} nähdään gradienttiprojektion eteneminen lineaarisen kernel-funktion TKV-esimerkkiongelmassa, missä\n",
    "\n",
    "\\begin{equation*}\n",
    "X = \\left[\\begin{array}{cc}\n",
    "0 & 0 \\\\\n",
    "2 & 2 \\\\\n",
    "2 & 0 \n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{y} = \\left[\\begin{array}{c}\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "+1 \n",
    "\\end{array}\\right].\\\\[6mm]\n",
    "\\end{equation*}\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\textbf{Teoreema 1.} Olkoon minimointitehtävän kvadraattinen kohdefunktio $f(\\textbf{x}) = \\textbf{c}^T\\textbf{x} + \\frac12\\textbf{x}^TH\\textbf{x}$, missä $\\textbf{x}\\in X$ ja $X \\subseteq \\mathbb{R}^n$ on suljettu konveksi joukko. Suoritetaan projisoitu konjugaattihaku käyttämällä kaavaa: \n",
    "$$\\textbf{x}_{k+1} = \\textbf{x}_k+\\lambda_k\\textbf{d}_k$$\n",
    ", missä\n",
    "$$\\textbf{d}_{k+1} = -P\\,\\nabla f(\\textbf{x}_k) + \\alpha_k\\textbf{d}_k.$$\n",
    "\n",
    "Matriisi $P$ on symmetrinen positiividefiniitti \\textit{projektiomatriisi}, $\\lambda_k \\geq 0$ on tehtävän $\\min f(\\textbf{x}_k+\\lambda_k\\textbf{d}_k)$ optimiratkaisu, $\\textbf{d}_1 = -P\\,\\nabla f(\\textbf{x}_k)$ ja \n",
    "\n",
    "$$\\alpha_k = \\frac{\\nabla f(\\textbf{x}_{k+1})^TPH\\textbf{d}_k}{\\textbf{d}_k^TH\\textbf{d}_k}.$$\n",
    "\n",
    "Tällöin kaikille $k=1,...,n$ pätee\n",
    "\n",
    "\\begin{enumerate}\n",
    "\\item Suunnat $\\textbf{d}_1, ..., \\textbf{d}_n$ ovat $H$-konjugaatteja\n",
    "\\item Suunnat $\\textbf{d}_1, ..., \\textbf{d}_n$ ovat funktion $f$ laskusuuntia\n",
    "\\end{enumerate}\n",
    "\n",
    "TODISTUS: \n",
    "\n",
    "\n",
    "$$\\textbf{x}_{i+1} = \\textbf{x}_i+\\lambda_i\\textbf{d}_i,\\;\\;\\;\\;\\textbf{d}_{i}=\\lambda_i^{-1}(\\textbf{x}_{i+1}-\\textbf{x}_i),\\;\\;\\;\\;\\nabla f(\\textbf{x}_{i})=\\textbf{c}+H\\textbf{x}_{i}=\\textbf{g}_{i}$$\n",
    "\n",
    "$$\\textbf{d}_{i+1}=-P\\,\\nabla f(\\textbf{x}_{i+1})+\\alpha_i\\textbf{d}_{i}$$\n",
    "\n",
    "$$\\textbf{d}_{i+1}^TH\\textbf{d}_{i} = \\left(-P\\,\\nabla f(\\textbf{x}_{i+1})+\\alpha_i\\textbf{d}_{i}\\right)^TH\\textbf{d}_{i}=0\\rightarrow$$\n",
    "\n",
    "$$\\alpha_i = \\frac{\\nabla f(\\textbf{x}_{i+1})^TPH\\textbf{d}_{i}}{\\textbf{d}_{i}^TH\\textbf{d}_{i}}$$\n",
    "\n",
    "$$\\textbf{d}_{i}=-P\\,\\nabla f(\\textbf{x}_{i})+\\alpha_{i-1}\\textbf{d}_{i-1}$$\n",
    "$$H\\textbf{d}_{i} = H\\left(\\lambda_i^{-1}(\\textbf{x}_{i+1}-\\textbf{x}_{i})\\right) = \\lambda_i^{-1}\\left(\\nabla f(\\textbf{x}_{i+1})-\\nabla f(\\textbf{x}_{i})\\right)$$\n",
    "\n",
    "$$\\textbf{d}_{i}^TH\\textbf{d}_{i} = \\lambda_i^{-1}\\textbf{d}_{i}^T\\left(\\nabla f(\\textbf{x}_{i+1})-\\nabla f(\\textbf{x}_{i})\\right)$$\n",
    "\n",
    "$$\\nabla f(\\textbf{x}_{i+1})^T\\textbf{d}_{k} = 0\\;\\;\\;\\text{for}\\;\\;k=1,...,i$$\n",
    "\n",
    "$$\\textbf{d}_{i}^TH\\textbf{d}_{i} = -\\lambda_i^{-1}\\textbf{d}_{i}^T\\nabla f(\\textbf{x}_{i}) = -\\lambda_i^{-1}\\left(-P\\,\\nabla f(\\textbf{x}_{i})+\\alpha_{i-1}\\textbf{d}_{i-1}\\right)^T\\nabla f(\\textbf{x}_{i})=\\lambda_i^{-1}\\nabla f(\\textbf{x}_{i})^TP\\,\\nabla f(\\textbf{x}_{i}) = \\lambda_i^{-1}\\textbf{g}_{i}^TP\\textbf{g}_{i}$$\n",
    "\n",
    "$$\\alpha_i = \\frac{\\textbf{g}_{i+1}^TP\\textbf{q}_{i}}{\\textbf{g}_{i}^TP\\textbf{g}_{i}}$$\n",
    "\n",
    "$$\\textbf{q}_{i}=\\textbf{g}_{i+1}-\\textbf{g}_{i}$$\n",
    "\n",
    "1. Conjugate, 2. Linearly independent:\n",
    "\n",
    "$$\\textbf{d}_{i+1}^TH\\textbf{d}_{i}=\\left(-P\\,\\textbf{g}_{i+1}+\\frac{\\textbf{g}_{i+1}^TPH\\textbf{d}_{i}}{\\textbf{d}_{i}^TH\\textbf{d}_{i}}\\textbf{d}_{i}\\right)^TH\\textbf{d}_{i} = -\\textbf{g}_{i+1}^TPH\\textbf{d}_{i}+\\frac{\\textbf{g}_{i+1}^TPH\\textbf{d}_{i}}{\\textbf{d}_{i}^TH\\textbf{d}_{i}}\\textbf{d}_{i}^TH\\textbf{d}_{i}=0$$\n",
    "\n",
    "$$\\textbf{d}_{i+1}^TH\\textbf{d}_{k}=0\\;\\;\\;\\;\\text{for}\\;k <i\\;\\;?$$\n",
    "\n",
    "Assume by induction hypothesis that $\\textbf{d}_{i}^TH\\textbf{d}_{k} = 0\\;\\;\\;\\text{for}\\;k<i$\n",
    "\n",
    "$$\\textbf{d}_{i+1}^TH\\textbf{d}_{k} = \\left(-P\\textbf{g}_{i+1}+\\alpha_i\\textbf{d}_{i}\\right)^TH\\textbf{d}_{k}=-\\textbf{g}_{i+1}^TPH\\textbf{d}_{k}$$\n",
    "\n",
    "$$\\textbf{d}_{k+1}= -P\\textbf{g}_{k+1}+\\alpha_k\\textbf{d}_{k} = -P\\left(\\textbf{g}_{k}+\\lambda_kH\\textbf{d}_{k}\\right)+\\alpha_k\\textbf{d}_{k} = -P\\textbf{g}_{k}-\\lambda_kPH\\textbf{d}_{k}+\\alpha_k\\textbf{d}_{k} $$\n",
    "\n",
    "$$=\\textbf{d}_{k}-\\alpha_{k-1}\\textbf{d}_{k-1} -\\lambda_kPH\\textbf{d}_{k}+\\alpha_k\\textbf{d}_{k}$$\n",
    "\n",
    "Beacuse $\\textbf{d}_{k}$ is descent direction because of induction hypothesis we get $\\lambda_k > 0$, so: \n",
    "\n",
    "$$PH\\textbf{d}_{k} = -\\lambda_k^{-1}\\left(\\textbf{d}_{k+1}-(1+\\alpha_k)\\textbf{d}_{k}+\\alpha_{k-1}\\textbf{d}_{k-1}\\right)$$\n",
    "\n",
    "From which we get: \n",
    "\n",
    "$$\\textbf{d}_{i+1}^TH\\textbf{d}_{k} = -\\textbf{g}_{i+1}^TPH\\textbf{d}_{k}=\\lambda_k^{-1}\\textbf{g}_{i+1}^T\\left(\\textbf{d}_{k+1}-(1+\\alpha_k)\\textbf{d}_{k}+\\alpha_{k-1}\\textbf{d}_{k-1}\\right)$$\n",
    "\n",
    "Because directions $\\textbf{d}_{1}, ..., \\textbf{d}_{i}$ are conjugate directions we get: \n",
    "\n",
    "$$\\textbf{g}_{i+1}^T\\textbf{d}_{k+1}=\\textbf{g}_{i+1}^T\\textbf{d}_{k}=\\textbf{g}_{i+1}^T\\textbf{d}_{k-1}=0$$\n",
    "\n",
    "Which means \n",
    "\n",
    "$$\\textbf{d}_{i+1}^TH\\textbf{d}_{k}=0 \\;\\;\\;\\text{for}\\;k\\leq i$$\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\n",
    "\n",
    "\\begin{thebibliography}{9}\n",
    "\n",
    "\\bibitem{Mostafa}\n",
    "  Y.S. Abu-Mostafa, M. Magdon-Ismail, H.-T Lin. \\emph{Learning From Data}. California Institute of Technology, 2012.\n",
    "  \n",
    "\\bibitem{Boyd}\n",
    " S. Boyd, L. Vandenberghe. \\emph{Convex Optimization}. Cambridge University Press, 2009.\n",
    "\n",
    "\\bibitem{Luenberger}\n",
    " D.G. Luenberger, Y. Ye. \\emph{Linear and Nonlinear programming}. Springer, 3rd edition, 2008.\n",
    "\n",
    "\\bibitem{Makela1}\n",
    " M.M. Mäkelä. \\emph{Konveksi analyysi ja optimointi}. Turun yliopisto, matematiikan laitos, 2014.\n",
    " \n",
    "\\bibitem{Makela2}\n",
    " M.M. Mäkelä. \\emph{Optimointialgoritmit}. Turun yliopisto, matematiikan laitos, 2014.\n",
    "\n",
    "\\bibitem{Vapnik}\n",
    "  V.N. Vapnik. \\emph{Statistical Learning Theory}. Jon Wiley \\& Sons Inc., 1998.\n",
    "\\end{thebibliography}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{comment}\n",
    "\n",
    "\\\\[4mm] Todistus: Olkoon $\\textbf{u}_0$ optimaaliratkaisu primaalitehtävälle \\ref{quadprog}\n",
    " ja $\\textbf{u}_1$ optimaaliratkaisu tehtävälle \\ref{opteq4}. Koska $\\textbf{u}_0$ on ratkaisu tehtävälle \\ref{quadprog} saadaan että\n",
    " \n",
    "$$c_m - \\textbf{a}_m^T\\textbf{u}_0 \\leq 0\\;\\;\\;\\; m = 1, ..., M,$$\n",
    "\n",
    "jolloin saadaan että \n",
    "\n",
    "$$\\max_{\\alpha_m \\geq 0} \\alpha_m \\left(c_m - \\textbf{a}_m^T\\textbf{u}_0 \\right) = 0\\;\\;\\;\\; m = 1, ..., M,$$\n",
    "\n",
    "eli \n",
    "\n",
    "$$\\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\sum_{m=1}^M \\alpha_m \\left( c_m - \\textbf{a}_m^T\\textbf{u} \\right) = 0.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "#### Appendix: code listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name :  hah , Salary:  10\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#######################################\n",
    "# - Used to plot the example figures. #\n",
    "#######################################\n",
    "def drawClassifierLines(showRadius):\n",
    "    # Four data points from two different classes.\n",
    "    x = [0, 2, 2, 3]\n",
    "    y = [0, 2, 0, 0]\n",
    "    # Three classifier lines.\n",
    "    lines = [[-1, 4, -2, 2.2], [-1, 3.4, -2, 3], [-1, 4, -2, 3]]\n",
    "    # Three subplots.\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(20,5))\n",
    "    # Set plot limits.\n",
    "    xlim = [-2, 4]\n",
    "    ylim = [-2, 3]\n",
    "    # Plot all data points.\n",
    "    for i in range(0, len(axarr)):\n",
    "        axarr[i].plot(x[0:2], y[0:2], \"x\", color=[0, 0, .8], markersize=15, markeredgewidth=3)\n",
    "        axarr[i].plot(x[2:4], y[2:4], \"o\", color=[.8, 0, 0], markersize=15, markeredgewidth=3)\n",
    "        axarr[i].set_xlim(xlim)\n",
    "        axarr[i].set_ylim(ylim)\n",
    "        axarr[i].plot(lines[i][0:2], lines[i][2:4], 'k-')\n",
    "        axarr[i].set_xticks([])\n",
    "        axarr[i].set_yticks([])\n",
    "    # Draw uncertainty circles.\n",
    "    if showRadius:\n",
    "        rad = [0.4, 0.4, 1/np.sqrt(2)]\n",
    "        for j in range(0, len(axarr)):\n",
    "            for i in range(0, len(x)):\n",
    "                circle1 = plt.Circle((x[i], y[i]), rad[j], color=[.3,.3,.3], clip_on=False, alpha=0.5)\n",
    "                axarr[j].add_patch(circle1)\n",
    "    # Plot the hard coded decision areas.\n",
    "    axarr[0].fill([-1, -2, -2, 4, 4, -1], [-2, -2, 3, 3, 2.2, -2], color = [0, 0, 1], alpha=0.2)\n",
    "    axarr[0].fill([-1, 4, 4, -1], [-2, 2.2, -2, -2], color = [1, 0, 0], alpha=0.2)\n",
    "    axarr[1].fill([-1, -2, -2, 3.4, -1], [-2, -2, 3, 3, -2], color = [0, 0, 1], alpha=0.2)\n",
    "    axarr[1].fill([-1, 3.4, 4, 4, -1], [-2, 3, 3, -2, -2], color = [1, 0, 0], alpha=0.2)\n",
    "    axarr[2].fill([-1, -2, -2, 4, -1], [-2, -2, 3, 3, -2], color = [0, 0, 1], alpha=0.2)\n",
    "    axarr[2].fill([-1, 4, 4, -1], [-2, 3, -2, -2], color = [1, 0, 0], alpha=0.2)\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# SVM model class will be created here. Contains e.g. fit-function, kernel.     \n",
    "class Employee:\n",
    "    empCount = 0\n",
    "\n",
    "    def __init__(self, name, salary):\n",
    "        self.name = name\n",
    "        self.salary = salary\n",
    "        Employee.empCount += 1\n",
    "   \n",
    "    def displayCount(self):\n",
    "        print(\"Total Employee %d\" % Employee.empCount)\n",
    "\n",
    "    def displayEmployee(self):\n",
    "        print(\"Name : \", self.name,  \", Salary: \", self.salary)\n",
    "        \n",
    "svmModel = Employee(\"hah\", 10)\n",
    "svmModel.displayEmployee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE, SANDBOX\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Latex\n",
    "\n",
    "#1. [Machine learning](#intro)<br>\n",
    "#    1.1 [Part A](#pA)<br>\n",
    "#    1.2 [Part B](#pB)<br>\n",
    "#    1.3 [How to find the maximum margin model?](#3.1)<br>\n",
    "#2. [Main](#main)<br>\n",
    "\n",
    "\n",
    "Latex(r\"\"\"\n",
    "\\text{FF}\"\"\")\n",
    "\n",
    "Latex(r\"\"\"\\begin{eqnarray}\n",
    "\\nabla \\times \\vec{\\mathbf{B}} -\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{E}}}{\\partial t} & = \\frac{4\\pi}{c}\\vec{\\mathbf{j}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{E}} & = 4 \\pi \\rho \\\\\n",
    "\\nabla \\times \\vec{\\mathbf{E}}\\, +\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{B}}}{\\partial t} & = \\vec{\\mathbf{0}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{B}} & = 0 \n",
    "\\end{eqnarray}\"\"\")\n",
    "\n",
    "def hello():\n",
    "    print(\"hello!\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function [Qd, kmatrix] = kernel(ktype, params, data, labels)\n",
    "\n",
    "Qd = [];\n",
    "kmatrix = [];\n",
    "L = kron(labels', labels);\n",
    "\n",
    "if strcmp(ktype, 'linear')\n",
    "    K = data*data';\n",
    "    Qd = L.*K;\n",
    "    kmatrix = K;\n",
    "elseif strcmp(ktype, 'poly')\n",
    "    K = (params(1) + params(2)*(data*data')).^params(3);\n",
    "    Qd = L.*K;\n",
    "    kmatrix = K;\n",
    "elseif strcmp(ktype, 'gaussian')\n",
    "    % TODO\n",
    "else\n",
    "    error('Unidentified kernel selection!')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function [weights, alphas, iters] = solveSVM(data, labels, C, e, ktype, params)\n",
    "% FUNCTION [weights, alphas, iters] = solveSVM(data, labels, C, e)\n",
    "% \n",
    "% AUTHOR: Jonne Pohjankukka, 2015\n",
    "%\n",
    "% VERSION: 1.0\n",
    "%\n",
    "% DESCRIPTION: \n",
    "% - This function will attempt to solve the optimal weights for a Support\n",
    "% Vector Machines (SVM) model using active set method with gradient\n",
    "% projection.\n",
    "% \n",
    "% INPUTS: \n",
    "% \"data\" a n-by-m data matrix. The number of rows 'n' corresponds to the\n",
    "% number of data points and the number of columns 'm' corresponds to the\n",
    "% number of variables. \n",
    "% \"labels\" a 1-by-n row vector of data labels from the set {-1,1}. \n",
    "% \"C\" Box costraint upper limit. This will constrain the values of 'alphas'\n",
    "% to the range 0 <= alphas <= C. If hard-margin SVM model is required set\n",
    "% C=Inf. \n",
    "% \"e\" a real value corresponding to the convergence criterion, that is if\n",
    "% solution Xi and Xi-1 are within distance 'e' from each other stop the\n",
    "% learning process, i.e. IF |F(Xi)-F(Xi-1)| < e ==> stop learning process.\n",
    "%\n",
    "% OUTPUTS: \n",
    "% \"weights\" a vector corresponding to the optimal decision line parameters.\n",
    "% \"alphas\" a vector of alpha-values corresponding to the optimal solution\n",
    "% of the dual optimization problem of SVM.\n",
    "% \"iters\" number of iterations until learning stopped.\n",
    "%\n",
    "% EXAMPLE USAGE 1: \n",
    "% \n",
    "% 'Hard-margin SVM': \n",
    "%\n",
    "% data = [0 0;2 2;2 0;3 0];\n",
    "% labels = [-1 -1 1 1];\n",
    "% [weights, alphas, iters] = solveSVM(data, labels, Inf, 10^-100)\n",
    "% \n",
    "% EXAMPLE USAGE 2:\n",
    "%\n",
    "% 'Soft-margin SVM':\n",
    "%\n",
    "% data = [0 0;2 2;2 0;3 0];\n",
    "% labels = [-1 -1 1 1];\n",
    "% [weights, alphas, iters] = solveSVM(data, labels, 0.8, 10^-100)\n",
    "\n",
    "\n",
    "% STEP 1: INITIALIZATION OF THE PROBLEM\n",
    "format long g\n",
    "\n",
    "[Qd, kmatrix] = kernel(ktype, params, data, labels);\n",
    "\n",
    "% The minimization function \n",
    "L = @(a) ((1/2)*a'*Qd*a - ones(1, length(a))*a)*(1/10);\n",
    "% Gradient of the minimizable function \n",
    "gL = @(a) (a'*Qd - ones(1, length(a)))*(1/10);\n",
    "\n",
    "\n",
    "% STEP 2: THE LEARNING PROCESS, ACTIVE SET WITH GRADIENT PROJECTION\n",
    "\n",
    "% Initial feasible solution (required by gradient projection)\n",
    "x = zeros(length(labels), 1);\n",
    "iters = 1;\n",
    "optfound = 0;\n",
    "\n",
    "solmat = [x'];\n",
    "while optfound == 0 % criterion met    \n",
    "    \n",
    "    % Negative of the gradient at initial solution\n",
    "    g = -gL(x);\n",
    "    g = g/norm(g);\n",
    "    % Set the active set and projection matrix\n",
    "    Aq = labels; % In plane y^Tx = 0\n",
    "    P = eye(length(x))-Aq'*inv(Aq*Aq')*Aq; % In plane projection\n",
    "    % Values smaller than 'eps' are changed into 0\n",
    "    P(abs(P) < eps) = 0;\n",
    "    d = P*g'; % Projection onto plane\n",
    "    % Values smaller than 'eps' are changed into 0\n",
    "    d(abs(d) < eps) = 0;\n",
    "    if ~isempty(x(x==0 | x==C)) % Constraints active? If yes, we need to change projection matrix\n",
    "        acinds = find(x==0 | x==C);\n",
    "        constMat = eye(length(x));\n",
    "        aclog = zeros(length(x), 1);\n",
    "        for i = 1:length(acinds)\n",
    "            if (x(acinds(i)) == 0 && d(acinds(i)) < 0) || x(acinds(i)) == C && d(acinds(i)) > 0\n",
    "                % Make the constraint vector\n",
    "                aclog(acinds(i)) = 1;\n",
    "            end\n",
    "        end\n",
    "        if sum(aclog) > 0 % Meaning we need new projection matrix and search direction\n",
    "            Aq = [labels; constMat(logical(aclog),:)];\n",
    "            % Update the projection matrix\n",
    "            P = eye(length(x))-Aq'*inv(Aq*Aq')*Aq; % In plane / box projection\n",
    "            % Values smaller than 'eps' are changed into 0\n",
    "            P(abs(P) < eps) = 0;\n",
    "            d = P*g'; % Projection onto plane / border\n",
    "            % Values smaller than 'eps' are changed into 0\n",
    "            d(abs(d) < eps) = 0;\n",
    "        end\n",
    "    end\n",
    "    d = d/norm(d);\n",
    "    % Line search for optimal step size into search direction 'd'\n",
    "    if ~isempty(d(d~=0)) && rank(Aq) < length(x) \n",
    "        lopt = ((g*d)/(d'*Qd*d));       \n",
    "        uplamlims = [(C-x)./d; -x./d];\n",
    "        lmax = min(uplamlims(uplamlims > 0));\n",
    "        lambda = max(0, min([lopt, lmax]));\n",
    "        if abs(lambda) < eps\n",
    "            lambda = 0;\n",
    "        end\n",
    "        x_old = x;\n",
    "        x = x + lambda*d;\n",
    "        solmat = [solmat; x'];\n",
    "    end\n",
    "    \n",
    "     %%%% DISPLAY INFORMATION, THIS PART IS NOT NECESSAY, ONLY FOR DEBUGGING\n",
    "    \n",
    "    if Aq*x ~= 0 & 1 == 1\n",
    "        disp('ACTIVE SET CONSTRAINTS Aq :')\n",
    "        Aq\n",
    "        disp('CURRENT SOLUTION x :')\n",
    "        x\n",
    "        disp('MULTIPLICATION OF Aq and x')\n",
    "        Aq*x\n",
    "    end\n",
    "    \n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    \n",
    "    % Check whether search direction is 0-vector or 'e'-criterion met.\n",
    "    if isempty(d(d~=0)) || abs(L(x)-L(x_old)) < e\n",
    "        optfound = 1;\n",
    "    end\n",
    "    iters = iters + 1;\n",
    "    if iters > 10000 % Stop if solution hasn't converged until 100000 iterations\n",
    "        disp('No convergence after 10000 iterations, learning stopped')\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "%%% STEP 3: GET THE MODEL\n",
    "alphas = x;\n",
    "alphas(abs(alphas) < eps) = 0;\n",
    "w = zeros(1, length(data(1,:)));\n",
    "for i = 1:size(data,1)\n",
    "    w = w + labels(i)*alphas(i)*data(i,:);\n",
    "end\n",
    "svinds = find(alphas>0); \n",
    "svind = svinds(1);\n",
    "b = 1/labels(svind) - w*data(svind, :)';\n",
    "\n",
    "\n",
    "\n",
    "%%% STEP 4: OPTIMALITY CHECK, KKT conditions. See KKT-conditions for reference.\n",
    "weights = [b; w'];\n",
    "datadim = length(data(1,:));\n",
    "Q = [zeros(1,datadim+1); zeros(datadim, 1), eye(datadim)];\n",
    "A = [ones(size(data,1), 1), data];\n",
    "for i = 1:length(labels)\n",
    "    A(i,:) = A(i,:)*labels(i);\n",
    "end\n",
    "LagDuG = Q*weights - A'*alphas;\n",
    "Ac = A*weights - ones(length(labels),1);\n",
    "alpA = alphas.*Ac;\n",
    "LagDuG(any(abs(LagDuG-0) < 10^-14)) = 0;\n",
    "if ~any(alphas < 0) && all(LagDuG == zeros(datadim+1,1)) && all(abs(Ac) >= 0) && all(abs(alpA) < 10^-6)\n",
    "    disp('Optimal found, Karush-Kuhn-Tucker conditions satisfied.')\n",
    "elseif C < Inf\n",
    "    disp('Soft-margin SVM solution found.')\n",
    "else\n",
    "    disp('Optimal not found, Karush-Kuhn-Tucker conditions not satisfied.')\n",
    "    disp('This is caused because of numerical issues, accuracy of calculations...')\n",
    "end\n",
    "\n",
    "\n",
    "ainds = find(alphas > 0);\n",
    "bpoints = [];\n",
    "for i = min(data(:,1))-1:0.01:max(data(:,1))+1\n",
    "    for j = min(data(:,2))-1:0.01:max(data(:,2))+1\n",
    "        d = [i,j];\n",
    "        signal = 0;\n",
    "        for k = 1:length(ainds)\n",
    "            Kv = (params(1) + params(2)*(data(ainds(k), :)*d')).^params(3);\n",
    "            signal = signal + labels(ainds(k))*alphas(ainds(k))*Kv;\n",
    "        end\n",
    "        signal = signal + b;\n",
    "        if abs(signal) <= 0.005\n",
    "            bpoints = [bpoints; d];\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "%subplot(1,2,1)\n",
    "% VISUALIZATION FOR 2D-CASE\n",
    "if size(data, 2) == 2 \n",
    "    \n",
    "    % Draw the margin \n",
    "    Xb = min(data(:,1))-1;\n",
    "    Xe = max(data(:,1))+1;\n",
    "    Yb = -(b+w(1)*Xb)/w(2);\n",
    "    Ye = -(b+w(1)*Xe)/w(2);\n",
    "    tvec = [alphas, labels'];\n",
    "    pSupInds = find(tvec(:,1) > 0 & tvec(:,2) == 1);\n",
    "    nSupInds = find(tvec(:,1) > 0 & tvec(:,2) == -1);\n",
    "    pSups = data(pSupInds, :);\n",
    "    nSups = data(nSupInds, :);\n",
    "    supV = nSups(1,:);\n",
    "    Yline = (-b-w(1)*supV(1))/w(2)\n",
    "    Ydif = abs(supV(2)-Yline);\n",
    "    Ylb = (-b-w(1)*Xb)/w(2);\n",
    "    Yle = (-b-w(1)*Xe)/w(2);\n",
    "    %upM = fill([Xb, Xb, Xe, Xe], [Ylb Ylb+Ydif Yle+Ydif Yle], [252/255, 255/255, 173/255]); \n",
    "    %hold on\n",
    "    %loM = fill([Xb, Xb, Xe, Xe], [Ylb Ylb-Ydif Yle-Ydif Yle], [252/255, 255/255, 173/255]);\n",
    "    %alpha(upM, 0.8)\n",
    "    %alpha(loM, 0.8)\n",
    "    \n",
    "    Xl = min(data(:,1))-1;\n",
    "    Xm = max(data(:,1))+1;\n",
    "    Yl = min(data(:,2))-1;\n",
    "    Ym = max(data(:,2))+1;\n",
    "    \n",
    "    minY = min(bpoints(:,2));\n",
    "    maxY = max(bpoints(:,2));\n",
    "    minX = min(bpoints(:,1));\n",
    "    maxX = max(bpoints(:,1));\n",
    "    bot = bpoints(find(bpoints(:,2)==minY), :);\n",
    "    maxXb = max(bot(:,1));\n",
    "    p1 = bpoints(find(bpoints(:,1) <= 0 & bpoints(:,2) >= 0), :);\n",
    "    p2 = bpoints(find(bpoints(:,1) > 0 & bpoints(:,2) >= 0), :);\n",
    "    p3 = bpoints(find(bpoints(:,1) > maxXb & bpoints(:,2) < 0 & bpoints(:,2) >= minY), :);\n",
    "    p4 = bpoints(find(bpoints(:,1) <= maxXb & bpoints(:,2) < 0), :);\n",
    "    p1 = sortrows(p1, 2);\n",
    "    p2 = flipud(sortrows(p2, 2));\n",
    "    p3 = flipud(sortrows(p3, 2));\n",
    "    p4 = sortrows(p4, 2);\n",
    "    \n",
    "    bpa = [p1;];\n",
    "    bpa = [p1; p2;];\n",
    "    bpa = [p1; p2; p3;];\n",
    "    bpa = [p1; p2; p3; p4];\n",
    "    \n",
    "    % Fill areas\n",
    "    %fb = fill([Xl Xl Xm Xm],[Yl Ym Ym Yl], [222/255 226/255 255/255]); hold on\n",
    "    %fr = fill(bpa(:,1), bpa(:,2), [255/255 219/255 223/255]); hold on\n",
    "    fr = fill([Xl Xl Xm Xm],[Yl Ym Ym Yl], [255/255 219/255 223/255]); hold on\n",
    "    fb = fill(bpa(:,1), bpa(:,2), [222/255 226/255 255/255]); hold on\n",
    "    \n",
    "    \n",
    "    % Draw the data points\n",
    "    pinds = find(labels > 0);\n",
    "    ninds = find(labels < 0);\n",
    "    \n",
    "    ph = plot(data(pinds, 1), data(pinds, 2), 'o', 'MarkerFaceColor', 'red', 'MarkerEdgeColor', 'black', 'MarkerSize', 12); hold on\n",
    "    nh = plot(data(ninds, 1), data(ninds, 2), 'x', 'MarkerFaceColor', 'blue', 'MarkerEdgeColor', 'blue', 'MarkerSize', 15, 'LineWidth', 2);\n",
    "    %lineh = plot([Xb Xe], [Yb Ye], 'm', 'LineWidth', 2);\n",
    "    %supv = find(alphas~=0);\n",
    "    %supvInds \n",
    "    supvh = plot(data(find(alphas~=0), 1), data(find(alphas~=0), 2), 's', 'color', 'k', 'MarkerSize', 18);\n",
    "    %legend([lineh, upM, supvh, ph, nh], 'Decision boundary', 'Margin', 'Support vectors', '+1 vector', '-1 vector');\n",
    "    grid on\n",
    "    %title(['{\\color{blue}2ndO-Polynomial kernel, W1 = ' num2str(weights(2)) ', W2 = ' num2str(weights(3)) ', b = ' num2str(weights(1)) ', margin = ' num2str(1/norm(w)) '}'], 'fontweight', 'bold')\n",
    "    title(['{\\color{blue} K(x,x'') = (1+x^Tx'')^4 }'], 'fontweight', 'bold')\n",
    "    xlabel('')\n",
    "    ylabel('')\n",
    "    xlim([Xb Xe])\n",
    "    \n",
    "    set(gca,'xtick',[])\n",
    "    set(gca,'xticklabel',[])\n",
    "    set(gca,'ytick',[])\n",
    "    set(gca,'yticklabel',[])\n",
    "    % Boundary points\n",
    "    bnd = plot(bpa(:,1), bpa(:,2), 'm', 'LineWidth', 2);\n",
    "    legend([ph nh supvh bnd], '+1 vector', '-1 vector', 'Support vector', 'Decision boundary')\n",
    "    %plot(bpoints(:,1), bpoints(:,2), 'm')\n",
    "    \n",
    "    xlim([Xl Xm])\n",
    "    ylim([Yl Ym])\n",
    "    hold off\n",
    "    \n",
    "end\n",
    "\n",
    "% for i = 1:size(data, 1)\n",
    "%     ainds = find(alphas > 0);\n",
    "%     signal = 0;\n",
    "%     for k = 1:length(ainds)\n",
    "%         signal = signal + labels(ainds(k))*alphas(ainds(k))*kmatrix(ainds(k), i);\n",
    "%     end\n",
    "%     signal = signal + b;\n",
    "%     data(i,:)\n",
    "%     sign(signal)\n",
    "%     disp('____________________')\n",
    "% end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%subplot(1,2,2)\n",
    "% minA = min(alphas)-5;\n",
    "% maxA = max(alphas)+5;\n",
    "% X = minA:0.5:maxA;\n",
    "% Y = X;\n",
    "% [Xx, Yy] = meshgrid(X,Y);\n",
    "% Zz = zeros(size(Xx));\n",
    "% for i = 1:size(Xx,1)\n",
    "%     for j = 1:size(Xx,2)\n",
    "%         Zz(i,j) = L([Xx(i,j); Yy(i,j)]);\n",
    "%     end\n",
    "% end\n",
    "% surf(Xx,Yy,Zz)\n",
    "% hold on\n",
    "% if C < Inf\n",
    "%     maxA = C;\n",
    "% end\n",
    "% \n",
    "% plot3(0,0,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% plot3(maxA,0,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% plot3(0,maxA,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% plot3(maxA,maxA,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% fill3([0 0 maxA maxA 0], [0 maxA maxA 0 0], [0 0 0 0 0], 'g')\n",
    "% for i = 1:size(solmat, 1)\n",
    "%     plot3(solmat(i,1), solmat(i,2), L([solmat(i,1); solmat(i,2)]), 'ko', 'MarkerFaceColor', 'm')\n",
    "% end\n",
    "hold off\n",
    "% fill3([0,0,C,C,0],[0,C,C,0,0],[0,0,0,0,0], 'blue');\n",
    "% hold on\n",
    "% fill3([0,C,C,0,0],[0,0,0,0,0],[0,0,C,C,0], 'blue');\n",
    "% fill3([0,0,0,0,0],[0,C,C,0,0],[0,0,C,C,0], 'blue');\n",
    "% fill3([0,C,C,0,0],[C,C,C,C,C],[0,0,C,C,0], 'blue');\n",
    "% fill3([C,C,C,C,C],[C,0,0,C,C],[0,0,C,C,0], 'blue');\n",
    "% fill3([0,0,C,C,0],[0,C,C,0,0],[C,C,C,C,C], 'blue');\n",
    "% grid on\n",
    "% alpha(0.2);\n",
    "% po = 0;\n",
    "% solmat\n",
    "% X = 0:31;\n",
    "% Y = X;\n",
    "% [Xx, Yy] = meshgrid(X,Y);\n",
    "% Zz = zeros(size(Xx));\n",
    "% for i = 1:size(Xx, 1)\n",
    "%     for j = 1:size(Xx,2)\n",
    "%         Zz(i,j) = L([Xx(i,j) Yy(i,j)]');\n",
    "%     end\n",
    "% end\n",
    "% surf(Xx,Yy,Zz)\n",
    "%   for i = 1:size(solmat, 1)\n",
    "%     p = solmat(i,:);\n",
    "%     if i == 1\n",
    "%         plot3(p(1), p(2), p(3), 'o', 'MarkerFaceColor', 'yellow', 'MarkerEdgeColor', 'black')\n",
    "%     elseif i == size(solmat, 1)\n",
    "%         plot3(p(1), p(2), p(3), 'o', 'MarkerFaceColor', 'green', 'MarkerEdgeColor', 'black')\n",
    "%         vectarrow(po,p, [1 .5 0]);\n",
    "%     else\n",
    "%         plot3(p(1), p(2), p(3), 'o', 'MarkerFaceColor', 'red', 'MarkerEdgeColor', 'black')\n",
    "%         vectarrow(po,p, [1 .5 0]);\n",
    "%     end\n",
    "%     po = p;\n",
    "%end\n",
    "\n",
    "% X = [0, 10];\n",
    "% Y = X;\n",
    "% Z = -(labels(1)*X+labels(2)*Y)/labels(3);\n",
    "% ap = fill3([X(1) X(1) X(2) X(2) X(1)], [Y(1) Y(2) Y(2) Y(1) Y(1)], [Z(1) Z(1) Z(2) Z(2) Z(1)], 'red');\n",
    "% alpha(ap, 0.4)\n",
    "% hold off\n",
    "\n",
    "%%% LEGACY \n",
    "% \n",
    "%         for i = 1:length(d)\n",
    "%             if d(i) < 0 && x(i) ~= 0 && -x(i)/d(i) < lmax\n",
    "%                 lmax = -x(i)/d(i);\n",
    "%             elseif d(i) > 0 && (C-x(i))/d(i) < lmax\n",
    "%                 lmax = (C-x(i))/d(i);\n",
    "%             end\n",
    "%         end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function [weights, alphas, iters] = solveSVM(data, labels, C, e)\n",
    "% FUNCTION [weights, alphas, iters] = solveSVM(data, labels, C, e)\n",
    "% \n",
    "% AUTHOR: Jonne Pohjankukka, 2015\n",
    "%\n",
    "% VERSION: 1.0\n",
    "%\n",
    "% DESCRIPTION: \n",
    "% - This function will attempt to solve the optimal weights for a Support\n",
    "% Vector Machines (SVM) model using active set method with gradient\n",
    "% projection.\n",
    "% \n",
    "% INPUTS: \n",
    "% \"data\" a n-by-m data matrix. The number of rows 'n' corresponds to the\n",
    "% number of data points and the number of columns 'm' corresponds to the\n",
    "% number of variables. \n",
    "% \"labels\" a 1-by-n row vector of data labels from the set {-1,1}. \n",
    "% \"C\" Box costraint upper limit. This will constrain the values of 'alphas'\n",
    "% to the range 0 <= alphas <= C. If hard-margin SVM model is required set\n",
    "% C=Inf. \n",
    "% \"e\" a real value corresponding to the convergence criterion, that is if\n",
    "% solution Xi and Xi-1 are within distance 'e' from each other stop the\n",
    "% learning process, i.e. IF |F(Xi)-F(Xi-1)| < e ==> stop learning process.\n",
    "%\n",
    "% OUTPUTS: \n",
    "% \"weights\" a vector corresponding to the optimal decision line parameters.\n",
    "% \"alphas\" a vector of alpha-values corresponding to the optimal solution\n",
    "% of the dual optimization problem of SVM.\n",
    "% \"iters\" number of iterations until learning stopped.\n",
    "%\n",
    "% EXAMPLE USAGE 1: \n",
    "% \n",
    "% 'Hard-margin SVM': \n",
    "%\n",
    "% data = [0 0;2 2;2 0;3 0];\n",
    "% labels = [-1 -1 1 1];\n",
    "% [weights, alphas, iters] = solveSVM(data, labels, Inf, 10^-100)\n",
    "% \n",
    "% EXAMPLE USAGE 2:\n",
    "%\n",
    "% 'Soft-margin SVM':\n",
    "%\n",
    "% data = [0 0;2 2;2 0;3 0];\n",
    "% labels = [-1 -1 1 1];\n",
    "% [weights, alphas, iters] = solveSVM(data, labels, 0.8, 10^-100)\n",
    "\n",
    "\n",
    "% STEP 1: INITIALIZATION OF THE PROBLEM\n",
    "format rat\n",
    "% Calculate linear kernel matrix\n",
    "L = kron(labels', labels);\n",
    "K = data*data';\n",
    "% Hessian matrix\n",
    "Qd = L.*K;\n",
    "% The minimization function \n",
    "L = @(a) ((1/2)*a'*Qd*a - ones(1, length(a))*a)*(1/10);\n",
    "% Gradient of the minimizable function \n",
    "gL = @(a) (a'*Qd - ones(1, length(a)))*(1/10);\n",
    "\n",
    "\n",
    "% STEP 2: THE LEARNING PROCESS, ACTIVE SET WITH GRADIENT PROJECTION\n",
    "\n",
    "% Initial feasible solution (required by gradient projection)\n",
    "x = zeros(length(labels), 1);\n",
    "iters = 1;\n",
    "optfound = 0;\n",
    "\n",
    "solmat = [x'];\n",
    "while optfound == 0 % criterion met    \n",
    "    \n",
    "    % Negative of the gradient at initial solution\n",
    "    g = -gL(x);\n",
    "    g = g/norm(g);\n",
    "    % Set the active set and projection matrix\n",
    "    Aq = labels; % In plane y^Tx = 0\n",
    "    P = eye(length(x))-Aq'*inv(Aq*Aq')*Aq; % In plane projection\n",
    "    % Values smaller than 'eps' are changed into 0\n",
    "    P(abs(P) < eps) = 0;\n",
    "    d = P*g'; % Projection onto plane\n",
    "    % Values smaller than 'eps' are changed into 0\n",
    "    d(abs(d) < eps) = 0;\n",
    "    if ~isempty(x(x==0 | x==C)) % Constraints active? If yes, we need to change projection matrix\n",
    "        acinds = find(x==0 | x==C);\n",
    "        constMat = eye(length(x));\n",
    "        aclog = zeros(length(x), 1);\n",
    "        for i = 1:length(acinds)\n",
    "            if (x(acinds(i)) == 0 && d(acinds(i)) < 0) || x(acinds(i)) == C && d(acinds(i)) > 0\n",
    "                % Make the constraint vector\n",
    "                aclog(acinds(i)) = 1;\n",
    "            end\n",
    "        end\n",
    "        if sum(aclog) > 0 % Meaning we need new projection matrix and search direction\n",
    "            Aq = [labels; constMat(logical(aclog),:)];\n",
    "            % Update the projection matrix\n",
    "            P = eye(length(x))-Aq'*inv(Aq*Aq')*Aq; % In plane / box projection\n",
    "            % Values smaller than 'eps' are changed into 0\n",
    "            P(abs(P) < eps) = 0;\n",
    "            d = P*g'; % Projection onto plane / border\n",
    "            % Values smaller than 'eps' are changed into 0\n",
    "            d(abs(d) < eps) = 0;\n",
    "        end\n",
    "    end\n",
    "    d = d/norm(d);\n",
    "    % Line search for optimal step size into search direction 'd'\n",
    "    if ~isempty(d(d~=0)) && rank(Aq) < length(x) \n",
    "        lopt = ((g*d)/(d'*Qd*d));       \n",
    "        uplamlims = [(C-x)./d; -x./d];\n",
    "        lmax = min(uplamlims(uplamlims > 0));\n",
    "        lambda = max(0, min([lopt, lmax]));\n",
    "        if abs(lambda) < eps\n",
    "            lambda = 0;\n",
    "        end\n",
    "        x_old = x;\n",
    "        x = x + lambda*d;\n",
    "        solmat = [solmat; x'];\n",
    "    end\n",
    "    \n",
    "     %%%% DISPLAY INFORMATION, THIS PART IS NOT NECESSAY, ONLY FOR DEBUGGING\n",
    "    \n",
    "    if Aq*x ~= 0 & 1 == 1\n",
    "        disp('ACTIVE SET CONSTRAINTS Aq :')\n",
    "        Aq\n",
    "        disp('CURRENT SOLUTION x :')\n",
    "        x\n",
    "        disp('MULTIPLICATION OF Aq and x')\n",
    "        Aq*x\n",
    "    end\n",
    "    \n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    \n",
    "    % Check whether search direction is 0-vector or 'e'-criterion met.\n",
    "    if isempty(d(d~=0)) || abs(L(x)-L(x_old)) < e\n",
    "        optfound = 1;\n",
    "    end\n",
    "    iters = iters + 1;\n",
    "    if iters > 10000 % Stop if solution hasn't converged until 100000 iterations\n",
    "        disp('No convergence after 10000 iterations, learning stopped')\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "%%% STEP 3: GET THE MODEL\n",
    "alphas = x;\n",
    "alphas(abs(alphas) < eps) = 0;\n",
    "w = zeros(1, length(data(1,:)));\n",
    "for i = 1:size(data,1)\n",
    "    w = w + labels(i)*alphas(i)*data(i,:);\n",
    "end\n",
    "svinds = find(alphas>0); \n",
    "svind = svinds(1);\n",
    "b = 1/labels(svind) - w*data(svind, :)';\n",
    "\n",
    "\n",
    "\n",
    "%%% STEP 4: OPTIMALITY CHECK, KKT conditions. See KKT-conditions for reference.\n",
    "weights = [b; w'];\n",
    "datadim = length(data(1,:));\n",
    "Q = [zeros(1,datadim+1); zeros(datadim, 1), eye(datadim)];\n",
    "A = [ones(size(data,1), 1), data];\n",
    "for i = 1:length(labels)\n",
    "    A(i,:) = A(i,:)*labels(i);\n",
    "end\n",
    "LagDuG = Q*weights - A'*alphas;\n",
    "Ac = A*weights - ones(length(labels),1);\n",
    "alpA = alphas.*Ac;\n",
    "LagDuG(any(abs(LagDuG-0) < 10^-14)) = 0;\n",
    "if ~any(alphas < 0) && all(LagDuG == zeros(datadim+1,1)) && all(abs(Ac) >= 0) && all(abs(alpA) < 10^-6)\n",
    "    disp('Optimal found, Karush-Kuhn-Tucker conditions satisfied.')\n",
    "elseif C < Inf\n",
    "    disp('Soft-margin SVM solution found.')\n",
    "else\n",
    "    disp('Optimal not found, Karush-Kuhn-Tucker conditions not satisfied.')\n",
    "    disp('This is caused because of numerical issues, accuracy of calculations...')\n",
    "end\n",
    "\n",
    "%subplot(1,2,1)\n",
    "% VISUALIZATION FOR 2D-CASE\n",
    "if size(data, 2) == 2 \n",
    "    \n",
    "    % Draw the margin \n",
    "    Xb = min(data(:,1))-1;\n",
    "    Xe = max(data(:,1))+1;\n",
    "    Yb = -(b+w(1)*Xb)/w(2);\n",
    "    Ye = -(b+w(1)*Xe)/w(2);\n",
    "    tvec = [alphas, labels'];\n",
    "    pSupInds = find(tvec(:,1) > 0 & tvec(:,2) == 1);\n",
    "    nSupInds = find(tvec(:,1) > 0 & tvec(:,2) == -1);\n",
    "    pSups = data(pSupInds, :);\n",
    "    nSups = data(nSupInds, :);\n",
    "    supV = nSups(1,:);\n",
    "    Yline = (-b-w(1)*supV(1))/w(2)\n",
    "    Ydif = abs(supV(2)-Yline);\n",
    "    Ylb = (-b-w(1)*Xb)/w(2);\n",
    "    Yle = (-b-w(1)*Xe)/w(2);\n",
    "    upM = fill([Xb, Xb, Xe, Xe], [Ylb Ylb+Ydif Yle+Ydif Yle], [252/255, 255/255, 173/255]); \n",
    "    hold on\n",
    "    loM = fill([Xb, Xb, Xe, Xe], [Ylb Ylb-Ydif Yle-Ydif Yle], [252/255, 255/255, 173/255]);\n",
    "    %alpha(upM, 0.8)\n",
    "    %alpha(loM, 0.8)\n",
    "    \n",
    "    % Draw the data points\n",
    "    \n",
    "    pinds = find(labels > 0);\n",
    "    ninds = find(labels < 0);\n",
    "    ph = plot(data(pinds, 1), data(pinds, 2), 'o', 'MarkerFaceColor', 'red', 'MarkerEdgeColor', 'black', 'MarkerSize', 8);\n",
    "    nh = plot(data(ninds, 1), data(ninds, 2), 'o', 'MarkerFaceColor', 'blue', 'MarkerEdgeColor', 'black', 'MarkerSize', 8);\n",
    "    lineh = plot([Xb Xe], [Yb Ye], 'm', 'LineWidth', 2);\n",
    "    %supv = find(alphas~=0);\n",
    "    %supvInds \n",
    "    supvh = plot(data(find(alphas~=0), 1), data(find(alphas~=0), 2), '.', 'color', 'green');\n",
    "    legend([lineh, upM, supvh, ph, nh], 'Decision boundary', 'Margin', 'Support vectors', '+1 vector', '-1 vector');\n",
    "    grid on\n",
    "    title(['{\\color{blue}W1 = ' num2str(weights(2)) ', W2 = ' num2str(weights(3)) ', b = ' num2str(weights(1)) ', margin = ' num2str(1/norm(w)) '}'], 'fontweight', 'bold')\n",
    "    xlabel('X')\n",
    "    ylabel('Y')\n",
    "    xlim([Xb Xe])\n",
    "    hold off\n",
    "    \n",
    "end\n",
    "\n",
    "%subplot(1,2,2)\n",
    "% minA = min(alphas)-5;\n",
    "% maxA = max(alphas)+5;\n",
    "% X = minA:0.5:maxA;\n",
    "% Y = X;\n",
    "% [Xx, Yy] = meshgrid(X,Y);\n",
    "% Zz = zeros(size(Xx));\n",
    "% for i = 1:size(Xx,1)\n",
    "%     for j = 1:size(Xx,2)\n",
    "%         Zz(i,j) = L([Xx(i,j); Yy(i,j)]);\n",
    "%     end\n",
    "% end\n",
    "% surf(Xx,Yy,Zz)\n",
    "% hold on\n",
    "% if C < Inf\n",
    "%     maxA = C;\n",
    "% end\n",
    "% \n",
    "% plot3(0,0,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% plot3(maxA,0,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% plot3(0,maxA,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% plot3(maxA,maxA,0, 'ko', 'MarkerFaceColor', 'green')\n",
    "% fill3([0 0 maxA maxA 0], [0 maxA maxA 0 0], [0 0 0 0 0], 'g')\n",
    "% for i = 1:size(solmat, 1)\n",
    "%     plot3(solmat(i,1), solmat(i,2), L([solmat(i,1); solmat(i,2)]), 'ko', 'MarkerFaceColor', 'm')\n",
    "% end\n",
    "hold off\n",
    "% fill3([0,0,C,C,0],[0,C,C,0,0],[0,0,0,0,0], 'blue');\n",
    "% hold on\n",
    "% fill3([0,C,C,0,0],[0,0,0,0,0],[0,0,C,C,0], 'blue');\n",
    "% fill3([0,0,0,0,0],[0,C,C,0,0],[0,0,C,C,0], 'blue');\n",
    "% fill3([0,C,C,0,0],[C,C,C,C,C],[0,0,C,C,0], 'blue');\n",
    "% fill3([C,C,C,C,C],[C,0,0,C,C],[0,0,C,C,0], 'blue');\n",
    "% fill3([0,0,C,C,0],[0,C,C,0,0],[C,C,C,C,C], 'blue');\n",
    "% grid on\n",
    "% alpha(0.2);\n",
    "% po = 0;\n",
    "% solmat\n",
    "% X = 0:31;\n",
    "% Y = X;\n",
    "% [Xx, Yy] = meshgrid(X,Y);\n",
    "% Zz = zeros(size(Xx));\n",
    "% for i = 1:size(Xx, 1)\n",
    "%     for j = 1:size(Xx,2)\n",
    "%         Zz(i,j) = L([Xx(i,j) Yy(i,j)]');\n",
    "%     end\n",
    "% end\n",
    "% surf(Xx,Yy,Zz)\n",
    "%   for i = 1:size(solmat, 1)\n",
    "%     p = solmat(i,:);\n",
    "%     if i == 1\n",
    "%         plot3(p(1), p(2), p(3), 'o', 'MarkerFaceColor', 'yellow', 'MarkerEdgeColor', 'black')\n",
    "%     elseif i == size(solmat, 1)\n",
    "%         plot3(p(1), p(2), p(3), 'o', 'MarkerFaceColor', 'green', 'MarkerEdgeColor', 'black')\n",
    "%         vectarrow(po,p, [1 .5 0]);\n",
    "%     else\n",
    "%         plot3(p(1), p(2), p(3), 'o', 'MarkerFaceColor', 'red', 'MarkerEdgeColor', 'black')\n",
    "%         vectarrow(po,p, [1 .5 0]);\n",
    "%     end\n",
    "%     po = p;\n",
    "%end\n",
    "\n",
    "% X = [0, 10];\n",
    "% Y = X;\n",
    "% Z = -(labels(1)*X+labels(2)*Y)/labels(3);\n",
    "% ap = fill3([X(1) X(1) X(2) X(2) X(1)], [Y(1) Y(2) Y(2) Y(1) Y(1)], [Z(1) Z(1) Z(2) Z(2) Z(1)], 'red');\n",
    "% alpha(ap, 0.4)\n",
    "% hold off\n",
    "\n",
    "%%% LEGACY \n",
    "% \n",
    "%         for i = 1:length(d)\n",
    "%             if d(i) < 0 && x(i) ~= 0 && -x(i)/d(i) < lmax\n",
    "%                 lmax = -x(i)/d(i);\n",
    "%             elseif d(i) > 0 && (C-x(i))/d(i) < lmax\n",
    "%                 lmax = (C-x(i))/d(i);\n",
    "%             end\n",
    "%         end \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
