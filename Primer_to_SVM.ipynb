{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer to Support Vector Machines\n",
    "\n",
    "### By Dr. Jonne Pohjankukka\n",
    "\n",
    " This tutorial is an ongoing self-made project based on my exercise work on mathematical optimization in 2015. The project is yet unfinished and not in a fully readable state. The upcoming versions will include: \n",
    "\n",
    " updates in the text, fixing of typos, references, added code etc. \n",
    " \n",
    " **STATUS OF TUTORIAL**: I'm currently editing section 5 so sections 1-4 are (or should be) in a readable state. The code at the bottom is working at the moment, of course cleaning, commenting etc. needs to be done. The tutorial is I'd say 70% ready.  \n",
    "\n",
    "1. [Machine learning](#1.)<br>\n",
    "2. [Linear vs. nonlinear models](#2.)<br>\n",
    "3. [Support vector machines](#3.)<br>\n",
    "    3.1 [How to find the maximum margin model?](#3.1)<br>\n",
    "    3.2 [Is maximum margin model really better?](#3.2)<br>\n",
    "    3.3 [What if data is not linearly separable?](#3.3)<br>\n",
    "    3.4 [Kernel methods](#3.4)<br>\n",
    "4. [Solving the optimal SVM](#4.)<br>\n",
    "    4.1 [Quadratic programming](#4.1)<br>\n",
    "    4.2 [Lagrangian dual](#4.2)<br>\n",
    "    4.3 [The dual of hard margin SVM](#4.3)<br>\n",
    "    4.4 [Example: solving the SVM classifier via the Lagrangian dual form](#4.4)<br>\n",
    "    4.5 [The dual in the $\\mathcal{Z}$-space](#4.5)<br>\n",
    "5. [Simple algorithm for solving the SVM model](#5.)<br>\n",
    "    5.1 [Active set methods](#5.1)<br>\n",
    "    5.2 [Gradient projection](#5.2)<br>\n",
    "    5.3 [Constructing a gradient projection/active set-based SVM solver](#5.3)<br>\n",
    "    5.4 [Pseudocode for the GPAS SVM solver](#5.4)<br>\n",
    "6. [Appendix: code listing](#Appendix)<br>\n",
    "\n",
    "\n",
    "#### Preface: to whom is this tutorial for? \n",
    "I think it was back in 2014 or 2015 when I first came across with support vector machines and I remember being frustrated for the difficulty of finding good practical tutorials on the subject. I could find many tutorials online describing the theory and general ideas, but everytime after reading them I had one question: okay, now what? What do I type in my IDE? I could understand the idea, but when I needed to do some programming I found out I actually did not understand it that well. So what I did next was to search again online for some advices, and I found out these guides repeating themselves: \"use a package\", \"download and apply a package\". I understand that this is a perfectly valid solution if your goal is simply to apply support vector machines. In my case, I wanted to understand **every step going on under the hood**. I wanted to understand how one constructs a support vector machine model, how does one train it et cetera **without using any packages at all** (excluding trivial packages of course such as NumPy). How does one implement the model, the mathematical optimization, kernel tricks, everything by yourself? \n",
    "\n",
    "This tutorial is an attempt to provide readers a primer into the subject who have similar problems and frustrations that I had when first running into the subject. I will provide the reader with the general idea of support vector machines, its basic theretical background, practical pen-and-paper examples, pseudocode and Python codes which do not apply any packages related to machines learning or mathematical optimization. I will assume the reader has some understanding on probability, linear algebra and mathematical optimization. Enjoy the ride =) \n",
    "\n",
    "\n",
    "\n",
    "<a id='1.'></a>\n",
    "#### 1. Machine learning\n",
    "What is machine learning (ML)? ML is a subfield of computer science focused on the research and desing of models, which aim to discover and learn patterns from data. Applications of ML could be for example the prediction of stock price values, classification of soil bearing capacity, or forecasting the effects of drinking milk to the acidity levels in human stomach. ML combines techniques from many fields of science, such as probability theory, statistics, physics, mathematical optimization and neuroscience. \n",
    "\n",
    "One of the most important (or maybe better say **the most important**) issues in ML is the concept of generalizability, which measures how well a ML model performs in making predictions in new situations (that is, with new data). A model probably fails to generalize well, if it is \"fitted\" too much to the data (this claim is also backed up theoretically). The notion of \"fitting a model to data\", usually means that we minimize some error function, which describes the goodness-of-fit of our model to the data. The lower the error, the better the fit. In fields such as mathmetical optimization or calculus of variation, the goal is many times to find the absolute minimum of this error (say, the optimum trajectory of a particle). It has however been shown, both from a theoretical and practical perspective, that if you train a model too much _overfitting_ will occur. \n",
    "\n",
    "Overfitting means that the model has learned not only the intrinsic phenomena in the data, but also an additional non-existing relationship called _noise_, which can not be learned by definition. Noise is present in all data you ever measure and can be caused e.g. by measurement errors, weather or malfunctioning sensors. Thus by overfitting a model, we have learned an incorrent relationship from the data, and are more likely to generalize worse. A common way to tackel overfitting is to apply a method known as _regularization_, which basically means restricting the learning process by preventing it from learning too complicated functions. There are many ways to tackle overfitting such as using penalty term or early-stopping methods, but we will not go deeper into this subject in this tutorial. Readers interested with overfitting can find more information from standard ML literature. \n",
    "\n",
    "<a id='2.'></a>\n",
    "#### 2. Linear vs. nonlinear models\n",
    "All methods of ML can be divided into two groups: linear or nonlinear. Linear models are simple and effective methods in many applications describing real world phenomena, but sometimes their expressive power is not enough to learn more complicated relationships in the data. In cases like this, nonlinear methods are usually applied due to their higher expressive power. However, due to its simplicity a linear model is less likely to overfit than nonlinear model. Also in general, nonlinear models require more data than linear models to achieve succesful generalization. There is therefore a trade-off between the expressive power of a model and its likelihood of overfitting to the data. Because of the higher expressive power, nonlinear models are more easily fitted to the noise in the data. With this in mind, it begs now the question: does there exist a model which contains both the resistance towards noise (as in linear models) and high expressive power (as in nonlinear models). A clever method called _support vector machines_ (SVM) was proposed for achieving this by Vladimir Vapnik and Alexey Chervonenkis in 1963, which we will discuss next. In what follows, we will go through the motivation, theory, examples and a self-made implementation (in pseudo- and Python code) of the SVM method.   \n",
    "\n",
    "<a id='3.'></a>\n",
    "#### 3. Support vector machines\n",
    "We will begin with a simple geometric illustration, which best explains the intuition behind SVM. In figure 1 is presented data from two different classes denoted by blue crosses and red circles. The lines depict three competing decision lines which we use to determine the classification regions. The data is _linearly separable_ which means that the data can be divided into two distinct subspaces by the line (or a hyperplane in higher dimensions). Which of these three lines would the best choice and why? Or would you say they are all equally good? After all, all the lines perfectly classify the data. I'm betting however, that you would probably select the line in the rightmost image. Why would we choose this line? What is the intuition behind our choice? Remember that all data we ever measure contains noise. It would be good therefore to select such a decision line which would be most robust against this noise. You can think of the effect of noise in the four data points by shifting them randomly into arbitrary directions a tiny bit. We would want the decision line to allow as much of this random shifting (due to noise) as possible and still achieve correct classification. You would agree that decision line in the rightmost image allows the largest amount of random shifting in the points in the image, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drawClassifierLines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-592c359452a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make Figure 1 plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdrawClassifierLines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshowRadius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'drawClassifierLines' is not defined"
     ]
    }
   ],
   "source": [
    "# Make Figure 1 plot\n",
    "drawClassifierLines(showRadius=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 1: Geometric intuition behind the SVM.</center>\n",
    "\n",
    "As we discussed, we would like to select a decision line which allows as much as possible this random shifting in the data (i.e. noise) without affecting the classification. We can picture this random shifting caused by noise in terms of circles or spheres with radius $r$ around the data points (see figure 2). Notice that in the rightmost image the circles have the largest radius, and so we can think that the data points are allowed to move within this circle (i.e. we have uncertainty) and still we get correct classification. What we would like to do, is to select a linear classifier which maximizes the radius of these spheres. In this way, we have maximized the model's robustness against noise in the data. Notice that in the rightmost image, the distance from the line to the closest data points is maximized. The data points at which the spheres first touch the decision line are called _support vectors_, from which the name of the SVM comes from. The support vectors play a special role in the SVM since they are solely responsible in determining the classifier line. If we would for example remove the rightmost data point from the below images, it would not affect the choice of the classifier line, only the support vectors have impact on this. Lastly, since in SVM the point is to maximize the distance of the line to the support vectors (called the _margin_), SVMs are also called _maximum margin models_.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make Figure 2 plot\n",
    "drawClassifierLines(showRadius=True)"
   ]
  },
  {
   "attachments": {
    "pic.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 2: Uncertainties around the data points in figure 1 illustrated.</center>\n",
    "\n",
    "<a id='3.1'></a>\n",
    "#### 3.1 How to find the maximum margin model? \n",
    "Lets now proceed to formalize the concepts of the SVM and find out how can we solve the maximum margin hyperplane. Denote by $\\textbf{x}\\in\\mathcal{X}\\subset\\mathbb{R}^d$ an input vector, $\\textbf{w}\\in\\mathbb{R}^d$ as hyperplane weight values, $y\\in\\{-1, 1\\}$ as label value and $b\\in\\mathbb{R}$ as a constant intercept term. A hyperplane $h$ defined by vector $\\textbf{w}$ and constant $b$ separates the data points $(\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), ..., (\\textbf{x}_n, y_n)$ if and only if: \n",
    "\n",
    "$$y_i(\\textbf{w}^T\\textbf{x}_i+b) > 0\\;\\;\\;(i = 1, 2, ..., n),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(1)$$\n",
    "\n",
    "where $h(\\textbf{x}) = \\textbf{w}^T\\textbf{x}+b$ is called the _signal_ of input $\\textbf{x}$. That is, for data points having $y=-1$ we want the hyperplane to have a negative signal $h(\\textbf{x}) <0$ and for data points with $y=1$ we want $h(\\textbf{x}) >0$ correspondingly. Notice next that the equations in $(1)$ are always satisfied also for any $h(\\textbf{x})/\\rho$, where $\\rho > 0$. Define next (for reasons later coming clear):\n",
    "\n",
    "$$\\rho := \\underset{i = 1, 2, ..., n}{\\text{min}} y_i(\\textbf{w}^T\\textbf{x}_i+b),$$\n",
    "\n",
    "and redefine the separating hyperplane as $h(\\textbf{x})/\\rho$. For this redefined hyperplane we have: \n",
    "\n",
    "$$\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\frac{h(\\textbf{x})}{\\rho}\\right)=\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\frac{\\textbf{w}^T\\textbf{x}_i}{\\rho}+\\frac{b}{\\rho}\\right)=\\frac{1}{\\rho}\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)=\\frac{\\rho}{\\rho}=1,$$\n",
    "\n",
    "that is, the hyperplane separates all the data points if and only if: \n",
    "\n",
    "$$\\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)=1.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(2)$$\n",
    "\n",
    "So we have now learned that a hyperplane $h$ separates the data points only if the condition in $(2)$ is met. This condition is not yet though sufficient enough to define the problem of finding a maximum margin hyperplane. Recall that one of the defining factors of the SVM was also the fact that the margin (distance to the closest, a.k.a support vectors) needs to maximized. In other words, we want to maximize the distance between the hyperplane $h$ and the vector $\\textbf{x}$ closest to it. \n",
    "\n",
    "To start, lets first figure out how one generally calculates the distance between a hyperplane and a vector point. To solve this distance we need to calculate the perpendicular distance between $h$ and $\\textbf{x}$. Let $\\textbf{u}$ be a unit vector perpendicular to $h$ and $\\textbf{x}'$ some point on $h$, i.e. $h(\\textbf{x}') = \\textbf{w}^T\\textbf{x}' + b = 0$. Then, from basic linear algebra (make e.g. a Google search) we know that the distance between $h$ and point $\\textbf{x}$ is the projection $d(h, \\textbf{x}) = |\\textbf{u}^T(\\textbf{x}-\\textbf{x}')|$. Note that $\\textbf{w}$ is perpendicular to the plane $h$, since for two points $\\textbf{x}', \\textbf{x}''$ on the plane $h$ we have:\n",
    "\n",
    "$$\\textbf{w}^T(\\textbf{x}''-\\textbf{x}') = \\textbf{w}^T\\textbf{x}''-\\textbf{w}^T\\textbf{x}'=-b+b = 0,$$\n",
    "\n",
    "that is $\\textbf{w}$ is perpendicular to any vector $\\textbf{x}''-\\textbf{x}'$ on $h$. We can now set $\\textbf{u} = \\textbf{w}/||\\textbf{w}||$, where $||\\textbf{w}||$ is the magnitude of vector $\\textbf{w}$. Hence, we get the distance from an arbitrary point $\\textbf{x}$ to plane $h$ as: \n",
    "\n",
    "$$d(h, \\textbf{x}) = \\frac{\\left\\lvert\\textbf{w}^T(\\textbf{x}-\\textbf{x}')\\right\\rvert}{||\\textbf{w}||}= \\frac{\\left\\lvert\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\textbf{x}'\\right\\rvert}{||\\textbf{w}||} = \\frac{\\left\\lvert\\textbf{w}^T\\textbf{x}+b\\right\\rvert}{||\\textbf{w}||}.$$\n",
    "\n",
    "Furthermore, since for a binary SVM classifier we have $y_i\\in \\{-1, 1\\} \\forall i$, we get: \n",
    "\n",
    "$$\\left\\lvert\\textbf{w}^T\\textbf{x}_i+b\\right\\rvert = y_i(\\textbf{w}^T\\textbf{x}_i+b)\\;\\forall i,$$\n",
    "\n",
    "and because of $(2)$ we have that:\n",
    "\n",
    "$$\\underset{i = 1, 2, ..., n}{\\text{min}} d(h, \\textbf{x}_i) = \\underset{i = 1, 2, ..., n}{\\text{min}} \\frac{y_i(\\textbf{w}^T\\textbf{x}_i+b)}{||\\textbf{w}||}= \\frac{1}{||\\textbf{w}||}\\underset{i = 1, 2, ..., n}{\\text{min}} y_i(\\textbf{w}^T\\textbf{x}_i+b) = \\frac{1}{||\\textbf{w}||}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(3)$$\n",
    "\n",
    "The nice and simple form of equation $(3)$ is the reason why we redefined the hyperplane $h$ earlier by scaling it with the constant $\\rho$, giving us thus a nice numerator of 1. We have now found all the ingredients we need to define the problem of solving the maximum margin model: A SVM model (binary classification) is such a hyperplane, which maximizes the value of $1/||\\textbf{w}||$ (margin) and satisfies the condition in equation $(2)$. This can expressed in the following optimization problem: \n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & \\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)=1.\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(4)\n",
    "\\end{alignat}\n",
    "\n",
    "From the perspective of mathematical optimization, it is easier to solve the problem:\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)\\geq 1,\\;\\;\\;(i=1,2, ..., n)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(5)\n",
    "\\end{alignat}\n",
    "\n",
    "which is equivalent to problem $(4)$ at the optimal solution, given that the data set contains samples with negative and positive labels. To show that equations $(4)$ and $(5)$ are equivalent at the optimum, suppose the solution $(\\textbf{w}^*, b^*)$ of equation $(5)$ has: \n",
    "\n",
    "$$\\rho^* = \\underset{i = 1, 2, ..., n}{\\text{min}} y_i\\left(\\textbf{w}^{*T}\\textbf{x}_i+b^*\\right) > 1,$$\n",
    "\n",
    "which means that $(\\textbf{w}^*, b^*)$ is not a solution to equation $(4)$. Consider now a hyperplane $(\\textbf{w}, b) = \\frac{1}{\\rho^*}(\\textbf{w}^*, b^*)$ which satisfies the constraints in equation $(5)$. But now we have that $||\\textbf{w}|| = \\frac{1}{\\rho^*}||\\textbf{w}^*|| < ||\\textbf{w}^*||$, which means that $\\textbf{w}^*$ can not be optimal for equation $(5)$ unless $\\textbf{w}^*=\\textbf{0}$. This however is not possible, because the hyperplane $(\\textbf{0}, b)$ can not correctly classify the negative and positive data samples $\\blacksquare$. So, once we have solved the optimal solution $(\\textbf{w}^*, b^*)$ to equation $(5)$, we get the maximum margin classifier model (a.k.a SVM) as: \n",
    "\n",
    "$$g(\\textbf{x})=\\text{sign}\\left(\\textbf{w}^{*T}\\textbf{x}+b^*\\right)\\in\\{-1, 1\\}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(6)$$\n",
    "\n",
    "In the upcoming sections, we will see how to solve the optimization problem of equation $(5)$. First however, we will have a short discussion on why the maximum margin model is better than a non-maximum margin model in terms of generalizability. \n",
    "\n",
    "<a id='3.2'></a>\n",
    "#### 3.2 Is maximum margin model really better?\n",
    "In earlier sections, we had the intuition that a linear classifier with a larger margin would probably perform better than one with a smaller margin. In fact, our intuition is justified also mathematically by a special number called the _Vapnik-Chervonenkis-dimension_ $d_{\\text{VC}}\\in \\mathbb{N}_{>0}$ (VC-dimension), which quantifies a probabilistic bound for the classification error of the SVM model. Generally speaking, a model with a smaller VC-dimension is more likely to achieve succesful generalization than one with a higher VC-dimension. And it is indeed the case (see the works of e.g. V. Vapnik), that the VC-dimension of a SVM model is smaller than the VC-dimension of an unrestricted linear classifier (unrestricted by the margin that is). This fact is a result from an inequality called the _VC-inequality_ (related closely to the Hoeffding inequality), which states that: \n",
    "\n",
    "$$P\\left[\\underset{h\\in\\mathcal{H}}{\\text{sup}}\\left\\lvert E_{\\text{in}}(h)-E_{\\text{out}}(h)\\right\\rvert > \\varepsilon \\right] \\leq 4m_{\\mathcal{H}}(2n)\\exp\\left(-\\frac{1}{8}\\varepsilon^2 n\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(7)$$\n",
    "\n",
    "where $\\mathcal{H}$ is the hypothesis set, that is, a set of functions from which we are searching for the model $h$, $\\varepsilon >0$ is an error bound and $n\\in\\mathbb{N}_{>0}$ is the number of data points. The functions $E_{\\text{in}}(h), E_{\\text{out}}(h): \\mathcal{H}\\to \\mathbb{R}$ denote the training and generalization errors of hypothesis $h$. That is, $E_{\\text{in}}(h)$ describes how well we were able to fit model $h$ to the observed data $\\mathcal{D}=\\{(\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), ..., (\\textbf{x}_n, y_n)\\}$, and $E_{\\text{out}}(h)$ describes how well the model $h$ performs with new yet unseen data. Our goal is to have $E_{\\text{in}}(h)\\approx E_{\\text{out}}(h)$, because in this case the we can trust that our model's performance estimated with the data set $\\mathcal{D}$ reflects its performance in a general situation. \n",
    "\n",
    "The function $m_{\\mathcal{H}}(2n):\\mathbb{N}_{>0}\\to \\mathbb{N}$ maps $2n$ to a number, which describes the maximum number of dichotomies that can be generated by the hypothesis set $\\mathcal{H}$ on any $2n$ data points. In other words, it gives a quantifying number on how many ways the hypothesis set $\\mathcal{H}$ can split $2n$ data points into two categories. If $m_{\\mathcal{H}}(c) = 2^{c}$ for $c$ data points, then we say that the hypothesis set $\\mathcal{H}$ is able _shatter_ the $c$ data points, that is find all possible dichotomies (classifications) for the data. To put the relationship between $d_{\\text{VC}}$ and $m_{\\mathcal{H}}(c)$ explicit, the VC-dimension is defined as $d_{\\text{VC}}\\equiv \\max\\{c\\in\\mathbb{N}\\,|\\,m_\\mathcal{H}(c)=2^c\\}$. In other words, VC-dimension is the maximum number of data points the hypothesis set $\\mathcal{H}$ is able to shatter. The next question now is, what has the VC-dimension got to do with the VC-inequality? It is known, that if a hypothesis set $\\mathcal{H}$ has a finite $d_{\\text{VC}}$, then it holds that: \n",
    "\n",
    "$$m_{\\mathcal{H}}(2n) \\leq \\sum_{i=0}^{d_{\\text{VC}}} {{2n}\\choose i},$$\n",
    "\n",
    "and therefore a hypothesis set with a smaller VC-dimension has a smaller multiplying factor in the right side of equation $(7)$ resulting in a smaller probability bound. This is also somewhat intuitive if you think about. If you are using a hypothesis set with unrestricted (by the margin) classifier models, then you would probably be able to find more dichotomies for the data set $\\mathcal{D}$, than with the hypothesis set consisting from models with the restriction to maximize the margin. With a large number of equally good hypotheses to choose from, you would have smaller chances (a higher probability bound in the VC-inequality) to pick the right one unless you are very lucky.    \n",
    "\n",
    "<a id='3.3'></a>\n",
    "#### 3.3 What if data is not linearly separable?\n",
    "So far, we have been talking about cases where the data set can be separated by a linear model. In many practical situations however the data set can not be separated by a linear model. In Figure 3 I have illustrated two examples of data sets which can not be separated by a linear model.\n",
    "\n",
    "![pic.png](attachment:pic.png)\n",
    "<center>Figure 3: Data which is not linearly separable.</center>\n",
    "\n",
    "So how should we proceed from this? Well, fortunately we two options two consider. Firstly, we can loosen up the condition on the SVM which requires all the data points to be classified correctly. We do this by allowing few misclassifications for the model to occur. An SVM with a looser condition like this is called the _soft margin SVM_, which does not force a perfect classification and allows some mistakes to occur. On the left side of figure 3, we see an example of a soft margin SVM model. Our second option to the separability problem, is to first apply a nonlinear transformation to the data making it linearly separable, and then fit a hard margin linear SVM into this transformed data. In the right side of figure 3 is an example of this second approach. Note that the nonlinear model you seen here is a representation of the linear model of the transformed space in the original data space. The nonlinear transformation is achieved by a function $\\Phi:\\mathcal{X} \\to \\mathcal{Z}\\subset \\mathbb{R}^q$, which transforms the input vectors into $\\textbf{z}_i = \\Phi(\\textbf{x}_i)$. After the transformation, the optimization problem in equation $(5)$ becomes: \n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & y_i\\left(\\textbf{w}^T\\textbf{z}_i+b\\right)\\geq 1,\\;\\;\\;(i=1,2, ..., n),\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(8)\n",
    "\\end{alignat}\n",
    "\n",
    "and similarly to equation $(6)$ we get the nonlinear hard margin SVM model as:\n",
    "\n",
    "$$g(\\textbf{x})=\\text{sign}\\left(\\textbf{w}^{*T}\\Phi(\\textbf{x})+b^*\\right)\\in\\{-1, 1\\}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(9)$$\n",
    "\n",
    "What happens to the generalization capability if we use nonlinear transformation for the data? Does it get worse? In general, the price we pay for using nonlinear models with higher expressive power is that we have a larger risk to overfit the data and that the probability of successful generalization decreases. Fortunately, a neat mathematical theorem exists (see e.g. Mostafa et al. for proofs) which helps us tackle the generalization problem of SVMs using nonlinear transformation. The theorem states that: \n",
    "\n",
    "**Theorem:** the VC-dimension of maximum margin classifier SVM with bounded input data $||\\textbf{x}||\\leq R$ and margin $r\\in \\mathbb{R}^{+}$ follows the inequality:\n",
    "\n",
    "$$d_{\\text{VC}} \\leq \\lceil R^2 / r^2 \\rceil +1,$$\n",
    "\n",
    "regardless on the nonlinear transformation $\\Phi$. This means that even if SVM incorporates an infinite dimensional transformation, generalization is achieved as long as we use large enough margin $r$.\n",
    "\n",
    "<a id='3.4'></a>\n",
    "#### 3.4 Kernel methods\n",
    "In earlier sections, we went through formulating the mathematical optimization problem of solving a SVM model. This problem was presented explicitly in equations $(5)$ and $(8)$. It will in later sections become clear that when solving these problems, we need to calculate inner products $\\textbf{x}_i^T\\textbf{x}_j$ (linear SVM) and $\\textbf{z}_i^T\\textbf{z}_j=\\Phi(\\textbf{x}_i)^T\\Phi(\\textbf{x}_j)$ (nonlinear SVM). This seems simple enough, as it is in many cases but one can find transformations $\\Phi$ which map the inputs $\\textbf{x}$ into infinite dimensional vectors (i.e. $d=\\infty$). It is obvious, that we can not calculate the inner products of infinite dimensional vectors $\\textbf{z}$, since this would require us first to explicitly calculate the infinite vectors $\\textbf{z}$. Fortunately though, there exists cool functions which allow us to calculate these infinite inner products without explicitly knowing the vectors $\\textbf{z}$. These functions are called _kernel functions_, which describe the inner products of vectors in some (possibly infinite dimensional) space $\\mathcal{Z}$. To be explicit, the kernel functions defined by transformation $\\Phi$ are defined as: \n",
    "\n",
    "$$K_{\\Phi}(\\textbf{x}, \\textbf{x}')\\equiv \\Phi(\\textbf{x})^T\\Phi(\\textbf{x}').$$\n",
    "\n",
    "In other words, kernel function $K_{\\Phi}$ takes two vectors $\\textbf{x}, \\textbf{x}' \\in \\mathcal{X}$ as input and returns the inner product of $\\Phi$-transformed vectors in $\\mathcal{Z}$-space. This process is called the _kernel trick_, where the trick part comes from the fact that we do not need to explicitly calculate the vectors in $\\mathcal{Z}$-space in order to calculate their inner product. To give few examples of common kernel functions, a first example is the _polynomial kernel function_ of $Q$-degree defined as: \n",
    "\n",
    "$$K(\\textbf{x}, \\textbf{x}') \\equiv (\\lambda + \\gamma\\textbf{x}^T\\textbf{x}')^Q,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(10)$$\n",
    "\n",
    "where $\\lambda, \\gamma > 0$. Another  common example is the _Gaussian radial basis function kernel_ defined as: \n",
    "\n",
    "$$K(\\textbf{x}, \\textbf{x}') \\equiv \\exp\\left(-\\frac{||\\textbf{x}-\\textbf{x}'||^2}{2\\sigma^2}\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(11)$$\n",
    "\n",
    "where $\\sigma > 0$. It is straightforward to show, that if we set $\\sigma = 1$ in equation $(11)$, then we have\n",
    "\n",
    "$$K(\\textbf{x}, \\textbf{x}') = \\exp\\left(-\\frac12||\\textbf{x}-\\textbf{x}'||^2\\right) = \\sum_{k=0}^{\\infty} \\frac{(\\textbf{x}^T\\textbf{x}')^k}{k!}\\exp\\left(-\\frac{1}{2}||\\textbf{x}||^2\\right)\\exp\\left(-\\frac{1}{2}||\\textbf{x}'||^2\\right),$$\n",
    "\n",
    "which represents an inner product in an _infinite dimensional_ $\\mathcal{Z}$-space (the $\\Phi$-function produces an infinite dimensional vector). It turns out also, that one can construct his own kernel functions $K$, if it is valid that the symmetric matrix: \n",
    "\n",
    "$$K_{M}=\\begin{bmatrix}\n",
    "    K(\\textbf{x}_1, \\textbf{x}_1) & K(\\textbf{x}_1, \\textbf{x}_2) & \\cdots &  K(\\textbf{x}_1, \\textbf{x}_n) \\\\\n",
    "    K(\\textbf{x}_2, \\textbf{x}_1) & K(\\textbf{x}_2, \\textbf{x}_2) & \\cdots &  K(\\textbf{x}_2, \\textbf{x}_n) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    K(\\textbf{x}_n, \\textbf{x}_1) & K(\\textbf{x}_n, \\textbf{x}_2) & \\cdots &  K(\\textbf{x}_n, \\textbf{x}_n) \n",
    "\\end{bmatrix}, \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(12)$$\n",
    "\n",
    "is positive semidefinite (i.e. $\\textbf{x}_i^TK_{M}\\textbf{x}_i \\geq 0\\; \\forall\\; i$) for all vectors $\\{\\textbf{x}_1, \\textbf{x}_2, ..., \\textbf{x}_n\\}$. This condition is called _Mercer's condition_. In other words, if your $K$ satisfies Mercer's condition, then you have a valid kernel function.  \n",
    "\n",
    "<a id='4.'></a>\n",
    "#### 4. Solving the optimal SVM\n",
    "We have so far gone through the core ideas of SVM models: the maximum margin classifier, VC-inequality, linearly/non-linearly separable data and the kernel methods. We have observed, that in order to solve the SVM classifier $(\\textbf{w}, b)$ we need to solve the problem in equation $(5)$:\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "&\\underset{\\textbf{w}, b}{\\text{minimize:}}        &\\qquad& \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "&\\text{subject to:} &      & y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)\\geq 1\\;\\;\\;(i=1,2, ..., n).\n",
    "\\end{alignat}\n",
    "\n",
    "Lets now go on and solve this problem in a toy example. Let the input data be defined in the following way:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    0 & 0 \\\\\n",
    "    2 & 2 \\\\\n",
    "    2 & 0 \\\\\n",
    "    3 & 0 \n",
    "\\end{bmatrix}\\;\\;\\;\\textbf{y} = \\begin{bmatrix}\n",
    "    -1 \\\\ -1 \\\\ +1 \\\\ +1 \n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where matrix $X$ represents the container of input vectors of $\\textbf{x}$ and $\\textbf{y}$ represents the class labels correspondingly. Each row in $X$ and $\\textbf{y}$ correspond to a single data point. Formulating the problem of equation $(5)$ in terms of this data we get: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{w_1, w_2, b}{\\text{minimize:}}\n",
    "& & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{1}{2}(w_1^2+w_2^2) \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\begin{aligned}[c]\n",
    "-b & \\geq 1\\;\\;(\\text{i})\\\\\n",
    "-(2w_1+2w_2+b) & \\geq 1 \\;\\; (\\text{ii})\\\\\n",
    "2w_1+b & \\geq 1 \\;\\; (\\text{iii})\\\\\n",
    "3w_1+b & \\geq 1 \\;\\; (\\text{iv}).\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "By combining inequalities (i), (iii) and (ii), (iii) together we get the conditions $w_1 \\geq 1$ and $w_2 \\leq -1$. By now squaring both these two new conditions and combining them we get the condition $\\frac{1}{2}(w_1^2+w_2^2)\\geq 1$. From this we see that the minimum is achieved when (with conditions satisfied) $w_1 = 1$ ja $w_2 = -1$. Substituting these values of $w_1, w_2$ into inequalities (ii)-(iv) we get that $b=-1$ and so the SVM model in this case is: \n",
    "\n",
    "$$(w_1^*, w_2^*, b^*) = (1, -1, -1),$$  \n",
    "\n",
    "and so in terms of equation $(6)$ we get the SVM classifier as:  \n",
    "\n",
    "\\begin{equation*}\n",
    "g(\\textbf{x}) = \\text{sign}\\left(x_1 - x_2 -1 \\right).\n",
    "\\end{equation*}\n",
    "\n",
    "Notice that the width of the margin in this case is $\\frac{1}{||\\textbf{w}^*||}=\\frac{1}{\\sqrt{2}}\\approx 0.707$. This is illustrated in figure X, with the distance from the line to the support vectors is $\\frac{1}{\\sqrt{2}}$. It was fairly easy to solve this particular SVM model but in general it is not this easy. The problem might have a lot more parameters and inequalities in which case we need to use more sophisticated methods to solve the SVM model. We will next consider _quadratic programming_, which we use for solving more general SVM models. \n",
    "\n",
    "<a id='4.1'></a>\n",
    "#### 4.1 Quadratic programming\n",
    "In this section we will aim to solve the parameters $(\\textbf{w}, b)$ of the SVM model in equation $(6)$ using the methods of quadratic programming (QP). In a general QP-problem, we aim to solve problem:    \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{a}_i^T\\textbf{u} \\geq c_i\\;\\;\\;\\;\\;\\left(i = 1, ..., m\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(13)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $Q$ is a $l\\times l$ symmetric positive semidefinite matrix, $\\textbf{u}, \\textbf{p}, \\textbf{a}_i$ are $l$-dimensional vectors and $c_i$ is some constant. The optimization problem in $(13)$ contains a qudratic term $\\frac{1}{2}\\textbf{u}^T Q\\textbf{u}$ and a linear term $\\textbf{p}^T\\textbf{u}$. The constraints $\\textbf{a}_i^T\\textbf{u}\\geq c_i$ are linear. Because $Q$ is positive semidefinite the problem in $(13)$ is a convex optimization problem. If the vectors $\\textbf{a}_i$ and constants $c_i$ are grouped into matrix $A$ and vector $\\textbf{c}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left[\\begin{array}{c}\n",
    "\\textbf{a}_1^T  \\\\\n",
    "\\vdots   \\\\\n",
    "\\textbf{a}_m^T\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{c} = \\left[\\begin{array}{c}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_m\n",
    "\\end{array}\\right],\n",
    "\\end{equation*}\n",
    "\n",
    "then the optimization problem $(13)$ can be presented in the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{quadprog2}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^T Q\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A\\textbf{u} \\geq \\textbf{c}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The constraints of problem $(13)$ can also be presented in the form $A\\textbf{u}\\leq \\textbf{c}$ by multiplying both sides with $-1$. Next we will show that the SVM optimization problem in $(5)$ can be represented as a QP-problem. In order to do this, we must identify $A, Q, \\textbf{u}, \\textbf{p}$ ja $\\textbf{c}$ from problem $(5)$. This requires finding the optimal values of $(\\textbf{w}, b)$ so we have now: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\textbf{u} = \\left[ \\begin{array}{c}\n",
    "b\\\\\n",
    "\\textbf{w}\n",
    "\\end{array} \\right] \\in \\mathbb{R}^{d+1},\n",
    "\\end{equation*}\n",
    "\n",
    "so $l=d+1$. Our task is to minimize the term $\\frac{1}{2}\\textbf{w}^T\\textbf{w}$, which must be presented in the form $\\frac{1}{2}\\textbf{u}^T Q\\textbf{u} + \\textbf{p}^T\\textbf{u}$. We note that:  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\textbf{w}^T\\textbf{w} = \\left[b \\;\\;\\textbf{w}^T \\right]\\left[\\begin{array}{cc}\n",
    "0 & \\textbf{0}_d^T\\\\\n",
    "\\textbf{0}_d & \\textbf{I}_d\n",
    "\\end{array}\\right]\n",
    "\\left[ \\begin{array}{c}\n",
    "b\\\\\n",
    "\\textbf{w}\n",
    "\\end{array} \\right],\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\textbf{I}_d$ is a $d\\times d$ identity matrix and $\\textbf{0}_d$ is a $d$-dimensional zero vector. From this we see that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q = \\left[\\begin{array}{cc}\n",
    "0 & \\textbf{0}_d^T\\\\\n",
    "\\textbf{0}_d & \\textbf{I}_d\n",
    "\\end{array}\\right] \\;\\;\\;\\;\\;\\;\n",
    "\\textbf{p} = \\textbf{0}_{d+1},\n",
    "\\end{equation*}\n",
    "\n",
    "where $Q$ is positive semidefinite. Because there are total of $n$ $y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right) \\geq 1$ inequalities we get that $m=n$. Also note that these inequalities are equivalent with the inequalities:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left[y_i \\;\\; y_i\\textbf{x}_i^T\\right]\\textbf{u} \\geq 1,\n",
    "\\end{equation*}\n",
    "\n",
    "and so by setting $\\textbf{a}_i^T = y_i\\left[1 \\;\\; \\textbf{x}_i^T\\right]$ and $c_i=1$ in problem $(13)$ we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left[\\begin{array}{cc}\n",
    "y_1 & y_1\\textbf{x}_1^T \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "y_n & y_n\\textbf{x}_n^T\n",
    "\\end{array}\\right] \\;\\;\\;\\;\\;\\;\n",
    "\\textbf{c} = \\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore we have shown that problem $(5)$ is indeed a QP-problem of form $(13)$. Next, we will take a look at the Lagrangian duality form for hard margin SVM, which makes solving the problem $(13)$ easier and brings in the kernel methods introduced in section 3.4. We will also go through why the problem $(5)$ includes calculating the inner products $\\textbf{x}_i^T\\textbf{x}_j, \\forall i,j \\in \\{1, ..., n\\}$ as we promised earlier in section 3.4.\n",
    "\n",
    "<a id='4.2'></a>\n",
    "#### 4.2 Lagrangian dual\n",
    "In section 3, we discussed about applying the nonlinear transformation $\\Phi: \\mathcal{X} \\to \\mathcal{Z} \\subset \\mathbb{R}^q$ to the input vectors and we ended up into the optimization problem $(8)$. In this problem, we have $q+1$ variables to solve since $\\textbf{u} = [\\tilde{\\textbf{w}}, \\tilde{b}] \\in \\mathbb{R}^{q+1}$. It is quite difficult to solve this problem if $q$ is very large or even infinite $(q=\\infty)$. By transforming the problem $(8)$ called the _primal_ into another form called the _Lagrangian dual form_ the SVM problem is transformed into a QP-problem with $n$ variables to be solved with $n+1$ constraints. The dual problem is independent of the dimensionality of the space $\\mathcal{Z}$, and depends only from the amount of data points $n$. This is a very useful method to apply especially when $q$ is very large. We will next see how to transform the QP-problem in $(13)$ into a dual form. To begin, we start from the primal form of the problem: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{a}_i^T\\textbf{u} \\geq c_i\\;\\;\\;\\;\\;\\left(i = 1, ..., m\\right).\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We proceed next by dropping out the constraints in the above problem and adding a _penalty term_ in the following way: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} +  \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\sum_{i=1}^m \\alpha_i \\left( c_i - \\textbf{a}_i^T\\textbf{u} \\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(14)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\boldsymbol\\alpha = \\left(\\alpha_1, ..., \\alpha_m\\right)$. It can be shown (see e.g. literature on nonlinear programming) that the primal problem $(13)$ is equivalent with this new problem $(14)$, as long as there exists at least one solution which satisfies the constraints of problem $(13)$. The penalty term in $(14)$ encourages the optimization to choose vectors $\\textbf{u}$ such that $c_i-\\textbf{a}_i^T\\textbf{u}\\leq 0$ (because $\\alpha_i \\geq 0$), satisfying thus the constraints in $(13)$. Notice that in $(14)$, at the optimum solution $\\left(\\textbf{u}^*, \\boldsymbol\\alpha^*\\right)$ we have either $\\alpha^*_i = 0$ or $c_i - \\textbf{a}_i^T\\textbf{u}^*=0\\;\\forall\\,i$. Thus, we can instead of problem $(13)$ solve the simpler unconstrained problem $(14)$. The function in $(14)$ is called the _Lagrangian_ which is defined as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right) = \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} + \\sum_{i=1}^m \\alpha_i \\left( c_i - \\textbf{a}_i^T\\textbf{u} \\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(15)\n",
    "\\end{equation}\n",
    "\n",
    "so our task is to solve the optimization task: \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{LagrangeOpt}\n",
    "\\min_{\\textbf{u}\\in \\mathbb{R}^l}\\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(16)\n",
    "\\end{equation}\n",
    "\n",
    "In convex quadratic programming we can take advantage of the so-called _strong duality_: \n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\textbf{u}\\in \\mathbb{R}^l}\\max_{\\boldsymbol\\alpha \\geq \\textbf{0}} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right) = \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}}\\min_{\\textbf{u}\\in \\mathbb{R}^l} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(17)\n",
    "\\end{equation}\n",
    "\n",
    "which can be shown to be true if the Lagrangian $\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right)$ has the form as in $(15)$ and there exists a solution satisying the constraints $\\textbf{a}_i^T\\textbf{u}\\geq c_i$. A proof for these facts can be found from literature related to quadratic programming and convex optimization. With the help of the Lagrangian and strong duality we have transformed the original problem $(13)$ into an easier unconstrained optimization problem:   \n",
    "\n",
    "\\begin{equation}\n",
    " \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}}\\min_{\\textbf{u}\\in \\mathbb{R}^l} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right),\n",
    "\\end{equation}\n",
    "\n",
    "which is called the _Lagrangian dual problem_. To wrap up this section, we will state the necessary conditions a solution of the QP-problem in $(13)$ must satisfy.\n",
    "\n",
    "**Necessary optimality conditions for the QP-problem $(13)$.** If the primal convex QP-problem in $(13)$: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{u}\\in \\mathbb{R}^l}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{a}_i^T\\textbf{u} \\geq c_i\\;\\;\\;\\;\\;\\left(i = 1, ..., m\\right),\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "has the corresponding Lagrangian function in $(15)$: \n",
    "\n",
    "$$\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right) = \\frac{1}{2}\\textbf{u}^TQ\\textbf{u} + \\textbf{p}^T\\textbf{u} + \\sum_{i=1}^m \\alpha_i \\left( c_i - \\textbf{a}_i^T\\textbf{u} \\right),$$ \n",
    "\n",
    "then the solution $\\textbf{u}^*$ is optimal for the primal problem $(13)$, if and only if $\\left(\\textbf{u}^*, \\boldsymbol\\alpha^*\\right)$ is a solution for the dual problem in $(14)$:\n",
    "\n",
    "$$ \\max_{\\boldsymbol\\alpha \\geq \\textbf{0}}\\min_{\\textbf{u}\\in \\mathbb{R}^l} \\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right).$$\n",
    "\n",
    "The optimal solution $\\left(\\textbf{u}^*, \\boldsymbol\\alpha^*\\right)$ of problem $(14)$ must satisfy the following conditions: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}[c]\n",
    "(\\text{i}) & \\;\\;\\;\\textbf{a}_i^T\\textbf{u}^* \\geq c_i\\;\\;\\text{and}\\;\\;\\alpha_i^* \\geq 0\\;\\;\\forall\\,i.\\\\[2mm]\n",
    "(\\text{ii}) & \\;\\;\\; \\alpha_i^*\\left( \\textbf{a}_i^T\\textbf{u}^* - c_i \\right) = 0\\;\\;\\forall\\,i.\\\\[2mm]\n",
    "(\\text{iii}) & \\;\\;\\;\\nabla_{\\textbf{u}}\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right)|_{\\textbf{u}=\\textbf{u}^*,\\; \\boldsymbol\\alpha = \\boldsymbol\\alpha^*} = \\textbf{0},\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\nabla_{\\textbf{u}}$ denotes the gradient with respect to $\\textbf{u}$. The conditions (i)-(iii) are called the Karush-K$\\ddot{\\text{u}}$hn-Tucker (KKT)-conditions. The KKT-conditions give us an analytical way to either check or solve the optimal solution of the QP-problem $(13)$. At the last sections of this tutorial, we will produce an iterative algorithm for solving the SVM model and we therefore do not directly apply the KKT-conditions for solving the optimal SVM model. We can however use the KKT-conditions to check whether our iteratively solved solution is indeed optimal or not.  \n",
    "\n",
    "<a id='4.3'></a>\n",
    "#### 4.3 The dual of hard margin SVM\n",
    "Lets now next get back to solving the SVM model. In section 4.1, we identified the factors $A, Q, \\textbf{u}, \\textbf{p}, \\textbf{c}$ in order to incorporate the SVM problem $(5)$ into the form in $(13)$. We will now proceed to find the Lagrangian dual function $\\mathcal{L}\\left(\\textbf{u}, \\boldsymbol\\alpha \\right)$ of the SVM problem. To recall, our original problem was: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{b,\\textbf{w}}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\textbf{w}^T\\textbf{w} \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\;y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right) \\geq 1\\;\\;\\;\\;\\;\\;\\;(i = 1, ..., n).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The optimal solution we aim to find for this problem is the vector $\\textbf{u} = [b, \\textbf{w}]^T$. Lets go on and solve the Lagrangian function. Because there are $n$ contraints, we will get $n$ penalty terms into the Lagrangian with coefficients $\\alpha_i$. The Lagrangian function is therefore: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(b, \\textbf{w}, \\boldsymbol\\alpha\\right) &= \\frac{1}{2}\\textbf{w}^T\\textbf{w} + \\sum_{i=1}^n \\alpha_i\\left(\n",
    "1-y_i\\left(\\textbf{w}^T\\textbf{x}_i+b\\right)\\right)\\\\\n",
    "&= \\frac{1}{2}\\textbf{w}^T\\textbf{w} - \\sum_{i=1}^n\\alpha_iy_i\\textbf{w}^T\\textbf{x}_i - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(18)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Our first step is to minimize the Lagrangian function with respect to the vector $\\textbf{u}$ and then maximize with respect to $\\boldsymbol\\alpha \\geq \\textbf{0}$. So lets take the derivative of $\\mathcal{L}$ with respect to $b$ and $\\textbf{w}$. We get: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^n\\alpha_iy_i \\;\\;\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;\\;\\;\\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} = \\textbf{w}- \\sum_{i=1}^n\\alpha_iy_i\\textbf{x}_i,\n",
    "\\end{equation*}\n",
    "\n",
    "and setting these to zero we get: \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n\\alpha_iy_i = 0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(19)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{w} = \\sum_{i=1}^n\\alpha_iy_i\\textbf{x}_i.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(20)\n",
    "\\end{equation}\n",
    "\n",
    "By plugging these into the Lagrangian function $\\mathcal{L}\\left(b, \\textbf{w}, \\boldsymbol\\alpha\\right)$ above we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(b, \\textbf{w}, \\boldsymbol\\alpha\\right) &= \\frac{1}{2}\\textbf{w}^T\\textbf{w} - \\sum_{i=1}^n\\alpha_iy_i\\textbf{w}^T\\textbf{x}_i - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i\\\\\n",
    "& = \\frac{1}{2}\\sum_{i=1}^n\\alpha_iy_i\\textbf{x}_i^T\\sum_{j=1}^n\\alpha_jy_j\\textbf{x}_j - \\sum_{i=1}^n\\alpha_iy_i\\sum_{j=1}^n\\alpha_jy_j\\textbf{x}_j^T\\textbf{x}_i \\\\ \n",
    "& - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "&= \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^ny_iy_j\\alpha_i\\alpha_j\\textbf{x}_i^T\\textbf{x}_j - \\sum_{i=1}^n\\sum_{j=1}^ny_iy_j\\alpha_i\\alpha_j\\textbf{x}_i^T\\textbf{x}_j + \\sum_{i=1}^n\\alpha_i \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^ny_iy_j\\alpha_i\\alpha_j\\textbf{x}_i^T\\textbf{x}_j + \\sum_{i=1}^n\\alpha_i,\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "and so we get that: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) = -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^ny_iy_j\\alpha_i\\alpha_j\\textbf{x}_i^T\\textbf{x}_j + \\sum_{i=1}^n\\alpha_i,\n",
    "\\end{equation}\n",
    "\n",
    "which is a function of only the vector $\\boldsymbol\\alpha \\geq \\textbf{0}$. Thus, in order to solve the SVM model we need to maximize the function $\\mathcal{L}\\left(\\boldsymbol\\alpha\\right)$ so that $\\boldsymbol\\alpha \\geq \\textbf{0}$. Note that we have now new constraints for the variables $\\alpha_i$ by the equation $(19)$, from which we get a total of $n+1$ constraints. Instead of maximizing the function $\\mathcal{L}\\left(\\boldsymbol\\alpha\\right)$ we can also equivalently minimize the function $-\\mathcal{L}\\left(\\boldsymbol\\alpha\\right)$, and so we have transformed the original optimization problem $(5)$ into the problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^ny_iy_j\\alpha_i\\alpha_j\\textbf{x}_i^T\\textbf{x}_j - \\sum_{i=1}^n\\alpha_i \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(21)\\\\\n",
    "& \\text{subject to:}\n",
    "  \\;\\;\\;\\;\\;\\; \\sum_{i=1}^n\\alpha_iy_i = 0 \\\\\n",
    "& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha_i \\geq 0\\;\\;\\;\\;\\;\\;\\;(i = 1, ..., n).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "From problem $(21)$ we see why the calculation of inner products $\\textbf{x}_i^T\\textbf{x}_j, \\forall \\:i, j \\in \\{1,...,n\\}$ is important as we discussed before in section 3.4. The problem $(21)$ can also be rewritten equivalently as the convex QP-problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A_D\\boldsymbol\\alpha \\geq \\textbf{0}_{n+2},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(22)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{ccc}\n",
    "y_1y_1\\textbf{x}_1^T\\textbf{x}_1 & \\cdots & y_1y_n\\textbf{x}_1^T\\textbf{x}_n \\\\\n",
    "y_2y_1\\textbf{x}_2^T\\textbf{x}_1 & \\cdots & y_2y_n\\textbf{x}_2^T\\textbf{x}_n \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "y_ny_1\\textbf{x}_n^T\\textbf{x}_1 & \\cdots & y_ny_n\\textbf{x}_n^T\\textbf{x}_n\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "A_D = \\left[\\begin{array}{c}\n",
    "\\textbf{y}^T \\\\\n",
    "-\\textbf{y}^T \\\\\n",
    "\\textbf{I}_{n\\times n}\n",
    "\\end{array}\\right]\n",
    "\\end{equation*}\n",
    "\n",
    "and $\\textbf{I}_{n\\times n}, \\textbf{y}^T, \\textbf{0}_{n+2}, \\textbf{1}_{n}$ stand for $n\\times n$ identity matrix, row vector of labels $\\{y_1, y_2, ..., y_n\\}$, $(n+2)$-dimensional zero vector and $n$-dimensional vector of ones respectively. It can be shown, that if $Q_D$ is positive semidefinite, then the problem $(21)$ is a convex optimization problem (again, to verify check the literature). When the optimal solution $\\boldsymbol\\alpha^*$ for this problem has been solved, we get the parameters $(\\textbf{w}^*, b^*)$ of the optimal SVM hyperplane as (see equations 2 and 20): \n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{w}^* = \\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(23)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "b^* &= \\frac{1}{y_s} - \\textbf{w}^{*T}\\textbf{x}_s\\\\\n",
    "&= \\frac{1}{y_s} -\\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i^T\\textbf{x}_s,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(24)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textbf{x}_s$ is any support vector satisfying $\\alpha_s^* > 0$. For all non-support vectors it holds that $\\alpha_i^* = 0$. Because of the constraint in equation $(2)$, it holds for support vectors $\\textbf{x}_s$ that:\n",
    "\n",
    "$$y_s\\left(\\textbf{w}^{*T}\\textbf{x}_s + b^*\\right) = 1.$$ \n",
    "\n",
    "The optimal SVM hyperplane model $(\\textbf{w}^*, b^*)$ is therefore:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "g(\\textbf{x}) &= \\text{sign}\\left(\\textbf{w}^{*T}\\textbf{x} + b^*\\right)\\\\\n",
    "&= \\text{sign}\\left(\\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i^T\\textbf{x} + b^*\\right)\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(25)\\\\\n",
    "&= \\text{sign}\\left(\\sum_{i=1}^ny_i\\alpha_i^*\\textbf{x}_i^T\\left(\\textbf{x}-\\textbf{x}_s\\right) + y_s\\right).\n",
    "\\end{aligned}\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "Also, because only the support vectors $\\textbf{x}_s$ affect the selection of the SVM hyperplane $(\\alpha_s^* > 0)$ we can write the model in $(25)$ in terms of the support vectors as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "g(\\textbf{x}) = \\text{sign}\\left(\\sum_{\\alpha^*_i > 0} y_i\\alpha_i^*\\textbf{x}_i^T\\textbf{x} + b^*\\right).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(26)\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "<a id='4.4'></a>\n",
    "#### 4.4 Example: solving the SVM classifier via the Lagrangian dual form\n",
    "Let the observed data in this example be the same as earlier, that is: \n",
    "\n",
    "\\begin{equation*}\n",
    "X = \\left[\\begin{array}{cc}\n",
    "0 & 0 \\\\\n",
    "2 & 2 \\\\\n",
    "2 & 0 \\\\\n",
    "3 & 0\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{y} = \\left[\\begin{array}{c}\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "+1 \\\\\n",
    "+1\n",
    "\\end{array}\\right].\\\\[6mm]\n",
    "\\end{equation*}\n",
    "\n",
    "Here we have that $n=4$. The matrices $Q_D$ and $A_D$ are identified as: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{rrrr}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 8 & -4 & -6\\\\\n",
    "0 & -4 & 4 & 6\\\\\n",
    "0 & -6 & 6 & 9\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "A_D = \\left[\\begin{array}{rrrr}\n",
    "-1 & -1 & 1 & 1 \\\\\n",
    "1 & 1 & -1 & -1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "By taking advantage of $(22)$ we get our optimization problem as minimizing the Lagrangian function: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\mathcal{L}\\left(\\boldsymbol\\alpha\\right) =\n",
    "  4\\alpha_2^2 + 2\\alpha_3^2 + \\frac{9}{2}\\alpha_4^2 - 4\\alpha_2\\alpha_3 -6\\alpha_2\\alpha_4 + 6\\alpha_3\\alpha_4 -\\alpha_1 -\\alpha_2-\\alpha_3-\\alpha_4 \\\\[2mm]\n",
    "& \\text{subject to:}\n",
    "  \\;\\;\\;\\;\\;\\; \\alpha_1 + \\alpha_2 = \\alpha_3 + \\alpha_4; \\\\\n",
    "& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4 \\geq 0.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "By plugging the first constraint equation into the Lagrangian function we get: \n",
    "\n",
    "$$\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) =\n",
    "  4\\alpha_2^2 + 2\\alpha_3^2 + \\frac{9}{2}\\alpha_4^2 - 4\\alpha_2\\alpha_3 -6\\alpha_2\\alpha_4 + 6\\alpha_3\\alpha_4 -2\\alpha_3-2\\alpha_4.$$\n",
    "  \n",
    "Now we calculate the partial derivative with respect to parameter $\\alpha_2$ and setting this to zero we get:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_2} = 8\\alpha_2-4\\alpha_3-6\\alpha_4 = 0\\;\\;\\;\\;\\leftrightarrow\\;\\;\\;\\; \\alpha_2 = \\frac{1}{2}\\alpha_3 + \\frac{3}{4}\\alpha_4.$$\n",
    "\n",
    "It then follows that:\n",
    "\n",
    "$$\\alpha_1 = \\alpha_3 + \\alpha_4 - \\alpha_2 = \\frac{1}{2}\\alpha_3 +\\frac{1}{4}\\alpha_4.$$\n",
    "\n",
    "Note also that $\\alpha_1, \\alpha_2 \\geq 0$. By plugging $\\alpha_1$ and $\\alpha_2$ into the Lagrangian we get: \n",
    "\n",
    "$$\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) = \\alpha_3^2+\\frac{9}{4}\\alpha_4^2+3\\alpha_3\\alpha_4-2\\alpha_3-2\\alpha_4.$$\n",
    "\n",
    "Next we note that:\n",
    "\n",
    "$$\\mathcal{L}\\left(\\boldsymbol\\alpha\\right) = \\frac{1}{4}\\left(2\\alpha_3 + 3\\alpha_4-2\\right)^2+\\alpha_4-1,$$\n",
    "\n",
    "and so taking into account the constraints $\\alpha_3, \\alpha_4\\geq 0$ we see that the minimum is achieved at the point $\\alpha_3 = 1, \\alpha_4 = 0$ and so: \n",
    "\n",
    "$$\\alpha_1 = \\frac{1}{2}\\;\\;\\;\\;\\text{ja}\\;\\;\\;\\;\\alpha_2 = \\frac{1}{2},$$\n",
    "\n",
    "so the solution is: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\boldsymbol\\alpha^* = \\left[\\begin{array}{c}\n",
    "1/2\\\\\n",
    "1/2\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "Now by using equations $(23)$ and $(24)$ we get:\n",
    "\n",
    "$$\\textbf{w}^*=-\\frac{1}{2}\\textbf{x}_1-\\frac{1}{2}\\textbf{x}_2 + \\textbf{x}_3 = (0, 0) - (1, 1) + (2, 0) = (1, -1),$$\n",
    "\n",
    "$$b^* = -1 + \\textbf{w}^{*T}\\textbf{x}_1 = -1 + 0 = -1,$$\n",
    "\n",
    "where $\\textbf{x}_1, \\textbf{x}_2, \\textbf{x}_3$ are support vectors. According to equation $(25)$ the optimal SVM classifier is therefore:\n",
    "\n",
    "$$g(\\textbf{x}) = \\text{sign}\\left(x_1 -x_2 -1 \\right),$$\n",
    "\n",
    "just like we obtained in the earlier example without using the Lagrangian dual form.\n",
    "\n",
    "<a id='4.5'></a>\n",
    "#### 4.5 The dual in the $\\mathcal{Z}$-space\n",
    "So what part do the kernel methods play in the SVM model? If we incorporate them into the SVM model does the developed theory change much? The answer is no, it is actually very easy to add the kernel methods into the developed SVM models, namely into the problem in $(21)$. We simply replace the inner products $\\textbf{x}_i^T\\textbf{x}_j$ with the inner products of space $\\mathcal{Z}$, that is: \n",
    "\n",
    "$$\\textbf{z}_i^T\\textbf{z}_j = \\Phi\\left(\\textbf{x}_i\\right)^T\\Phi\\left(\\textbf{x}_j\\right) = K\\left(\\textbf{x}_i, \\textbf{x}_j\\right),$$\n",
    "\n",
    "and so the problem $(21)$ becomes: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^ny_iy_j\\alpha_i\\alpha_j K\\left(\\textbf{x}_i, \\textbf{x}_j\\right) - \\sum_{i=1}^n\\alpha_i \\\\\n",
    "& \\text{subject to:}\n",
    "  \\;\\;\\;\\;\\;\\; \\sum_{i=1}^n\\alpha_iy_i = 0 \\\\\n",
    "& \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha_i \\geq 0\\;\\;\\;\\;\\;\\;\\;(i = 1, ..., n).\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(27)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The matrix $Q_D$ in problem $(22)$ becomes the so-called _kernel matrix_:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{ccc}\n",
    "y_1y_1K\\left(\\textbf{x}_1, \\textbf{x}_1\\right) & \\cdots & y_1y_nK\\left(\\textbf{x}_1, \\textbf{x}_n\\right) \\\\\n",
    "y_2y_1K\\left(\\textbf{x}_2, \\textbf{x}_1\\right) & \\cdots & y_2y_nK\\left(\\textbf{x}_2, \\textbf{x}_n\\right) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "y_ny_1K\\left(\\textbf{x}_n, \\textbf{x}_1\\right) & \\cdots & y_ny_nK\\left(\\textbf{x}_n, \\textbf{x}_n\\right)\n",
    "\\end{array}\\right],\n",
    "\\end{equation*}\n",
    "\n",
    "and so, together with the nonlinear transformation $\\Phi$ the SVM model of $(26)$ becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "g(\\textbf{x}) = \\text{sign}\\left(\\sum_{\\alpha^*_i > 0} y_i\\alpha_i^*K\\left(\\textbf{x}_i, \\textbf{x}\\right) + b^*\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(28)\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "where now $b^* = y_s - \\sum_{\\alpha_i^* > 0}y_i\\alpha_i^*K(\\textbf{x}_i, \\textbf{x}_s)$ and $\\textbf{x}_s$ is any support vector with $\\alpha_s^*>0$ (and corresponding $y_s$). \n",
    "\n",
    "Phew, we have now gone through everything we need to solve the SVM model of problem $(5)$. Now it's time to get hands dirty with the most interesting part of this tutorial, that is actually solving the model $(\\textbf{w}^*, b^*)$. We will also see (but not go through the proof), that with only slight modifications to the SVM problem we can solve either soft or hard margin SVM models. In fact, hard margin SVM is actually a special case of the soft margin SVM model.  \n",
    "\n",
    "<a id='5.'></a>\n",
    "#### 5. Simple algorithm for solving the SVM model\n",
    "Recall from previous sections, that the (hard margin) SVM model was solved by finding the optimal solution to the problem $(22)$: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A_D\\boldsymbol\\alpha \\geq \\textbf{0}_{n+2},\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_D = \\left[\\begin{array}{ccc}\n",
    "y_1y_1K\\left(\\textbf{x}_1, \\textbf{x}_1\\right) & \\cdots & y_1y_nK\\left(\\textbf{x}_1, \\textbf{x}_n\\right) \\\\\n",
    "y_2y_1K\\left(\\textbf{x}_2, \\textbf{x}_1\\right) & \\cdots & y_2y_nK\\left(\\textbf{x}_2, \\textbf{x}_n\\right) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "y_ny_1K\\left(\\textbf{x}_n, \\textbf{x}_1\\right) & \\cdots & y_ny_nK\\left(\\textbf{x}_n, \\textbf{x}_n\\right)\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "A_D = \\left[\\begin{array}{c}\n",
    "\\textbf{y}^T \\\\\n",
    "-\\textbf{y}^T \\\\\n",
    "\\textbf{I}_{n\\times n}\n",
    "\\end{array}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "Note that above we have incorporated the transformation $\\Phi$ into the game in order to make the problem as general as possible. In the case of a _linear kernel_ (as in $(22)$), we simply have $K(\\textbf{x}_i, \\textbf{x}_j) = \\textbf{x}_i^T\\textbf{x}_j$. In order to also incorporate the soft margin SVM into the above problem, we modify it slightly by adding a _penalty rate_ $C\\in \\mathbb{R}^+$ in the following way: \n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; \\textbf{y}^T\\boldsymbol\\alpha = 0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(29)\n",
    "\\\\\n",
    "& & & \\;\\textbf{0} \\leq \\boldsymbol\\alpha \\leq C\\cdot \\textbf{1}.\n",
    "\\end{aligned}\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "It is interesting to see that the above most general SVM problem so far (general $\\Phi$, soft margin) in $(29)$ differs only a little bit in the contraint part when compared to $(22)$. To skip unnecessary complications, we leave the proofs of this fact for the reader to find out from related literature. The penalty term $C$ controls the amount of strictness we put on perfect classification of the SVM model. The higher the value of $C$ is, the 'harder' our model is. In fact, when we have $C=\\infty$ then the problem in $(29)$ is equal to the hard margin problem in $(22)$. To mention lastly, it is a bit more complicated to solve the parameter $b^*$ in the case of a soft margin SVM, but I will get into that later. Next, we will have a short review on the techniques of nonlinear programming that our algorithm will be based upon. \n",
    "\n",
    "<a id='5.1'></a>\n",
    "#### 5.1 Active set methods\n",
    "The active set method, is a simple technique where the goal is to keep track of the so-called _active contraints_ and proceed with the problem optimization in the subspace defined by these constraints. The contraints in our SVM problem in $(29)$ form a closed convex subspace where we are to apply our optimization. We can think of the optimization problem in $(29)$ as moving along a convex function with borders set up by a hyperplane defined by the vector $\\boldsymbol\\alpha$. In the active set method, we therefore proceed freely with 'moving' in the convex function until some constraints become active, at which point we change direction so that feasibility is preserved (that is we move only in the allowed subspace). In the SVM problem, our constraints are of the form $A\\textbf{x} \\leq \\textbf{y}$, which are active at point $\\textbf{x}$ if $A\\textbf{x} = \\textbf{y}$ and inactive if $A\\textbf{x} < \\textbf{y}$. The inactive constraints are not relevant for the optimization and so we always need to keep track of only the active constraints. We have now set up the method that keeps track that all the contraints of our problem $(30)$ are satisfied. Next, we will discuss about how we 'move' in the space of $\\boldsymbol\\alpha$s towards the optimum. \n",
    "\n",
    "<a id='5.2'></a>\n",
    "#### 5.2 Gradient projection method of Rosen\n",
    "If you are familiar with basic concepts of mathematical optimization, then you should have heard at some point about _gradient descent_ based algorithms. Gradient descent algorithms work simply by firstly calculating the gradient $\\textbf{g}(\\textbf{x})$ of the (convex) function to $f(\\textbf{x})$ to be minimized (i.e. $\\textbf{g}(\\textbf{x})=\\nabla f(\\textbf{x})$), and secondly updating the current best solution $\\textbf{x}_i$ with the rule $\\textbf{x}_{i+1}=\\textbf{x}_i - \\lambda \\textbf{g}(\\textbf{x}_i)$, where $\\lambda \\geq 0$ is sometimes called the _learning rate_ which determines how much to move into the direction $-\\textbf{g}(\\textbf{x}_i)$. That is, we are moving into the direction of the negative gradient of the function $f(\\textbf{x})$, which is the direction of deepest decrement of the function value. The just described update rule is iteratively applied until convergence to optimum is achieved. Gradient descent is a standard technique of mathematical optimization and it is used in tons of applications and research fields. Many variates of this methods also exist, such as the _conjugate gradient method_ or the _quasi-Newton methods_.\n",
    "\n",
    "One the problems in gradient descent however is that it assumes the optimization process to be _unconstrained_, that is we are free to move in the parameter space. As we have seen above, this is not the case in the SVM problem where we have linear constraints on the $\\boldsymbol\\alpha$-vector. We are thus restricted to conduct the optimization problem in a restricted subspace as we discussed with active set methods above. Fortunately, many techniques exists which allow us to conduct gradient descent while maintaining the _feasibility_ of the new solutions $\\textbf{x}_{i+1}$. A solution $\\textbf{x}$ is called _feasible_, if it is a solution which satisfies the constraints of the optimization problem. One of these techniques that maintain feasibility, is called the _gradient projection method of Rosen_ (GP). GP does exactly what its name suggests, firstly, it project the gradient vector into a feasible subspace, and then an update step is conducted in this restricted space. In other words, we are simply doing gradient descent, but at every step we project the gradient vector into a subspace which preserves feasibility. We will next make this more explicit. Consider the general problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\textbf{x} \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & f(\\textbf{x}) \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A\\textbf{x}\\leq \\textbf{b} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(30)\n",
    "\\\\\n",
    "& & & \\;Q\\textbf{x}=\\textbf{q},\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is an $m\\times n$ matrix, $Q$ is an $l\\times n$ matrix ($l\\geq n$), $b$ is an $m$-dimensional vector, $\\textbf{q}$ is an $l$-dimensional vector, and $f:\\mathbb{R}^n\\to\\mathbb{R}$ is a differentiable function. It can be proven (e.g. nonlinear programming by Bazaraa) that the following lemma is true:\n",
    "\n",
    "*****\n",
    "**Lemma**: Let $\\textbf{x}$ be a feasible point such that $A_1\\textbf{x} =\\textbf{b}_1$ and $A_2\\textbf{x}<\\textbf{b}_2$, where $A^T=(A_1^T, A_2^T)$ and $\\textbf{b}^T=(\\textbf{b}_1^T, \\textbf{b}_2^T)$. Furthermore, suppose that $f$ is differentiable at $\\textbf{x}$ and that $P=\\textbf{I}-M^T\\left(MM^T\\right)^{-1}M$, where $M^T=(A_1^T, Q^T)$ is a full rank matrix. It then follows, that the vector $\\textbf{d}=-P\\nabla f(\\textbf{x})=-P\\textbf{g}(\\textbf{x})$ is an improving feasible direction of $f$ at $\\textbf{x}$.\n",
    "*****\n",
    "\n",
    "The matrix $P$ is called a _projection matrix_ and it is symmetric and positive semidefinite. So, we have all the required components at our disposal. We have formulated the SVM problem in $(29)$, and we have defined the optimization techniques we need for solving this problem, namely GP combined with active set method. Next, we will formulate the pseudocode of the algorithm for solving the problem $(29)$. \n",
    "\n",
    "<a id='5.3'></a>\n",
    "#### 5.3 Constructing a gradient projection/active set-based SVM solver \n",
    "Before constructing our gradient projection/active set-based algorithm (let us call it GPAS), we need to identify the key components of the above lemma in problem $(29)$. First of all, lets rewrite problem $(29)$ to make it resemble more the problem $(30)$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\boldsymbol\\alpha \\in \\mathbb{R}^n}{\\text{minimize:}}\n",
    "& & \\frac{1}{2}\\boldsymbol\\alpha^TQ_D\\boldsymbol\\alpha -\\textbf{1}_n^T\\boldsymbol\\alpha \\\\\n",
    "& \\text{subject to:}\n",
    "& &  \\; A_C\\boldsymbol\\alpha \\leq \\textbf{b} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(31) \\\\\n",
    "& & & \\; \\textbf{y}^T\\boldsymbol\\alpha = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $Q_D$ is the kernel matrix (see e.g. $(27)$) and\n",
    "\n",
    "\\begin{equation*}\n",
    "A_C = \\left[\\begin{array}{c}\n",
    "-\\textbf{I}_{n\\times n}\\\\\n",
    "\\textbf{I}_{n\\times n}\n",
    "\\end{array}\\right]\\;\\;\\;\n",
    "\\textbf{b} = \\left[\\begin{array}{c}\n",
    "\\textbf{0}_{n}\\\\\n",
    "\\textbf{C}_{n}\n",
    "\\end{array}\\right],\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\textbf{C}_{n}$ is a $n$-dimensional vector of values $C$. The problem $(31)$ is equivalent to problem $(29)$ and we have written it in the style of problem $(30)$ identifying the corresponding factors. We now proceed with constructing the matrix $M$ in the lemma of section 5.2 for problem $(31)$. Let the set $J$ be a set of indexes for which $\\alpha_i = 0$ or $\\alpha_i = C$ at point $\\boldsymbol\\alpha$, that is $J = \\{j\\in(1, 2, ..., n)\\,|\\, \\alpha_j = 0 \\;\\text{or}\\; \\alpha_j=C, \\boldsymbol\\alpha = (\\alpha_1, \\alpha_2, ..., \\alpha_n)\\}$. For example, if $C=1, n=3$ and $\\boldsymbol\\alpha = (\\alpha_1, \\alpha_2, \\alpha_3) = (0, 0.5, 1)$, then $J = \\{1, 3\\}$. Next, we define the matrix $A_1$ (as in the lemma) to be a $|J|\\times n$ matrix with zeros everywhere else, with the following exceptions: Let the $i$th value in $J$ be denoted as $k$. For all $i\\in(1, 2, ..., |J|)$, we set $A_1(i,k) = 1$. So for the example $J=\\{1, 3\\}$ we have:   \n",
    "\n",
    "\\begin{equation}\n",
    "A_1 = \\left[\\begin{array}{rrr}\n",
    "1 & 0 & 0  \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Next, we easily note that $Q=\\textbf{y}^T$ and so we get that $M$ is: \n",
    "\n",
    "\\begin{equation}\n",
    "M = \\left[\\begin{array}{r}\n",
    "A_1 \\\\\n",
    "\\textbf{y}^T\n",
    "\\end{array}\\right],\\;\\;\\; M^T = \\left[\\begin{array}{rr}\n",
    "A_1^T & \\textbf{y}\n",
    "\\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "For the same example above we have: \n",
    "\n",
    "\\begin{equation}\n",
    "M = \\left[\\begin{array}{rrr}\n",
    "1 & 0 & 0  \\\\\n",
    "0 & 0 & 1  \\\\\n",
    "y_1 & y_2 & y_3\n",
    "\\end{array}\\right],\\;\\;\\; M^T = \\left[\\begin{array}{rrr}\n",
    "1 & 0 & y_1  \\\\\n",
    "0 & 0 & y_2  \\\\\n",
    "0 & 1 & y_3\n",
    "\\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Now we simply apply $P=\\textbf{I}-M^T\\left(MM^T\\right)^{-1}M$ in the lemma and we have our projection matrix $P$, which projects any vector to the subspace specified by the set $J$ and equation $\\textbf{y}^T\\boldsymbol\\alpha = 0$. The constraints $\\textbf{0} \\leq \\boldsymbol\\alpha \\leq C\\cdot\\textbf{1}$ are called _box constraints_, and so we can think (in the GPAS) as moving along a hyperplane $\\textbf{y}^T\\boldsymbol\\alpha = 0$ constrained inside a box $\\textbf{0} \\leq \\boldsymbol\\alpha \\leq C\\cdot\\textbf{1}$. \n",
    "\n",
    "We are now almost done! We have found the 'active' constraints and constructed the projection matrix $P$. So, given that our current best solution is $\\boldsymbol\\alpha_i$, our next best guess is therefore: \n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol\\alpha_{i+1} = \\boldsymbol\\alpha_i - \\lambda_d P \\textbf{g}(\\boldsymbol\\alpha_i),\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\lambda_d \\geq 0$ (step size) will be determined by _line search_, which simply means solving $\\lambda_d$ from: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(\\boldsymbol\\alpha_i - \\lambda P\\textbf{g}(\\boldsymbol\\alpha_i))}{\\partial\\lambda} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "In the SVM problem $(31)$, we can think of our objective function at this point only a function of $\\lambda$ (since $\\boldsymbol\\alpha_i, P$ and $\\textbf{g}(\\boldsymbol\\alpha_i)$ are fixed), that is: \n",
    "\n",
    "$$f(\\lambda) = \\frac{1}{2}(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)^TQ_D(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)-\\textbf{1}_n^T(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i),$$\n",
    "\n",
    "where $\\textbf{d}_i=-P\\textbf{g}(\\boldsymbol\\alpha_i)=-P\\textbf{g}_i$. Lets now calculate the derivative of $f(\\lambda)$, equate it to zero, and solve for $\\lambda_d$:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\lambda) = \\frac{1}{2}(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)^TQ_D(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)-\\textbf{1}_n^T(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)\\\\\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "=\\frac{1}{2}(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)^T\\left[\\begin{array}{r}\\textbf{q}_1^T\\\\\\vdots\\\\\\textbf{q}_n^T\\end{array}\\right](\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)-\\textbf{1}_n^T(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= \\frac{1}{2}(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)^T\\left[\\begin{array}{r}\\textbf{q}_1^T\\boldsymbol\\alpha_{i}+\\lambda\\textbf{q}_1^T\\textbf{d}_i\\\\\\vdots\\\\\\textbf{q}_n^T\\boldsymbol\\alpha_{i}+\\lambda\\textbf{q}_n^T\\textbf{d}_i\\end{array}\\right]-\\textbf{1}_n^T(\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i)\\\\\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= \\frac{1}{2}\\left[\\sum_{i=1}^n \\alpha_i(\\textbf{q}_i^T\\boldsymbol\\alpha_i+\\lambda\\textbf{q}_i^T\\textbf{d}_i)+\\lambda d_i(\\textbf{q}_i^T\\boldsymbol\\alpha_i+\\lambda\\textbf{q}_i^T\\textbf{d}_i)\\right]-\\sum_{i=1}^n\\alpha_i-\\lambda\\sum_{i=1}^nd_i \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "=\\frac{1}{2}\\left[\\sum_{i=1}^n \\alpha_i\\textbf{q}_i^T\\boldsymbol\\alpha_i+\\alpha_i\\lambda\\textbf{q}_i^T\\textbf{d}_i+\\lambda d_i\\textbf{q}_i^T\\boldsymbol\\alpha_i+\\lambda^2 d_i\\textbf{q}_i^T\\textbf{d}_i\\right]-\\sum_{i=1}^n\\alpha_i-\\lambda\\sum_{i=1}^nd_i.\n",
    "\\end{equation}\n",
    "\n",
    "And now we calculate the derivative: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(\\lambda)}{\\partial\\lambda}=\\frac{1}{2}\\left[\\sum_{i=1}^n \\alpha_i\\textbf{q}_i^T\\textbf{d}_i+ d_i\\textbf{q}_i^T\\boldsymbol\\alpha_i+2\\lambda d_i\\textbf{q}_i^T\\textbf{d}_i\\right]-\\sum_{i=1}^nd_i \\\\\n",
    "= \\frac{1}{2}\\sum_{i=1}^n (\\alpha_i\\textbf{q}_i^T\\textbf{d}_i+ d_i\\textbf{q}_i^T\\boldsymbol\\alpha_i)+\\lambda\\sum_{i=1}^n d_i\\textbf{q}_i^T\\textbf{d}_i-\\sum_{i=1}^nd_i \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= \\frac{1}{2}\\boldsymbol\\alpha_i^TQ_D\\textbf{d}_i+\\frac{1}{2}\\textbf{d}_i^TQ_D\\boldsymbol\\alpha_i +\\lambda\\textbf{d}_i^TQ_D\\textbf{d}_i-\\sum_{i=1}^nd_i \\\\\n",
    "= \\boldsymbol\\alpha_i^TQ_D\\textbf{d}_i+\\lambda\\textbf{d}_i^TQ_D\\textbf{d}_i-\\textbf{1}_n^T\\textbf{d}_i \\\\\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= \\lambda\\textbf{d}_i^TQ_D\\textbf{d}_i+(\\boldsymbol\\alpha_i^TQ_D-\\textbf{1}_n^T)\\textbf{d}_i.\n",
    "\\end{equation}\n",
    "\n",
    "Before proceesing with $\\frac{\\partial f(\\lambda)}{\\partial\\lambda}$, let us calculate the gradient $\\textbf{g}(\\boldsymbol\\alpha)=\\nabla f(\\boldsymbol\\alpha)$ (we need it soon). We have: \n",
    "\n",
    "\\begin{equation}\n",
    "f(\\boldsymbol\\alpha) = \\frac{1}{2}\\boldsymbol\\alpha^T Q_D\\boldsymbol\\alpha-\\textbf{1}_n^T\\boldsymbol\\alpha\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= \\frac{1}{2}\\boldsymbol\\alpha^T \\left[\\begin{array}{r}\\textbf{q}_1^T\\\\\\vdots\\\\\\textbf{q}_n^T\\end{array}\\right]\\boldsymbol\\alpha-\\textbf{1}_n^T\\boldsymbol\\alpha\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= \\frac{1}{2}\\sum_{i=1}^n \\alpha_i\\textbf{q}_i^T\\boldsymbol\\alpha-\\sum_{i=1}^n\\alpha_i.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial\\alpha_k}=\\frac{1}{2}\\left[\\sum_{j\\neq k} \\alpha_j q_{jk}+\\sum_{j\\neq k} q_{kj}\\alpha_j+2\\alpha_kq_{kk}\\right]-1=\\sum_{i=1}^n q_{ki}\\alpha_i-1=\\textbf{q}_k^T\\boldsymbol\\alpha-1,\n",
    "\\end{equation}\n",
    "\n",
    "where $q_{ij}=y_i y_j K(\\textbf{x}_i, \\textbf{x}_j)$ and $q_{ij}=q_{ji}$ due to the symmetricity of $Q_D$. We thus have that:\n",
    "\n",
    "$$\\textbf{g}(\\boldsymbol\\alpha)=\\nabla f(\\boldsymbol\\alpha)=\\left[\\begin{array}{r}\\frac{\\partial f}{\\partial\\alpha_1}\\\\\\vdots\\\\\\frac{\\partial f}{\\partial\\alpha_n}\\end{array}\\right]=\\left[\\begin{array}{r}\\textbf{q}_1^T\\boldsymbol\\alpha-1\\\\\\vdots\\\\\\textbf{q}_n^T\\boldsymbol\\alpha-1\\end{array}\\right]=Q_D\\boldsymbol\\alpha-\\textbf{1}_n.$$\n",
    "\n",
    "Now continue with $\\frac{\\partial f(\\lambda)}{\\partial\\lambda}=0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda\\textbf{d}_i^TQ_D\\textbf{d}_i+(Q_D\\boldsymbol\\alpha_i-\\textbf{1}_n)^T\\textbf{d}_i=0\\\\\n",
    "\\rightarrow\\lambda_d=-\\frac{\\textbf{g}_i^T\\textbf{d}_i}{\\textbf{d}_i^TQ_D\\textbf{d}_i}=\\frac{-\\textbf{g}_i^T(-P\\textbf{g}_i)}{(-P\\textbf{g}_i)^TQ_D(-P\\textbf{g}_i)}=\\frac{\\textbf{g}_i^TP\\textbf{g}_i}{\\textbf{g}_i^TPQ_D P\\textbf{g}_i} \\geq 0,\n",
    "\\end{equation}\n",
    "\n",
    "where the last inequality follows from the symmetric positive semidefinite nature of matrices $P$ and $Q_D$. Furthermore, we also need to mind that while we are moving along the direction $\\textbf{d}_i = - \\lambda P\\textbf{g}(\\boldsymbol\\alpha_i)$, some other box constraints that were not active before, might become active for some $\\lambda < \\lambda_0$. That is, some new $\\alpha_j\\;j\\not\\in J$ might become $\\alpha_j=0$ or $\\alpha_j=C$ at point $\\boldsymbol\\alpha_{i+1}=\\boldsymbol\\alpha_{i}+\\lambda\\textbf{d}_i$. \n",
    "\n",
    "To make sure that we do not violate the box constraints, we also determine the following bounding $\\lambda$-values:\n",
    "\n",
    "$$\\lambda_0 = \\underset{j\\not\\in J}{\\min}-\\frac{\\alpha_j}{d_j}\\;\\;\\;\\lambda_C = \\underset{j\\not\\in J}{\\min}\\frac{C-\\alpha_j}{d_j},$$\n",
    "\n",
    "where $\\alpha_j, d_j$ are the $j$th coordinates of vectors $\\boldsymbol\\alpha_i$ and $\\textbf{d}_i$ respectively. To be exact, the values $\\lambda_0, \\lambda_C$ follow simply fr0m the equations: \n",
    "\n",
    "$$\\alpha_j+\\lambda d_j = 0\\;\\;\\text{and}\\;\\;\\alpha_j+\\lambda d_j = C.$$\n",
    "\n",
    "Now, the optimal step size $\\lambda^*$ is then defined as (assuming also that $\\lambda_0, \\lambda_C > 0$ ):  \n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda^* = \\max\\{0, \\min\\{\\lambda_0, \\lambda_C, \\lambda_d\\}\\},\n",
    "\\end{equation}\n",
    "\n",
    "and then we proceed to set:\n",
    "\n",
    "$$\\boldsymbol\\alpha_{i+1} = \\boldsymbol\\alpha_i + \\lambda^* \\textbf{d}_i,$$\n",
    "\n",
    "as our next solution. The whole above process from the beginning of this section is repeated until convergence criteria is met or KKT-conditions are satisfied. We have now constructed all the necessary theory for the GPAS-algorithm and we are ready to define its pseudocode. \n",
    "\n",
    "<a id='5.4'></a>\n",
    "#### 5.4 Pseudocode for the GPAS SVM solver\n",
    "\n",
    "<ol>\n",
    "<li>Start with a random feasible initial solution $\\boldsymbol\\alpha_0$ to problem $(31)$</li>\n",
    "<li>Calculate the projection matrix $d$</li>\n",
    "<li>Check constarints</li>\n",
    "<li>Update projection matrix</li>\n",
    "<li>project gradient</li>\n",
    "<li>Do line search</li>\n",
    "<li>Optimal $\\lambda$</li>\n",
    "<li>Update solution</li>\n",
    "<li>Repeat</li>\n",
    " </ol>\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "Y.S. Abu-Mostafa, M. Magdon-Ismail, H.-T Lin. \\emph{Learning From Data}. California Institute of Technology, 2012.\n",
    "  \n",
    "S. Boyd, L. Vandenberghe. \\emph{Convex Optimization}. Cambridge University Press, 2009.\n",
    "\n",
    "D.G. Luenberger, Y. Ye. \\emph{Linear and Nonlinear programming}. Springer, 3rd edition, 2008.\n",
    "\n",
    "V.N. Vapnik. \\emph{Statistical Learning Theory}. Jon Wiley \\& Sons Inc., 1998.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "#### Appendix: code listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#######################################\n",
    "# - Used to plot the example figures. #\n",
    "#######################################\n",
    "def drawClassifierLines(showRadius):\n",
    "    # Four data points from two different classes.\n",
    "    x = [0, 2, 2, 3]\n",
    "    y = [0, 2, 0, 0]\n",
    "    # Three classifier lines.\n",
    "    lines = [[-1, 4, -2, 2.2], [-1, 3.4, -2, 3], [-1, 4, -2, 3]]\n",
    "    # Three subplots.\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(20,5))\n",
    "    # Set plot limits.\n",
    "    xlim = [-2, 4]\n",
    "    ylim = [-2, 3]\n",
    "    # Plot all data points.\n",
    "    for i in range(0, len(axarr)):\n",
    "        axarr[i].plot(x[0:2], y[0:2], \"x\", color=[0, 0, .8], markersize=15, markeredgewidth=3)\n",
    "        axarr[i].plot(x[2:4], y[2:4], \"o\", color=[.8, 0, 0], markersize=15, markeredgewidth=3)\n",
    "        axarr[i].set_xlim(xlim)\n",
    "        axarr[i].set_ylim(ylim)\n",
    "        axarr[i].plot(lines[i][0:2], lines[i][2:4], 'k-')\n",
    "        axarr[i].set_xticks([])\n",
    "        axarr[i].set_yticks([])\n",
    "    # Draw uncertainty circles.\n",
    "    if showRadius:\n",
    "        rad = [0.4, 0.4, 1/np.sqrt(2)]\n",
    "        for j in range(0, len(axarr)):\n",
    "            for i in range(0, len(x)):\n",
    "                circle1 = plt.Circle((x[i], y[i]), rad[j], color=[.3,.3,.3], clip_on=False, alpha=0.5)\n",
    "                axarr[j].add_patch(circle1)\n",
    "    # Plot the hard coded decision areas.\n",
    "    axarr[0].fill([-1, -2, -2, 4, 4, -1], [-2, -2, 3, 3, 2.2, -2], color = [0, 0, 1], alpha=0.2)\n",
    "    axarr[0].fill([-1, 4, 4, -1], [-2, 2.2, -2, -2], color = [1, 0, 0], alpha=0.2)\n",
    "    axarr[1].fill([-1, -2, -2, 3.4, -1], [-2, -2, 3, 3, -2], color = [0, 0, 1], alpha=0.2)\n",
    "    axarr[1].fill([-1, 3.4, 4, 4, -1], [-2, 3, 3, -2, -2], color = [1, 0, 0], alpha=0.2)\n",
    "    axarr[2].fill([-1, -2, -2, 4, -1], [-2, -2, 3, 3, -2], color = [0, 0, 1], alpha=0.2)\n",
    "    axarr[2].fill([-1, 4, 4, -1], [-2, 3, -2, -2], color = [1, 0, 0], alpha=0.2)\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[[ 1  1  1 -1 -1 -1]]\n",
      "Using linear kernel\n",
      "\n",
      " W-values are: [0.35714286 0.42857143] b-values is: 0.35714285714285676\n",
      "CASE 4\n",
      "[-7.151019582872648, 6.151019582872648] [5.125849652393869, -5.959182985727198]\n",
      "[array([-6.0034786 ,  7.29856057])] [array([ 6.50289883, -4.58213381])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\Documents\\Omat_ohjelmat\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:174: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# SVM model class will be created here. Contains e.g. fit-function, kernel.     \n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, kernel, C):\n",
    "        self.kernel = kernel\n",
    "        self.penalty_term = C\n",
    "        \n",
    "    def getKernel(self):\n",
    "        return self.kernel\n",
    "    \n",
    "    def getPenaltyTerm(self):\n",
    "        return self.penalty_term\n",
    "    \n",
    "    def checkActiveConstraints(self, alpha, d, M):\n",
    "        # Check the constraints\n",
    "        active_set_indexes = []\n",
    "        #print(\"now a: \", alpha)\n",
    "        #print(\"now d: \", d)\n",
    "        for i in range(0, alpha.shape[0]):\n",
    "            if (alpha[i] == 0 and d[i] < 0) or (alpha[i] == self.penalty_term and d[i] > 0):\n",
    "                active_set_indexes.append(i)\n",
    "        # If true, we need to update M\n",
    "        \n",
    "        #print(\"d = \" + str(d))\n",
    "        #print(\"alpha = \" + str(alpha))\n",
    "        if len(active_set_indexes) > 0:\n",
    "            for i in range(0, len(active_set_indexes)):\n",
    "                active_constraint = np.zeros((1, alpha.shape[0]))\n",
    "                active_constraint[0, active_set_indexes[i]] = 1\n",
    "                M = np.vstack([active_constraint, M])\n",
    "            #print(M)\n",
    "            return (M, active_set_indexes)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        print(M)\n",
    "    \n",
    "    def checkActiveConstraints2(self, alpha, d, M, g, I):\n",
    "        # Check the constraints\n",
    "        active_set_indexes = []\n",
    "        needCon = True\n",
    "        #print(\"???????????????\")\n",
    "        #print(\"M initial: \\n\" + str(M))\n",
    "        for k in range(0, alpha.shape[0]):\n",
    "                if np.abs(alpha[k]) < np.finfo(float).eps*10:\n",
    "                    alpha[k] = 0.0\n",
    "                if np.abs(d[k]) < np.finfo(float).eps*10:\n",
    "                    d[k] = 0.0\n",
    "        while needCon:\n",
    "            #print(\"alpha, d at first step comp\")\n",
    "            #print(\"alpha\\n\", alpha)\n",
    "            #print(\"d\\n\", d)\n",
    "            for i in range(0, alpha.shape[0]):\n",
    "                if (alpha[i] == 0 and d[i] < 0) or (alpha[i] == self.penalty_term and d[i] > 0):\n",
    "                    active_set_indexes.append(i)\n",
    "            # If true, we need to update M\n",
    "            if len(active_set_indexes) > 0:\n",
    "                for i in range(0, len(active_set_indexes)):\n",
    "                    active_constraint = np.zeros((1, alpha.shape[0]))\n",
    "                    active_constraint[0, active_set_indexes[i]] = 1\n",
    "                    M = np.vstack([active_constraint, M])\n",
    "            # M has been updated, check that some other semiactive constrains are not broken\n",
    "            #print(\"M: \\n\" + str(M))\n",
    "            Mt = M.transpose()\n",
    "            #print(\"M:\\n\", M)\n",
    "            #print(\"Mt:\\n\", Mt)\n",
    "            #print(\"MMt\\n\", np.matmul(M, Mt))\n",
    "            #print(np.matmul(M, Mt))\n",
    "            #print(\"EPS\", np.finfo(float).eps)\n",
    "            MMt_1 = np.linalg.inv(np.matmul(M, Mt))\n",
    "            P = I - np.matmul(Mt, np.matmul(MMt_1, M))\n",
    "            d = np.matmul(-P, g(alpha))\n",
    "            for k in range(0, alpha.shape[0]):\n",
    "                if np.abs(d[k]) < np.finfo(float).eps*10:\n",
    "                    d[k] = 0.0\n",
    "            #print(\"d:\", d)\n",
    "            # Final check \n",
    "            #print(\"FINAL CHECK\")\n",
    "            #print(alpha,d)\n",
    "            for i in range(0, alpha.shape[0]):\n",
    "                #print(\"comparison\", i, alpha[i], d[i], alpha[i]==0, alpha[i] == 0 and d[i] < 0)\n",
    "                if (alpha[i] == 0 and d[i] < 0) or (alpha[i] == self.penalty_term and d[i] > 0):\n",
    "                    needCon = True\n",
    "                    active_set_indexes = []\n",
    "                    break\n",
    "                else:\n",
    "                    needCon = False\n",
    "        #print(\"M final: \\n\" + str(M))\n",
    "        #print(\"ALTERNATIVE d: \\n\" + str(d))\n",
    "        #print(\"alpha: \\n\" + str(alpha))\n",
    "        #print(\"ALTERNATIVE constraints: \\n\" + str(active_set_indexes))\n",
    "        #print(\"???????????????\")\n",
    "        return(d, active_set_indexes)\n",
    "\n",
    "            \n",
    "    \n",
    "    def train(self, data, labels, epsilon, maxIters, **kwargs):\n",
    "        \n",
    "        iters = 1\n",
    "        alpha = self.generateRandomAlpha(labels)\n",
    "        #alpha = np.random.rand(data.shape[0], 1)\n",
    "        #alpha[0] = 0.5\n",
    "        #alpha[1] = 0.5\n",
    "        #alpha[2] = 1.0\n",
    "        #alpha[3] = 0.0\n",
    "        alpha = np.zeros((data.shape[0], 1))\n",
    "        Q_D = self.getKernelMatrix(data, labels, **kwargs)\n",
    "        #print(\"kernel matrix Q_D = \\n\" + str(Q_D))\n",
    "        g = lambda alpha : np.matmul(Q_D, alpha) - np.ones([alpha.shape[0],1])\n",
    "        f = lambda alpha : 0.5*np.matmul(alpha.transpose(), np.matmul(Q_D, alpha)) - np.sum(alpha)\n",
    "        I = np.identity(np.max(labels.shape))\n",
    "        active_set_indexes = []\n",
    "        while True:\n",
    "            #print(\"kernel matrix Q_D = \" + str(Q_D))\n",
    "            #print(\"Q_D:\", Q_D, \"alpha\", alpha)\n",
    "            #print(g(alpha))\n",
    "            #print(labels)\n",
    "            # Initial projection matrix, only hyperplane constraint in beginning    \n",
    "            M = labels\n",
    "            Mt = M.transpose()\n",
    "            MMt_1 = np.linalg.inv(np.matmul(M, Mt))\n",
    "            P = I - np.matmul(Mt, np.matmul(MMt_1, M))\n",
    "            #print(I, P)\n",
    "            #print(\"P:\\n\" + str(np.matmul(P, I)))\n",
    "            #print(\"PP:\\n\" + str(np.matmul(P,P)))\n",
    "            #np.linalg.cholesky(P)\n",
    "            d = np.matmul(-P, g(alpha))\n",
    "            #print(\"P = \" + str(P))\n",
    "            #print(\"Current alpha: \\n\" + str(alpha))\n",
    "            #print(\"Gradient g(alpha): \\n\" + str(g(alpha)))\n",
    "            #print(\"Constraint matrix M: \\n\" + str(M))\n",
    "            #print(\"Initial projection matrix P: \\n\" + str(P))\n",
    "            #print(\"Initial search direction d: \\n\" + str(d))\n",
    "            Mn_and_constraints = self.checkActiveConstraints(alpha, d, M)\n",
    "            d_c = self.checkActiveConstraints2(alpha, d, M, g, I)\n",
    "            #Mn = Mn_and_constraints[0]\n",
    "            #active_set_indexes = Mn_and_constraints[1]\n",
    "            #Mn, active_set_indexes = self.checkActiveConstraints(alpha, d, M)\n",
    "            vari = None\n",
    "            active_set_indexes = []\n",
    "            if Mn_and_constraints is not None and vari is not None:\n",
    "                #print(\"ACTIVE CONSTRAINTS\")\n",
    "                #print(Mn_and_constraints)\n",
    "                M = Mn_and_constraints[0]\n",
    "                Mt = M.transpose()\n",
    "                MMt_1 = np.linalg.inv(np.matmul(M, Mt))\n",
    "                P = I - np.matmul(Mt, np.matmul(MMt_1, M))\n",
    "                d = np.matmul(-P, g(alpha))\n",
    "                active_set_indexes = Mn_and_constraints[1]\n",
    "                #print(\"Active indexes: \" + str(active_set_indexes))\n",
    "                #print(\"Updated constraint matrix M: \\n\" + str(M))\n",
    "                #print(\"Updated projection matrix P: \\n\"+str(P))\n",
    "                #print(\"Updated search direction d is = \" + str(d))\n",
    "            if vari is None:\n",
    "                d = d_c[0]\n",
    "                active_set_indexes = d_c[1]\n",
    "                \n",
    "            \n",
    "            non_active_indexes = [x for x in range(0,np.max(labels.shape)) if x not in active_set_indexes]\n",
    "            if kwargs[\"printInfo\"]:\n",
    "                print(\"Non-active set indexes:\\n\" + str(non_active_indexes))\n",
    "                print(\"Corresponding non-active alpha-values: \\n\" + str(alpha[non_active_indexes]))\n",
    "                print(\"Corresponding non-active d-values: \\n\" + str(d[non_active_indexes]))\n",
    "                print(\"-alpha/d:\\n\" + str(-np.divide(alpha[non_active_indexes], d[non_active_indexes])))\n",
    "                print(\"C-alpha/d:\\n\" + str(np.divide(self.penalty_term-alpha[non_active_indexes], d[non_active_indexes])))\n",
    "                print([l for l in -np.divide(alpha[non_active_indexes], d[non_active_indexes]) if l > 0])\n",
    "            lambda_0 = np.inf\n",
    "            lambda_C = np.inf\n",
    "            list_a0 = [l for l in -np.divide(alpha[non_active_indexes], d[non_active_indexes]) if l > 0]\n",
    "            list_aC = [l for l in np.divide(self.penalty_term-alpha[non_active_indexes], d[non_active_indexes]) if l > 0]\n",
    "            if len(list_a0) > 0:\n",
    "                lambda_0 = np.min(list_a0)\n",
    "            if len(list_aC) > 0:\n",
    "                lambda_C = np.min(list_aC)\n",
    "            \n",
    "            lamlims = [lambda_0, lambda_C]\n",
    "            #print(\"unedited lambdalims: \" + str(lamlims) )\n",
    "            lamlims = [l for l in lamlims if l > 0] \n",
    "            #np.linalg.cholesky(P)\n",
    "            gPQ_DPg = np.matmul(g(alpha).transpose(), np.matmul(np.matmul(P, np.matmul(Q_D, P)), g(alpha)))\n",
    "            gPg = np.matmul(g(alpha).transpose(), np.matmul(P, g(alpha)))\n",
    "            #print(\"gPg = \\n\" + str(gPg))\n",
    "            #print(\"gPQ_DPg = \\n\" +str(gPQ_DPg))\n",
    "            lambda_d = gPg / float(gPQ_DPg)\n",
    "            lambda_d = lambda_d[0][0]\n",
    "            #print([lambda_0, lambda_C, lambda_d])\n",
    "            lamlims.append(lambda_d)\n",
    "            opt_lambda = np.max([0.0, np.min(lamlims)])\n",
    "            if kwargs[\"printInfo\"]:\n",
    "                print(\"lambda limits: \" + str(lamlims))\n",
    "                print(\"lambda_d: \" + str(lambda_d))\n",
    "                print(\"Optimal step size: \" + str(opt_lambda))\n",
    "                print(\"Final search direction: \\n\" + str(d))\n",
    "            old_alpha = alpha[:]\n",
    "            alpha = alpha + opt_lambda*d\n",
    "            \n",
    "            #print(\"d=\"+str(d))\n",
    "            #print(\"old_alpha =\\n\"+str(old_alpha))\n",
    "            if kwargs[\"printInfo\"]:\n",
    "                print(\"alpha = \"+str(alpha.transpose())) \n",
    "                print(\"Difference: \" + str(np.sum(np.abs(alpha-old_alpha))))\n",
    "                print(\"Function value: \" + str(f(alpha)))\n",
    "                print(\"hyperplane value: \" + str(np.matmul(labels, alpha)))\n",
    "                print(\"Iteration: \" + str(iters))\n",
    "                print(\"*****************************************************\")\n",
    "            for k in range(0, alpha.shape[0]):\n",
    "                if np.abs(alpha[k]) < np.finfo(float).eps:\n",
    "                    alpha[k] = 0.0\n",
    "            if any(x<0 for x in alpha) == True:\n",
    "                sys.exit('Negative entry in alpha!')\n",
    "            if np.sum(np.abs(alpha-old_alpha)) < epsilon or iters == maxIters:\n",
    "                break\n",
    "            iters += 1\n",
    "        return self.getSvmParameters(alpha, data, labels)\n",
    "    \n",
    "        \n",
    "    def getKernelMatrix(self, data, labels, **kwargs):\n",
    "        Q_D = np.zeros([len(labels[0]), len(labels[0])])\n",
    "        kernel_function = None\n",
    "        if self.kernel == \"linear\":\n",
    "            kernel_function = lambda a, b : np.dot(a,b)\n",
    "            print(\"Using linear kernel\")\n",
    "        elif self.kernel == \"Gaussian\":\n",
    "            kernel_function = lambda a, b : np.exp(-np.dot(a-b, a-b)/(2*kwargs[\"sigma\"]**2))\n",
    "            print(\"Using Gaussian kernel\")\n",
    "        elif self.kernel == \"polynomial\":\n",
    "            kernel_function = lambda a, b : (kwargs[\"l\"] + kwargs[\"g\"]*np.dot(a,b))**kwargs[\"q\"]\n",
    "            print(\"Using polynomial kernel\")\n",
    "        #print(kwargs[\"g\"])\n",
    "        for i in range(0, len(labels[0])):\n",
    "            for j in range(0, len(labels[0])):\n",
    "                Q_D[i,j] = labels[0,i]*labels[0,j]*kernel_function(data[i,:], data[j,:])\n",
    "        return Q_D\n",
    "    \n",
    "    def generateRandomAlpha(self, labels):\n",
    "        negInds = []\n",
    "        posInds = []\n",
    "        data_length = np.max(labels.shape) # Assumes >= 4\n",
    "        for i in range(0, data_length):\n",
    "            if labels[0,i] < 0:\n",
    "                negInds.append(i)\n",
    "            elif labels[0,i] > 0:\n",
    "                posInds.append(i)\n",
    "        p1 = np.zeros([data_length,1])\n",
    "        p2 = np.zeros([data_length,1])\n",
    "        p1[posInds[0]] = 1\n",
    "        p1[negInds[0]] = 1\n",
    "        p2[posInds[1]] = 1\n",
    "        p2[negInds[1]] = 1\n",
    "        alpha = np.random.rand(1)[0]*p1 + np.random.rand(1)[0]*p2\n",
    "        if self.penalty_term < np.inf: # Finite C, soft-margin\n",
    "            alpha = np.random.rand(1)[0]*self.penalty_term*p1 + np.random.rand(1)[0]*self.penalty_term*p2\n",
    "        #print(p1.transpose(), p2)\n",
    "        #print(\"p1p2: \\n\" + str(np.matmul(p1.transpose(),p2)))\n",
    "        #print(\"hp\", np.matmul(labels, alpha))\n",
    "        return alpha\n",
    "    \n",
    "    def getSvmParameters(self, alpha, data, labels):\n",
    "        data_length = np.max(labels.shape)\n",
    "        w = np.zeros([1, data.shape[1]])\n",
    "        non_zero_alphas_inds = []\n",
    "        for i in range(0, data_length):\n",
    "            #print(labels[0,i], alpha[i,0], data[i,:])\n",
    "            w += labels[0,i]*alpha[i,0]*data[i]\n",
    "            if alpha[i] > 0:\n",
    "                non_zero_alphas_inds.append(i)\n",
    "        nonz_alpha_y = labels[0,non_zero_alphas_inds[0]] # Assuming at least one exists\n",
    "        nonz_alpha_x = data[non_zero_alphas_inds[0], :] # Assuming at least one exists\n",
    "        b = 1/float(nonz_alpha_y)\n",
    "        for i in range(0, data_length):\n",
    "            b -= labels[0,i]*alpha[i,0]*np.dot(data[i,:], nonz_alpha_x)\n",
    "        return(w[0], b)\n",
    "        \n",
    "    def createRandData(self, xdim, number_of_pos, number_of_neg, inter):\n",
    "        dataX = np.array([1,2])\n",
    "        dataY = []\n",
    "        # Create a random line\n",
    "        n_vector = np.random.rand(xdim, 1)\n",
    "        bias = np.random.rand(1)\n",
    "        bias = 0\n",
    "        posfound = 0\n",
    "        negfound = 0\n",
    "        print(dataX)\n",
    "        while posfound < number_of_pos:\n",
    "            #datap = np.random.rand(2)\n",
    "            datap= np.random.randint(inter[0],inter[1], (1,2))\n",
    "            #print(np.matmul(datap,n_vector))\n",
    "            if np.matmul(datap,n_vector) > -bias: \n",
    "                dataX = np.vstack((dataX, datap))\n",
    "                dataY.append(1)\n",
    "                posfound += 1\n",
    "        while negfound < number_of_neg:\n",
    "            #datap = np.random.rand(2)\n",
    "            datap= np.random.randint(inter[0],inter[1], (1,2))\n",
    "            #print(np.matmul(datap,n_vector))\n",
    "            np.matmul(datap,n_vector)\n",
    "            if np.matmul(datap,n_vector) < -bias: \n",
    "                dataX = np.vstack((dataX, datap))\n",
    "                dataY.append(-1)\n",
    "                negfound += 1\n",
    "        dataX = np.delete(dataX, 0, 0)\n",
    "        #print(dataX, dataY)\n",
    "        return(dataX, dataY)\n",
    "                \n",
    "    def plot2D(self, data, labels, model):\n",
    "        w = model[0] # List entry containing the weights\n",
    "        b = model[1] # List entry containing the bias\n",
    "        wnorm = float(np.linalg.norm(w))\n",
    "        margin = 1/wnorm # Margin of the SVM model\n",
    "        # Next, we create the boundaries for the plot.\n",
    "        minX = np.min(data[:,0])\n",
    "        maxX = np.max(data[:,0])\n",
    "        minY = np.min(data[:,1])\n",
    "        maxY = np.max(data[:,1])  \n",
    "        f,ax = plt.subplots(1)\n",
    "        # Next, we draw the classification areas. We need to check the borders\n",
    "        marginCoefficient = 1.2\n",
    "        xborder = [minX-margin*marginCoefficient, maxX + margin*marginCoefficient]\n",
    "        yborder = [minY-margin*marginCoefficient, maxY + margin*marginCoefficient]\n",
    "        lineYvaluesAtXborder = [(-b-w[0]*xborder[0])/float(w[1]), (-b-w[0]*xborder[1])/float(w[1])]\n",
    "        lineXvaluesAtYborder = [(-b-w[1]*yborder[0])/float(w[0]), (-b-w[1]*yborder[1])/float(w[0])]\n",
    "        xFillArea1 = []\n",
    "        yFillArea1 = []\n",
    "        xFillArea2 = []\n",
    "        yFillArea2 = []\n",
    "        slope = 0\n",
    "        xvalues = []\n",
    "        yvalues = []\n",
    "        if np.min(lineYvaluesAtXborder) < yborder[0] and np.max(lineYvaluesAtXborder) > yborder[1]: # Cases where y goes over border, does not touch x-border\n",
    "            minXind = np.where(lineXvaluesAtYborder==np.min(lineXvaluesAtYborder))[0][0]\n",
    "            maxXind = [i for i in [0, 1] if i not in [minXind]][0]\n",
    "            xvalues = [lineXvaluesAtYborder[minXind], lineXvaluesAtYborder[maxXind]]\n",
    "            yvalues = [yborder[minXind], yborder[maxXind]]\n",
    "            xFillArea1, yFillArea1 = xvalues[:], yvalues[:]\n",
    "            xFillArea2, yFillArea2 = xvalues[:], yvalues[:]\n",
    "            if yvalues[1] > yvalues[0]: # Positive slope\n",
    "                print(\"CASE 1\")\n",
    "                xFillArea1.extend([xborder[0], xborder[0]])\n",
    "                xFillArea2.extend([xborder[1], xborder[1]])\n",
    "                yFillArea1.extend([yborder[1], yborder[0]])\n",
    "                yFillArea2.extend([yborder[1], yborder[0]])\n",
    "            else: # Negative slope\n",
    "                print(\"CASE 2\")\n",
    "                xFillArea1.extend([xborder[0], xborder[0]])\n",
    "                xFillArea2.extend([xborder[1], xborder[1]])\n",
    "                yFillArea1.extend([yborder[0], yborder[1]])\n",
    "                yFillArea2.extend([yborder[0], yborder[1]])\n",
    "        elif np.min(lineYvaluesAtXborder) > yborder[0] and np.max(lineYvaluesAtXborder) < yborder[1]: # Y inside border, cases 3,4\n",
    "            xvalues = xborder[:]\n",
    "            yvalues = lineYvaluesAtXborder[:]\n",
    "            xFillArea1, yFillArea1 = xvalues[:], yvalues[:]\n",
    "            xFillArea2, yFillArea2 = xvalues[:], yvalues[:]\n",
    "            if yvalues[1] > yvalues[0]: # Positive slope\n",
    "                print(\"CASE 3\")\n",
    "                xFillArea1.extend([xborder[1], xborder[0]])\n",
    "                xFillArea2.extend([xborder[1], xborder[0]])\n",
    "                yFillArea1.extend([yborder[1], yborder[1]])\n",
    "                yFillArea2.extend([yborder[0], yborder[0]])\n",
    "            else: # Negative slope\n",
    "                print(\"CASE 4\")\n",
    "                xFillArea1.extend([xborder[1], xborder[0]])\n",
    "                xFillArea2.extend([xborder[1], xborder[0]])\n",
    "                yFillArea1.extend([yborder[1], yborder[1]])\n",
    "                yFillArea2.extend([yborder[0], yborder[0]])\n",
    "        elif (lineYvaluesAtXborder[0] > yborder[0] and lineYvaluesAtXborder[0] < yborder[1]) and lineYvaluesAtXborder[1] > yborder[1] : # Y inside border, cases 3,4\n",
    "            xvalues = [xborder[0], lineXvaluesAtYborder[1]]\n",
    "            yvalues = [lineYvaluesAtXborder[0], yborder[1]]\n",
    "            xFillArea1, yFillArea1 = xvalues[:], yvalues[:]\n",
    "            xFillArea2, yFillArea2 = xvalues[:], yvalues[:]\n",
    "            print(\"CASE 5\")\n",
    "            xFillArea1.extend([xborder[0]])\n",
    "            xFillArea2.extend([xborder[1], xborder[1], xborder[0]])\n",
    "            yFillArea1.extend([yborder[1]])\n",
    "            yFillArea2.extend([yborder[1], yborder[0], yborder[0]])\n",
    "        elif (lineYvaluesAtXborder[0] > yborder[0] and lineYvaluesAtXborder[0] < yborder[1]) and lineYvaluesAtXborder[1] < yborder[0]: # Y inside border, cases 3,4 \n",
    "            xvalues = [xborder[0], lineXvaluesAtYborder[0]]\n",
    "            yvalues = [lineYvaluesAtXborder[0], yborder[0]]\n",
    "            xFillArea1, yFillArea1 = xvalues[:], yvalues[:]\n",
    "            xFillArea2, yFillArea2 = xvalues[:], yvalues[:]\n",
    "            print(\"CASE 6\")\n",
    "            xFillArea1.extend([xborder[1], xborder[1], xborder[0]])\n",
    "            xFillArea2.extend([xborder[0]])\n",
    "            yFillArea1.extend([yborder[0], yborder[1], yborder[1]])\n",
    "            yFillArea2.extend([yborder[0]])\n",
    "        elif lineYvaluesAtXborder[0] < yborder[0] and lineYvaluesAtXborder[1] < yborder[1] and lineYvaluesAtXborder[1] > yborder[0]:\n",
    "            xvalues = [lineXvaluesAtYborder[0], xborder[1]]\n",
    "            yvalues = [yborder[0], lineYvaluesAtXborder[1]]\n",
    "            xFillArea1, yFillArea1 = xvalues[:], yvalues[:]\n",
    "            xFillArea2, yFillArea2 = xvalues[:], yvalues[:]\n",
    "            print(\"CASE 7\")\n",
    "            xFillArea1.extend([xborder[1], xborder[0], xborder[0]])\n",
    "            xFillArea2.extend([xborder[1]])\n",
    "            yFillArea1.extend([yborder[1], yborder[1], yborder[0]])\n",
    "            yFillArea2.extend([yborder[0]])\n",
    "        elif lineYvaluesAtXborder[0] > yborder[1] and lineYvaluesAtXborder[1] < yborder[1] and lineYvaluesAtXborder[1] > yborder[0]:\n",
    "            xvalues = [lineXvaluesAtYborder[1], xborder[1]]\n",
    "            yvalues = [yborder[1], lineYvaluesAtXborder[1]]\n",
    "            xFillArea1, yFillArea1 = xvalues[:], yvalues[:]\n",
    "            xFillArea2, yFillArea2 = xvalues[:], yvalues[:]\n",
    "            print(\"CASE 8\")\n",
    "            xFillArea1.extend([xborder[1]])\n",
    "            xFillArea2.extend([xborder[1], xborder[0], xborder[0]])\n",
    "            yFillArea1.extend([yborder[1]])\n",
    "            yFillArea2.extend([yborder[0], yborder[0], yborder[1]])\n",
    "\n",
    "        xPos, yPos = [], []\n",
    "        xNeg, yNeg = [], []\n",
    "        if xFillArea1[-1]*w[0] + yFillArea1[-1]*w[1] > -b:\n",
    "            xPos = xFillArea1\n",
    "            yPos = yFillArea1\n",
    "            xNeg = xFillArea2\n",
    "            yNeg = yFillArea2\n",
    "        else:\n",
    "            xPos = xFillArea2\n",
    "            yPos = yFillArea2\n",
    "            xNeg = xFillArea1\n",
    "            yNeg = yFillArea1\n",
    "        print(xvalues, yvalues)\n",
    "        ax.plot(xvalues, yvalues, 'k-', label=\"SVM model\") \n",
    "        \n",
    "        ba = margin*(w/wnorm)\n",
    "        xval1, yval1 = [xvalues+ba[0]], [yvalues+ba[1]]\n",
    "        xval2, yval2 = [xvalues-ba[0]], [yvalues-ba[1]]\n",
    "        print(xval1, yval1)\n",
    "        ax.plot(xval1[0], yval1[0], 'k--') \n",
    "        ax.plot(xval2[0], yval2[0], 'k--') \n",
    "        \n",
    "        ax.fill(xPos, yPos, color = [1, 0, 0], alpha=0.2)\n",
    "        ax.fill(xNeg, yNeg, color = [0, 0, 1], alpha=0.2)\n",
    "        markerSizeCircle = 10\n",
    "        markerSizeCross = 8\n",
    "        p1 = None;\n",
    "        p2 = None;\n",
    "        firstP = True\n",
    "        firstN = True\n",
    "        # Plot all the data points. \n",
    "        for i in range(0, data.shape[0]):\n",
    "            if labels[0,i] > 0:\n",
    "                if firstP:\n",
    "                    ax.plot(data[i,0], data[i,1], \"o\", color=[.8, 0, 0], markersize=markerSizeCircle, markeredgewidth=3, label=\"positive\")\n",
    "                    firstP = False\n",
    "                else:\n",
    "                    ax.plot(data[i,0], data[i,1], \"o\", color=[.8, 0, 0], markersize=markerSizeCircle, markeredgewidth=3)\n",
    "            else:\n",
    "                if firstN:\n",
    "                    ax.plot(data[i,0], data[i,1], \"x\", color=[0, 0, .8], markersize=markerSizeCross, markeredgewidth=3, label=\"negative\")\n",
    "                    firstN = False\n",
    "                else:\n",
    "                    ax.plot(data[i,0], data[i,1], \"x\", color=[0, 0, .8], markersize=markerSizeCross, markeredgewidth=3)\n",
    "\n",
    "            circle1 = plt.Circle((data[i,0], data[i,1]), margin, color=[.3,.3,.3], clip_on=False, alpha=0.5)\n",
    "            ax.add_patch(circle1)\n",
    "        \n",
    "        ax.axis('equal')\n",
    "        plt.rcParams['axes.xmargin'] = 0\n",
    "        f.tight_layout()\n",
    "        plt.margins(x=0,y=0, tight=True)\n",
    "        #f.subplots_adjust(bottom=0, top=1, left=0, right=1)\n",
    "        #ax.get_xaxis().set_visible(False)\n",
    "        #ax.get_yaxis().set_visible(False)\n",
    "        f.tight_layout()\n",
    "\n",
    "        str2 = None\n",
    "        str3 = None\n",
    "\n",
    "        if w[1] < 0:\n",
    "            str2 = str(np.round(w[1],4))\n",
    "        else:\n",
    "            str2 = \"+\" + str(np.round(w[1],4))\n",
    "        if b < 0:\n",
    "            str3 = str(np.round(b,4))\n",
    "        else:\n",
    "            str3 = \"+\" + str(np.round(b,4))\n",
    "        plt.title(\"SVM model: g(x,y) = sgn(\" + str(np.round(w[0],4)) + \"x\" + str2 + \"y\" + str3 + \")\", fontweight='bold')\n",
    "        plt.xlabel(\"X-value\", fontweight='bold')\n",
    "        plt.ylabel(\"Y-value\", fontweight='bold')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    "            \n",
    "            \n",
    "\n",
    "newSVM = SVM(\"linear\", np.inf)\n",
    "#newSVM = SVM(\"linear\", 5)\n",
    "\n",
    "kwargs = {\"sigma\":1, \"l\":0, \"g\":1, \"q\":1, \"printInfo\":False}\n",
    "#newSVM.getKernelMatrix(dataX, dataY, **kwargs)\n",
    "eps = np.finfo(float).eps\n",
    "#print(eps)\n",
    "mv = sys.float_info.min\n",
    "#print(mv)\n",
    "#a = newSVM.train(dataX, dataY, eps, **kwargs)\n",
    "inter = [-6,6]\n",
    "dataM = newSVM.createRandData(2,3,3, inter)\n",
    "dataX=dataM[0]\n",
    "dataY=dataM[1]\n",
    "#dataX = np.array([[0,0], [2,2], [2,0], [3,0]])\n",
    "#dataY = [-1, -1, 1, 1]\n",
    "dataY = np.reshape(dataY, (1, len(dataY)))\n",
    "print(dataY)\n",
    "a = newSVM.train(dataX, dataY, mv, 1000, **kwargs)\n",
    "#print(newSVM.generateRandomAlpha(dataY))\n",
    "print(\"\\n W-values are: \" + str(a[0]), \"b-values is: \" +str(a[1]))\n",
    "\n",
    "newSVM.plot2D(dataX, dataY, a)\n",
    "\n",
    "# SOME ISSUES WITH MATRIX SINGULARITY, FLOATING POINT PRECISION I THINK...NEED TO MAKE EPS CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "l = [1, 2]\n",
    "l.extend([3,4,5])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
